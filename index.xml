<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Justice的小站</title><link>https://justice.bj.cn/</link><description>Recent content on Justice的小站</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 10 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://justice.bj.cn/index.xml" rel="self" type="application/rss+xml"/><item><title>Justice's Blog</title><link>https://justice.bj.cn/homepage/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/about/</guid><description>&lt;h2 id="self-introduction">Self Introduction&lt;/h2>
&lt;p>Cras ex dui, tristique a libero eget, consectetur semper ligula. Nunc augue arcu, malesuada a nisi et, molestie finibus metus. Sed lacus odio, ultricies a nisl vitae, sollicitudin tempor ipsum. Vivamus quis feugiat arcu. Sed mi nunc, efficitur quis tellus vitae, posuere mattis metus. Phasellus in mattis dui. Nullam blandit, augue non ullamcorper dapibus, lacus dui molestie massa, in iaculis purus lectus eu lectus. Duis hendrerit lacinia tellus, sit amet feugiat dolor placerat id. Aenean ac velit massa. Vivamus feugiat dui at magna viverra, ut dictum nunc rutrum. Duis eget sapien finibus, lobortis orci id, vestibulum tellus. Maecenas lobortis urna libero, quis fermentum lectus lobortis nec. Nullam laoreet volutpat libero, ac mattis magna ullamcorper quis. Duis eget ipsum eu nisi mattis cursus et vitae turpis.&lt;/p>
&lt;p>Aliquam pretium diam eget leo feugiat finibus. Donec malesuada commodo ipsum. Aenean a massa in lacus venenatis vestibulum. Duis vel sem quis elit iaculis consectetur et quis dolor. Morbi eu ipsum hendrerit, malesuada ante sed, dapibus est. Suspendisse feugiat nulla ut gravida convallis. Phasellus id massa posuere, rhoncus justo ut, porttitor dolor. Nulla ultrices malesuada egestas. Nunc fermentum tincidunt sem ac vulputate. Donec mollis sollicitudin justo eget varius. Donec ornare velit et felis blandit, id molestie sapien lobortis. Morbi eget tristique justo. Mauris posuere, nibh eu laoreet ultricies, ligula erat iaculis sapien, vel dapibus lacus libero ut diam. Etiam viverra ante felis, et scelerisque nunc pellentesque vitae. Praesent feugiat dictum molestie.&lt;/p>
&lt;h2 id="details">Details&lt;/h2>
&lt;p>Nunc pellentesque vitae:&lt;/p>
&lt;ul>
&lt;li>Morbi accumsan nibh efficitur diam molestie, non dignissim diam facilisis.&lt;/li>
&lt;li>Donec dignissim leo in mollis faucibus.&lt;/li>
&lt;li>Donec blandit lacus a pellentesque fermentum.&lt;/li>
&lt;/ul>
&lt;p>Donec mollis sollicitudin:&lt;/p>
&lt;ul>
&lt;li>Nunc dictum purus ornare purus consectetur, eu pellentesque massa ullamcorper.&lt;/li>
&lt;li>Aliquam eu leo vitae justo aliquam tincidunt.&lt;/li>
&lt;li>Fusce non massa id augue interdum feugiat sed et nulla.&lt;/li>
&lt;li>Vivamus molestie augue in tristique laoreet.&lt;/li>
&lt;/ul></description></item><item><title>Pages</title><link>https://justice.bj.cn/homepage/pages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/pages/</guid><description/></item><item><title>Experiences</title><link>https://justice.bj.cn/homepage/experiences/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/experiences/</guid><description/></item><item><title>Vintage</title><link>https://justice.bj.cn/homepage/vintage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/vintage/</guid><description/></item><item><title>Blank</title><link>https://justice.bj.cn/homepage/blank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/blank/</guid><description>
&lt;div style="text-align:center">
&lt;p>Write anything you like here!&lt;/p>
&lt;/div></description></item><item><title>AWS常用概念</title><link>https://justice.bj.cn/post/30.architech/aws-s3/aws-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/aws-s3/aws-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</guid><description>&lt;h1 id="aws常用概念">AWS常用概念&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>每个&lt;strong>Region&lt;/strong>都是完全独立的。每个&lt;strong>Availability Zone&lt;/strong>都是隔离的，但是Region中的可用区通过低延迟链接连接。&lt;strong>Local Zone&lt;/strong>是一种AWS基础架构部署，可将所选服务放置在更接近最终用户的位置。&lt;strong>Local Zone&lt;/strong>是与您所在区域不同位置的区域的扩展。它为AWS基础架构提供了高带宽主干，非常适合对延迟敏感的应用程序，例如机器学习。下图说明了区域，可用区域和本地区域之间的关系。&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/21-08-59-25-2020-03-03-12-48-56-image.png" alt="">&lt;/p>
&lt;p>Amazon EC2资源是以下资源之一：全局，与Region，&lt;strong>Availability Zone&lt;/strong>或&lt;strong>Local Zone&lt;/strong>绑定&lt;/p>
&lt;h2 id="region">Region&lt;/h2>
&lt;p>每个Amazon EC2 &lt;strong>Region&lt;/strong>都旨在与其他Amazon EC2 &lt;strong>Region&lt;/strong>隔离。这样可以实现最大的容错能力和稳定性。&lt;/p>
&lt;p>当您查看您的资源时，您只会看到与您指定的区域相关联的资源。这是因为区域彼此隔离，并且我们不会自动在区域之间复制资源。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>BlobFS</title><link>https://justice.bj.cn/post/40.storage/spdk/blobfs%E6%BA%90%E7%A0%81/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/spdk/blobfs%E6%BA%90%E7%A0%81/</guid><description>&lt;h1 id="blobfs">BlobFS&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>blobfs 是 spdk 中基于 blobstore 块设备实现的一个简易的文件系统。&lt;/p>
&lt;h2 id="编译">编译&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ yum install -y libaio libaio-devel ncurses ncureses-devel CUnit fuse3 fuse3-devel jq
$ pip3 install meson
$ wget https://sourceforge.net/projects/cunit/files/latest/download
$ git clone https://github.com/spdk/spdk
$ &lt;span class="nb">cd&lt;/span> spdk
$ git submodule update --init
$ sh scripts/pkgdep.sh
$ wget -O /tmp/ninja-linux.zip https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-linux.zip &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> unzip /tmp/ninja-linux.zip &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> mv ninja /usr/local/bin/
$ ./configure --with-fuse
$ wget http://mirror.centos.org/centos/8/BaseOS/x86_64/os/Packages/libaio-devel-0.3.112-1.el8.x86_64.rpm -o /tmp/
$ sudo rpm -ivh /tmp/
$ make -j8
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="挂载-blobfs">挂载 blobfs&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1">#iommu 设置&lt;/span>
&lt;span class="c1">#/etc/default/grub中，增加&lt;/span>
&lt;span class="nv">GRUB_CMDLINE_LINUX&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;xxx default_hugepagesz=1G hugepagesz=1G hugepages=16 hugepagesz=2M hugepages=2048 intel_iommu=on&amp;#34;&lt;/span>
$ grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg &lt;span class="c1">#uefi&lt;/span>
$ grub2-mkconfig -o /boot/grub2/grub.cfg &lt;span class="c1">#no uefi&lt;/span>
$ mkdir -p /mnt/huge1G
$ &lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;nodev /mnt/huge1G hugetlbfs pagesize=1GB 0 0&amp;#34;&lt;/span> &amp;gt;&amp;gt; /etc/fstab
&lt;span class="c1"># 设置hugepage&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">16&lt;/span> &amp;gt; /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
$ mount -t hugetlbfs none /mnt/huge1G -o &lt;span class="nv">pagesize&lt;/span>&lt;span class="o">=&lt;/span>1G
&lt;span class="c1"># 加载vfio-pci 驱动模块&lt;/span>
$ modprobe vfio-pci
$ modprobe
&lt;span class="c1"># 查看nvme 设备&lt;/span>
$ lspci -nn &lt;span class="p">|&lt;/span> grep &lt;span class="s2">&amp;#34;Non-Volatile&amp;#34;&lt;/span>
&lt;span class="c1">#&amp;gt; 84:00.0 Non-Volatile memory controller [0108]: Intel Corporation NVMe Datacenter SSD [3DNAND, Beta Rock Controller] [8086:0a54]&lt;/span>
&lt;span class="c1"># [[[[&amp;lt;domain&amp;gt;]:]&amp;lt;bus&amp;gt;]:][&amp;lt;device&amp;gt;][.[&amp;lt;func&amp;gt;]]：0000:84:00.0&lt;/span>
&lt;span class="c1"># [&amp;lt;vendor&amp;gt;]:[&amp;lt;device&amp;gt;]： 8086:0a54&lt;/span>
&lt;span class="c1"># 解绑nvme驱动&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;0000:84:00.0&amp;#34;&lt;/span> &amp;gt;/sys/bus/pci/devices/0000:84:00.0/driver/unbind
&lt;span class="c1"># 绑定到vfio驱动&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">8086&lt;/span> 0a54 &lt;span class="p">|&lt;/span> tee /sys/bus/pci/drivers/vfio-pci/new_id
&lt;span class="c1">#&amp;gt; 绑定成功后，检查&lt;/span>
&lt;span class="c1">#&amp;gt; - lsblk 看不到nvme盘&lt;/span>
&lt;span class="c1">#&amp;gt; - vfio下面多了一个设备文件：ls /dev/vfio/4&lt;/span>
&lt;span class="c1"># 或者绑定到uio&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">8086&lt;/span> 0a54 &amp;gt; /sys/bus/pci/drivers/uio_pci_generic/new_id
&lt;span class="c1"># 驱动准备完成&lt;/span>
$ &lt;span class="nb">cd&lt;/span> &amp;lt;SPDK_HOME&amp;gt;
&lt;span class="c1"># 生成nvme配置文件&lt;/span>
$ scripts/gen_nvme.sh --json-with-subsystems &amp;gt; nvme.json
&lt;span class="c1"># 查看状态&lt;/span>
$ sudo scripts/setup.sh status
&lt;span class="c1"># 设置hugepage&lt;/span>
$ &lt;span class="nv">HUGEMEM&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">5120&lt;/span> scripts/setup.sh
&lt;span class="c1"># 生成配置文件&lt;/span>
$ scripts/gen_nvme.sh --json-with-subsystems &amp;gt; nvme.json
&lt;span class="c1"># 初始化blobfs&lt;/span>
$ ./test/blobfs/mkfs/mkfs ./nvme.json Nvme0n1
&lt;span class="c1"># 挂载至/mnt/fuse&lt;/span>
$ ./test/blobfs/fuse/fuse ./nvme.json Nvme0n1 /mnt/fuse
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="源码解读">源码解读&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>lib&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>module/blobfs&lt;/code>目录&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>module&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// 检测bdev设备是否存在blobfs，若存在，则加载blobfs
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_detect&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1">//
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="k">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_create&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1">// 挂载
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="k">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_mount&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="kt">char&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">bdev_name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="kt">char&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">mountpoint&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">spdk_blobfs_bdev_op_complete&lt;/span> &lt;span class="n">cb_fn&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">cb_arg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="c1">// include 目录
&lt;/span>&lt;span class="c1">// spdk/include/blobfs.h
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// lib 目录
&lt;/span>&lt;span class="c1">// spdk/lib/blobfs.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// module目录
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev_rpc.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_fuse.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_fuse.h
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// test目录
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/blobfs.sh //blobfs 功能测试启动脚本
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/fuse/fuse.c
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/mkfs/mkfs.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// spdk/test/blobfs/rocksdb/
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="blobfs-patch">blobfs patch&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://review.spdk.io/gerrit/c/spdk/spdk/+/5420">https://review.spdk.io/gerrit/c/spdk/spdk/+/5420&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="patch">Patch&lt;/h2>
&lt;h3 id="5420-random-write">5420: random write&lt;/h3>
&lt;p>新 API&lt;code>spdk_file_randomwrite()&lt;/code>用于支持文件随机写。原有代码仅支持追加写，在追加写和随机读之间有一个内存缓存。&lt;/p>
&lt;p>为了支持随机写特性，需要进行如下工作：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>将缓存中的数据刷入到后端 blobstore 中；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除所有的缓存&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>BlobStore</title><link>https://justice.bj.cn/post/40.storage/spdk/blobstore/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/spdk/blobstore/</guid><description>&lt;h1 id="blobstore">BlobStore&lt;/h1>
&lt;hr>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>SPDK bdev层类似于内核中的通用块设备层，是对底层不同类型设备（如NVMe bdev、Malloc bdev、AIO bdev等）的统一抽象管理。&lt;/p>
&lt;p>BlobStore是位于SPDK bdev之上，通过不同层级的抽象，实现对磁盘块(LBA)的管理，实现了对Blob的管理，包括Blob的分配、删除、读取、写入、元数据的管理等；&lt;/p>
&lt;p>BlobFS是在Blobstore的基础上进行封装的一个轻量级文件系统，用于提供部分对于文件操作的接口，并将对文件的操作转换为对Blob的操作，&lt;/p>
&lt;p>用于与用户态文件系统Blobstore Filesystem （BlobFS）集成，从而代替传统的文件系统，支持更上层的服务，如数据库MySQL、K-V存储引擎Rocksdb以及分布式存储系统Ceph、Cassandra等。&lt;/p>
&lt;hr>
&lt;h2 id="blobstore-1">BlobStore&lt;/h2>
&lt;h3 id="数据块管理">数据块管理&lt;/h3>
&lt;p>在blobstore中，将SSD中的块划分为多个抽象层：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-54-17-2020-11-09-10-54-11-image.png" alt="">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Logical Block&lt;/strong>：与块设备中所提供的逻辑块相对应，通常为512B或4KiB。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Page&lt;/strong>：由多个连续的Logical Block构成，通常一个page的大小为4KiB，因此一个Page由八个或一个Logical Block构成，取决于Logical Block的大小。&lt;/p>
&lt;ul>
&lt;li>在Blobstore中，Page是连续的，即从SSD的LBA 0开始，多个或一个块构成Page 0,接下来是Page 1，依次类推。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cluster&lt;/strong>：由多个连续的Page构成，通常一个Cluster的大小默认为1MiB，因此一个Cluster由256个Page构成。&lt;/p>
&lt;ul>
&lt;li>Cluster与Page一样，是连续的，即从SSD的LBA 0开始的位置依次为Cluster 0到Cluster N。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Blob&lt;/strong>：Blobstore中主要的操作对象为Blob，与BlobFS中的文件相对应，提供read、write、create、delete等操作。&lt;/p>
&lt;ul>
&lt;li>一个Blob由多个Cluster构成，但构成Blob中的Cluster并不一定是连续的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="数据块管理-1">数据块管理&lt;/h2>
&lt;p>在Blobstore中，会将cluster 0作为一个特殊的cluster。&lt;/p>
&lt;p>该cluster用于存放Blobtore的所有信息以及元数据，对每个blob数据块的查找、分配都是依赖cluster 0中所记录的元数据所进行的。&lt;/p>
&lt;p>Cluster 0的结构如下：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-54-53-2020-11-09-10-54-48-image.png" alt="">&lt;/p>
&lt;p>Cluster 0中的第一个page作为super block，Blobstore初始化后的一些基本信息都存放在super block中，例如cluster的大小、已使用page的起始位置、已使用page的个数、已使用cluster的起始位置、已使用cluster的个数、Blobstore的大小等信息。&lt;/p>
&lt;hr>
&lt;h3 id="元数据">元数据&lt;/h3>
&lt;p>Cluster 0中的其它page将组成元数据域（metadata region）。元数据域主要由以下几部分组成：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-14-10-48-2020-11-09-14-10-36-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Metadata Page Allocation：用于记录所有元数据page的分配情况。在分配或释放元数据页后，将会对metadata page allocation中的数据做相应的修改。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Cluster Allocation：用于记录所有cluster的分配情况。在分配新的cluster或释放cluster后会对cluster allocation中的数据做相应的修改。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Blob Id Allocation：用于记录blob id的分配情况。对于blobstore中的所有blob，都是通过唯一的标识符blob id将其对应起来。在元数据域中，将会在blob allocation中记录所有的blob id分配情况。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Metadata Pages Region：元数据页区域中存放着每个blob的元数据页。每个blob中所分配的cluster都会记录在该blob的元数据页中，在读写blob时，首先会通过blob id定位到该blob的元数据页，其次根据元数据页中所记录的信息，检索到对应的cluster。对于每个blob的元数据页，并不是连续的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>对于一个blob来说，metadata page记录了该blob的所有信息，数据存放于分配给该blob的cluster中。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在创建blob时，首先会为其分配blob id以及metadata page，其次更新metadata region。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当对blob进行写入时，首先会为其分配cluster，其次更新该blob的metadata page，最后将数据写入，并持久化到磁盘中。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为了实现对磁盘空间的动态分配管理，Blobstore中为每个blob分配的cluster并不是连续的。&lt;/p>
&lt;p>对于每个blob，通过相应的结构维护当前使用的cluster以及metadata page的信息：clusters与pages。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Cluster: 记录了当前该blob所有cluster的LBA起始地址，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pages: 记录了当前该blob所有metadata page的LBA起始地址。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Blobstore实现了对磁盘空间分配的动态管理，并保证断电不丢失数据，具有persistent特性。&lt;/p>
&lt;p>Blobstore中的配置信息与数据信息均在super block与metadata region中管理，在重启后，若要保持persistent，可以通过Blobstore中所提供的load操作。&lt;/p>
&lt;p>&lt;strong>注意&lt;/strong>：&lt;/p>
&lt;blockquote>
&lt;p>Blob的persistent主要是针对NVMe这类bdev。对于Malloc bdev，由于其本身的性质，是无法保证Blob的persistent，需要重启后进行重新配置。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="blobfs">BlobFS&lt;/h2>
&lt;h3 id="blobfs-文件接口">BlobFS 文件接口&lt;/h3>
&lt;p>blobfs文件系统接口实现了基本的文件操作，&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>操作&lt;/th>
&lt;th>同步API&lt;/th>
&lt;th>异步API&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>打开文件&lt;/td>
&lt;td>spdk_fs_open_file&lt;/td>
&lt;td>spdk_fs_open_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>创建文件&lt;/td>
&lt;td>spdk_fs_create_file&lt;/td>
&lt;td>spdk_fs_create_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>删除文件&lt;/td>
&lt;td>spdk_fs_delete_file&lt;/td>
&lt;td>spdk_fs_delete_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>重命名文件&lt;/td>
&lt;td>spdk_fs_rename_file&lt;/td>
&lt;td>spdk_fs_rename_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>文件状态&lt;/td>
&lt;td>spdk_fs_file_stat&lt;/td>
&lt;td>spdk_fs_file_stat_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>写&lt;/td>
&lt;td>spdk_file_write&lt;/td>
&lt;td>spdk_file_write_async/sspdkfile_writev_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>读&lt;/td>
&lt;td>spdk_file_read&lt;/td>
&lt;td>spdk_file_read_async/sspdkfile_readv_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>truncate&lt;/td>
&lt;td>spdk_file_truncate&lt;/td>
&lt;td>spdk_file_truncate_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sync&lt;/td>
&lt;td>spdk_file_sync&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>关闭&lt;/td>
&lt;td>spdk_file_close&lt;/td>
&lt;td>spdk_file_close_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h3 id="缓存">缓存&lt;/h3>
&lt;p>为了提高文件的读取效率，BlobFS在内存中提供了cache buffer，由多层树结构组成，其结构如下所示：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-58-02-2020-11-09-10-56-44-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>最底层Level 0叶子节点为buffer node，是用于存放数据的buffer。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Level 0以上的其它层中，均为tree node，用于构建树的索引结构。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在文件读写的时候，根据文件结构中的根节点以及读取位置的offset信息，在树结构中通过索引查找buffer node的位置，即从Level N，逐步定位到对应的Level 0的叶子节点。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="写">写&lt;/h3>
&lt;p>BlobFS目前用于支持上层的Rocksdb，在Rocksdb的抽象环境层中提供文件的接口，目前仅支持append类型的写操作。&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-57-32-2020-11-09-10-57-04-image.png" alt="">&lt;/p>
&lt;hr>
&lt;p>在进行文件写入：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先会根据文件当前的写入位置检查是否符合cache buffer写入需求，若满足，则直接将数据写入到cache buffer中，同时触发异步的flush操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在flush的过程中，BlobFS触发Blob的写操作，将cache buffer中的数据，写入到文件对应blob的相应位置。若不满足cache buffer的写入需求，BlobFS则直接触发文件对应的blob的写操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Blobstore首先为该blob分配cluster，根据计算得到的写入LBA信息，向SPDK bdev层发送异步的写请求，将数据写入，并更新相应的元数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于元数据的更新，出于性能考虑，当前对元数据的更新都在内存中操作，当用户使用强制同步或卸载Blobstore时，更新后的元数据信息才会同步到磁盘中。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此外，blob结构中维护了两份可变信息（指cluster与metadata page）的元数据，分别为clean与active。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Clean中记录的是当前磁盘的元数据信息，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而active中记录的是当前在内存中更新后的元数据信息。同步操作会将clean中记录的信息与active记录的信息相匹配。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="读流程">读流程&lt;/h3>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-47-19-2020-11-04-09-45-33-image.png" alt="">&lt;/p>
&lt;hr>
&lt;p>在文件读写时：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先会进行read ahead操作，将一部分数据从磁盘预先读取到内存的buffer中；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>其后，根据cache buffer的大小，对文件的I/O进行切分，使每个I/O的最大长度不超过一个cache buffer的大小；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于拆分后的文件I/O，会根据其offset在cache buffer tree中查找相应的buffer；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>若存在，则直接从cache buffer中读取数据，进行memcpy；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而对于没有缓存到cache buffer中的数据，将会对该文件的读取，转换到该文件对应的Blob进行读取。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对Blob读取时候，根据已打开的blob结构中记录的信息，可以获取该blob所有cluster的LBA起始位置，并根据读取位置的offset信息，计算相应的LBA地址。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后向SPDK bdev层发送异步的读请求，并等待I/O完成。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BlobFS所提供的读操作为同步读，I/O完成后会在callback函数中，通过信号量通知BlobFS完成信号，至此文件读取结束。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="blobfs-fuse">BlobFS FUSE&lt;/h3>
&lt;p>BlobFS提供了一个FUSE插件，用于将SPDK BlobFS作为内核文件系统安装，以便进行检查或调试。FUSE插件需要fuse3，并在系统上检测到fuse3时自动构建。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">test/blobfs/fuse/fuse /usr/local/etc/spdk/rocksdb.conf Nvme0n1 /mnt/fuse
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="k">static&lt;/span> &lt;span class="k">struct&lt;/span> &lt;span class="n">fuse_operations&lt;/span> &lt;span class="n">spdk_fuse_oper&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">getattr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_getattr&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">readdir&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_readdir&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">mknod&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_mknod&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">unlink&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_unlink&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">truncate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_truncate&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">utimens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_utimens&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">open&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_open&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">release&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_release&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">read&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_read&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">write&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_write&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">flush&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_flush&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">fsync&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_fsync&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">rename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_rename&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">};&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="blobfs当前限制">BlobFS当前限制&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>现有BlobFS只在RocksDB上进行了测试，其他不同于RocksDB的文件系统使用场合可能会有问题，以后会进行更严格的测试；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>现在只支持同步操作API。异步API开发中，未经过严格测试，将于以后版本中完成；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>文件&lt;code>rename&lt;/code>API不是原子操作。将于未来版本修复；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当前不支持目录，只支持扁平的文件命名空间。文件名作为xattrs存储于blob中，文件名&lt;code>lookup&lt;/code>为&lt;code>O(n)&lt;/code>级。&lt;code>btree&lt;/code>版本目录支持实现将于未来版本支持；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当前&lt;code>write&lt;/code>操作仅支持append到文件末尾。任意位置写操作将在未来版本实现；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Blobstore实现对Blob管理，Blob类似与文件的概念，但又不完全等同于文件，Blob没有完全遵循文件的POSIX接口，因此避免与文件混淆，在SPDK中称之为Blob而不是File。&lt;/p>
&lt;p>Blobstore Filesystem （BlobFS）是基于Blobstore实现的轻量级文件系统，对Blobstore进行封装，提供一些文件的常用接口，如read、write、open、sync等，其目的在于作为文件系统支持更上层的应用，例如Rocksdb。但其本质仍然是Blobstore，因此命名为BlobFS。&lt;/p>
&lt;p>目前SPDK基于维护了Rocksdb的一个分支，该分支下的Rocksdb在环境抽象层主要通过BlobFS进行对接，I/O可以经由BlobFS绕过内核I/O栈。&lt;/p>
&lt;hr>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://spdk.io/doc/blobfs.html">SPDK: BlobFS (Blobstore Filesystem)&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.sdnlab.com/22880.html">https://www.sdnlab.com/22880.html&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.atzlinux.com/atzlinux/doc/os2atc2019/SPDK-bytedance-miaoyu.pdf">SPDK在字节跳动存储业务中的应⽤&lt;/a>&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>BlueStore</title><link>https://justice.bj.cn/post/40.storage/ceph/ceph-bluestore/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/ceph/ceph-bluestore/</guid><description>&lt;h1 id="bluestore">BlueStore&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Ceph早期的单机对象存储引擎是&lt;code>FileStore&lt;/code>，为了维护数据的一致性，写入之前数据会先写&lt;code>Journal&lt;/code>，然后再写到文件系统，会有一倍的写放大，而同时现在的文件系统一般都是日志型文件系统(ext系列、xfs)，文件系统本身为了数据的一致性，也会写&lt;code>Journal&lt;/code>，此时便相当于维护了两份&lt;code>Journal&lt;/code>；另外&lt;code>FileStore&lt;/code>是针对&lt;code>HDD&lt;/code>的，并没有对&lt;code>SSD&lt;/code>作优化，随着&lt;code>SSD&lt;/code>的普及，针对&lt;code>SSD&lt;/code>优化的单机对象存储也被提上了日程，&lt;code>BlueStore&lt;/code>便由此应运而出。&lt;/p>
&lt;p>&lt;code>BlueStore&lt;/code>最早在&lt;code>Jewel&lt;/code>版本中引入，用于在&lt;code>SSD&lt;/code>上替代传统的&lt;code>FileStore&lt;/code>。作为新一代的高性能对象存储后端，&lt;code>BlueStore&lt;/code>在设计中便充分考虑了对&lt;code>SSD&lt;/code>以及&lt;code>NVME&lt;/code>的适配。针对&lt;code>FileStore&lt;/code>的缺陷，&lt;code>BlueStore&lt;/code>选择绕过文件系统，直接接管裸设备，直接进行对象数据IO操作，同时元数据存放在&lt;code>RocksDB&lt;/code>，大大缩短了整个对象存储的IO路径。&lt;code>BlueStore&lt;/code>可以理解为一个支持&lt;code>ACID&lt;/code>事物型的本地日志文件系统。&lt;/p>
&lt;h2 id="为什么需要bluestore">为什么需要BlueStore&lt;/h2>
&lt;p>ceph是目前业内比较普遍使用的开源分布式存储系统，实现有多种类型的本地存储系统；在较早的版本当中，ceph默认使用FileStore作为后端存储，但是由于FileStore存在一些缺陷，重新设计开发了BlueStore，并在L版本之后作为默认的后端存储。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-04d2616b034660107a7fadccd7ea4fa0_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>IO放大&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>FileStore底层使用POSIX规范的文件系统接口，例如xfs、ext4、btrfs，然而这类文件系统本身不支持数据或元数据的事务操作接口（btrfs提供事务钩子的接口，但是测试过程中发现会导致系统宕机），而ceph对于数据写入要求十分严格，需要满足事务的特性（ACID）；为此FileStore实现了FileJournal功能，所有的事务都需要先写到FileJournal中，之后才会写入对应的文件中，以此来保证事务的原子性，但是这导致了数据“双写”的问题，造成至少一半磁盘带宽的浪费。&lt;/p>
&lt;p>此外xfs、ext4、btrfs这类文件系统本身存在一定的IO放大（即一次读写请求实际在低层磁盘发生的IO次数），再加上FileStore的日志双写，放大倍数成倍增加。&lt;/p>
&lt;p>下图中的数据表示了以block大小为单位对不同文件系统进行读写，在不同场景下的读写放大及空间放大情况。我们以ext4文件系统说明下个参数的含义。在对文件进行Overwrite时，即将数据覆盖写入到文件中，除了写入数据外，还涉及到日志的写入（其中日志写入两次，一次记录更改的inode，一次为commit记录，具体可参考[5])、文件inode的更改，每次写的最小单位是block，因此最终相当于写入次数以及空间放大了四倍；而在进行Append写入时，由于需要新分配空间，因此相对于Overwrite增加了bitmap的更改以及superblock的更改（superblock记录总的空间分配情况），写放大和空间放大均为六倍。读文件时，在没有命中任何缓存的情况下（cold cache），需要读大量元数据，例如：目录、文件inode、superblock等，最终读放大为六倍；而如果是在顺序读的情况下（warm cache），像superblock、bitmap、目录等这些元数据都缓存在内存中，只需读取文件inode和文件数据。&lt;/p>
&lt;p>同理，其他文件系统由于不同的结构和设计原理，其IO放大和空间放大系数也各不相同。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-39087a1897ba0faa3597ca786d579471_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>对象遍历&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>ceph的数据被划分为object存放，object以32位的hash值进行标识，ceph在进行scrubbing、backfill或者recovery时都需要根据hash值遍历这些object；POSIX文件系统不提供有序的文件遍历接口，为此FileStore根据文件的数量和hash的前缀将object划分到不同的子目录，其原则如下：&lt;/p>
&lt;ul>
&lt;li>当目录下的文件个数&amp;gt;100个时，拆分子目录；目录名以文件名的hash前缀为依据（拆分一级目录时，以hash第一位为拆分依据，二级目录以第二位hash为拆分依据，依次类推）&lt;/li>
&lt;li>当所有子目录下的文件个数&amp;lt;50个时，将合并到上级目录&lt;/li>
&lt;/ul>
&lt;p>因此FileStore在使用过程中需要不断合并拆分目录结构；这种方式将文件按照前缀放到不同目录，但对于同一目录中的文件依然无法很好排序，因此需要将目录中的所有文件读到内存进行排序，这样在一定程度上增加了CPU开销。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-3c8f6b97fea136794ccb7cb2a1f3d006_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>其他&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>FileStore由于设计的较早，无法支持当前较新的存储技术，例如使用spdk技术读写NVMe盘。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据和元数据分离不彻底。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>流控机制不完整导致IOPS和带宽抖动（FileStore自身无法控制本地文件系统的刷盘行为）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>频繁syncfs系统调用导致CPU利用率居高不下。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bluestore介绍">&lt;strong>BlueStore介绍&lt;/strong>&lt;/h2>
&lt;h2 id="需求">&lt;strong>需求&lt;/strong>&lt;/h2>
&lt;p>首先看下BlueStore设计之初的一些需求：&lt;/p>
&lt;ul>
&lt;li>对全SSD及全NVMe SSD闪存适配&lt;/li>
&lt;li>绕过本地文件系统层，直接管理裸设备，缩短IO路径&lt;/li>
&lt;li>严格分离元数据和数据，提高索引效率&lt;/li>
&lt;li>使用KV索引，解决文件系统目录结构遍历效率低的问题&lt;/li>
&lt;li>支持多种设备类型&lt;/li>
&lt;li>解决日志“双写”问题&lt;/li>
&lt;li>期望带来至少2倍的写性能提升和同等读性能&lt;/li>
&lt;li>增加数据校验及数据压缩等功能&lt;/li>
&lt;/ul>
&lt;h2 id="逻辑架构">&lt;strong>逻辑架构&lt;/strong>&lt;/h2>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-c3c40bb427a8a562eed3d2040b5f8a5e_1440w.jpg" alt="">&lt;/p>
&lt;p>BlueStore的逻辑架构如上图所示，模块的划分都还比较清晰，我们来看下各模块的作用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>RocksDB&lt;/strong>：rocksdb是facebook基于leveldb开发的一款kv数据库，BlueStore将元数据全部存放至RocksDB中，这些元数据包括存储预写式日志、数据对象元数据、Ceph的omap数据信息、以及分配器的元数据 。&lt;/li>
&lt;li>&lt;strong>BlueRocksEnv&lt;/strong>：这是RocksDB与BlueFS交互的接口；RocksDB提供了文件操作的接口EnvWrapper，用户可以通过继承实现该接口来自定义底层的读写操作，BlueRocksEnv就是继承自EnvWrapper实现对BlueFS的读写。&lt;/li>
&lt;li>&lt;strong>BlueFS&lt;/strong>：BlueFS是BlueStore针对RocksDB开发的轻量级文件系统，用于存放RocksDB产生的.sst和.log等文件。&lt;/li>
&lt;li>&lt;strong>BlockDecive&lt;/strong>：BlueStore抛弃了传统的ext4、xfs文件系统，使用直接管理裸盘的方式；BlueStore支持同时使用多种不同类型的设备，在逻辑上BlueStore将存储空间划分为三层：慢速（Slow）空间、高速（DB）空间、超高速（WAL）空间，不同的空间可以指定使用不同的设备类型，当然也可使用同一块设备，具体我们会在后面的文章进行说明。&lt;/li>
&lt;li>&lt;strong>Allocator&lt;/strong>：负责裸设备的空间管理，只在内存做标记，目前支持StupidAllocator和BitmapAllocator两种分配器,Stupid基于extent的方式实现 。&lt;/li>
&lt;/ul>
&lt;h2 id="设计思想">&lt;strong>设计思想&lt;/strong>&lt;/h2>
&lt;p>在设计分布式文件系统的本地存储时，我们必须考虑数据的一致性和可靠性。在数据写入的过程中，由于可能存在异常掉电、进程崩溃等突发情况，导致数据还未全部写入成功便结束。虽然硬盘本身可以保证在扇区级别写入的原子性，但是一般文件系统的一个写请求通常包含多个扇区的数据和元数据更新，无法做到原子写。&lt;/p>
&lt;p>常用的解决办法是引入日志系统，数据写入磁盘之前先写到日志系统，然后再将数据落盘；日志写入成功后，即便写数据时出现异常，也可以通过日志回放重新写入这部分数据；如果写日志的过程中出现异常，则直接放弃这部分日志，视为写入失败即可，以此保证原子写入。但是这种方式导致每份数据都需要在磁盘上写入两次，严重降低了数据的写入效率。&lt;/p>
&lt;p>另一种方式则是采用ROW（Redirect on write）的方式，即数据需要覆盖写入时，将数据写到新的位置，然后更新元数据索引，这种方式由于不存在覆盖写，只需保证元数据更新的原子性即可。对于对齐的覆盖写入时，这种方式没有问题，但是如果是非对齐的覆盖写呢？&lt;/p>
&lt;p>我们举个例子：某文件的逻辑空间 [0,4096) 区间的数据在磁盘上的物理映射地址为[0, 4096)，磁盘的块（即磁盘读写的最小单元）大小为4096；如果要覆盖写文件[0,4096)区间的数据，那使用ROW的方式没有问题，重新再磁盘上分配一个新的块写入，然后更新元数据中的映射关系即可；但是如果写文件[512,4096)区域，也就是非对齐的覆盖写时，新分配的块中只有部分数据，旧的物理空间中仍有部分数据有效，这样元数据中需要维护两份索引，而且在读取文件的该块数据时，需要从多块磁盘块中读取数据，如果多次进行非对齐覆盖写，这种问题将更严重。&lt;/p>
&lt;p>解决这种问题办法是使用RMW（Read Modify Write）的方法，即在发生非对齐覆盖写时，先读取旧的数据，更新的数据合并后，对齐写入到磁盘中，从而减少元数据、提高读性能，但这种方式也存在一种缺点，写数据时需要先读数据，存在一定的性能损耗。&lt;/p>
&lt;p>分析完ROW的方式后，读者是否会有疑问，每次写入都放到新的位置，那么文件在磁盘中的物理连续性岂不是无法保证？的确，在传统的文件系统设计时，都是面向HDD盘，这种类型的盘在读写时会有磁头寻道的时间，对于非连续的物理空间读写，性能极差，在设计时会尽可能考虑数据存放的连续性，因此很少会采用ROW的方式。但是随着SSD盘的逐渐普及，随机读写的性能不再成为主要的性能关注点，越来越多的存储系统开始采用全闪存的磁盘阵列，相信ROW的方式会成为更加主流的方式。&lt;/p>
&lt;p>我们再来看下BlueStore是怎么实现的，BlueStore在设计时便考虑了全闪存的磁盘阵列，但是仍要考虑使用HDD盘的场景，因此并未完全采用ROW的方式。&lt;/p>
&lt;p>我们以下图为例进行说明，BlueStore提供了一个最小分配单元min_alloc_size的配置项，一般为磁盘块大小的整数倍，在此例中min_alloc_size为block大小的4倍。&lt;/p>
&lt;p>写入的数据如果与min_alloc_size大小对齐，则使用ROW的方式，将数据写到新的地址空间，然后更改元数据索引，并回收原先占用的空间，元数据更新的原子性由RocksDB的事务特性进行保障。&lt;/p>
&lt;p>而对于非min_alloc_size对齐的区域，则使用RMW的方式进行原地覆盖写（只读取非块大小对齐区域所在块，一般就是写入数据的第一个或最后一个块），写入的这部分数据可能跨多个块（因为min_alloc_size是块大小的整数倍），而磁盘只保证单个块大小的原子写入，对于多个块的原子写需要引入类似日志的功能，BlueStore用RocksDB来实现日志功能，将覆盖的这部分数据记到RocksDB中，完成以后再将数据覆盖写入到实际的数据区域，落盘成功以后再删除日志中的记录。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-206c24ef8e2e39e070256e2478a8f676_1440w.jpg" alt="">&lt;/p>
&lt;p> bluestore不使用本地文件系统，直接接管裸设备，并且只使用一个原始分区，HDD/SSD所在的物理块设备实现在用户态下使用linux aio直接对裸设备进行I/O操作。由于操作系统支持的aio操作只支持directIO，所以对BlockDevice的写操作直接写入磁盘，并且需要按照page对齐。其内部有一个aio_thread 线程，用来检查aio是否完成。其完成后，通过回调函数aio_callback 通知调用方。&lt;/p>
&lt;h2 id="存储模型">存储模型&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/03-14-38-09-2020-11-03-14-37-46-image.png?token=AARMIEXD2SMH33GE2RKURI27UD5RA" alt="">&lt;/p>
&lt;h3 id="缓存模块">缓存模块&lt;/h3>
&lt;p>BlueStore抛弃了文件系统，直接管理裸设备，用不了文件系统的Cache机制，自己实现元数据和数据的Cache。&lt;/p>
&lt;p>&lt;code>BlueStore&lt;/code>有两种Cache算法：&lt;code>LRU&lt;/code>和&lt;code>2Q&lt;/code>。元数据使用&lt;code>LRU&lt;/code>Cache策略，数据使用&lt;code>2Q&lt;/code>Cache策略。&lt;/p>
&lt;p>Bluestore实现了自己的缓存机制，定义了structure ：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>OnodeSpace，用来map 到内存中的ONODE；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BufferSpace，用来map 块信息blob，每个blob都在bufferSpace中缓存了状态数据。&lt;/p>
&lt;p>二者在缓存中依照LRU的方式决定生命周期。 &lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="freelistmanager模块">FreelistManager模块&lt;/h3>
&lt;p>FreelistManager用来映射磁盘的使用信息，最初实现是采用k-v的方式来存储对应的磁盘块的使用情况，但是由于更新数据时需要修改映射，需要线程锁来控制修改，而且这种方式对内存消耗很大；后续修改为bitmap的映射方式，设定一个offset来以bitmap的方式map多个block使用信息，使用XOR计算来更新块的使用情况，这种方式不会出现in-memory 状态。 &lt;/p>
&lt;h3 id="allocator模块">Allocator模块&lt;/h3>
&lt;p>用来委派具体哪个实际存储块用来存储当前的object数据；同样采用bitmap的方式来实现allocator，同时采用层级索引来存储多种状态，这种方式对内存的消耗相对较小，平均1TB磁盘需要大概35M左右的ram空间&lt;/p>
&lt;h2 id="bluestore的元数据管理">&lt;strong>BlueStore的元数据管理&lt;/strong>&lt;/h2>
&lt;p>bluestore自己管理裸盘，因此需要有元数据来管理对象，对应的就是Onode，Onode是常驻内存的数据结构，持久化的时候会以kv的形式存到rocksdb里。&lt;/p>
&lt;p>在onode里又分为lextent，表示逻辑的数据块，用一个map来记录，一个onode里会存在多个lextent，lextent通过blob的id对应到blob（bluestore_blob_t ），blob里通过pextent对应到实际物理盘上的区域（pextent里就是offset和length来定位物理盘的位置区域）。一个onode里的多个lextent可能在同一个blob里，而一个blob也可能对应到多个pextent。&lt;/p>
&lt;p>另外还有Bnode这个元数据，它是用来表示多个object可能共享extent，目前在做了快照后写I/O触发的cow进行clone的时候会用到。&lt;/p>
&lt;p>Onode代表对象，名字大概是从Linux VFS的Inode沿袭过来的。Onode常驻内存，在RocksDB中以KeyValue形式持久化；关于内存Cache的结构，在CDM的Slides中有讲。Onode包含多个lextent，即逻辑extent。Blob通过映射pextent、即物理extent，映射到磁盘上的物理区域。Blob通常包括来自同一个对象的多段数据，但是也可能被其它对象引用。Bnode是对象快照后，被用于多个对象共享数据的。&lt;/p>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/code/github/gitnote/img/2020-11-03-13-53-30-image.png" alt="">&lt;/p>
&lt;p>上面仅是关于对象映射的。更进一步，RocksDB中存储有许多类型的元数据，包括块分配、对象集合、快照、延迟写（Deferred Writes）、对象属性（Omap，即一个对象上可以附加一些KeyValue对作为属性，例如给图片加上地点、日期等），等等。在CDM的Slides中有详述。&lt;/p>
&lt;h2 id="bluestore的写路径">&lt;strong>BlueStore的写路径&lt;/strong>&lt;/h2>
&lt;p>写路径包含了对事务的处理，也回答了BlueStore如何解决日志双写问题。&lt;/p>
&lt;p>首先，Ceph的事务只工作于单个OSD内，能够保证多个对象操作被ACID地执行，主要是用于实现自身的高级功能。每个PG（Placement Group，类似Dynamo的vnode，将hash映射到同一个组内的对象组到一起）内有一个OpSequencer，通过它保证PG内的操作按序执行。事务需要处理的写分三种：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>写到新分配的区域。考虑ACID，因为此写不覆盖已有数据，即使中途断电，因为RocksDB中的元数据没有更新，不用担心ACID语义被破坏。后文可见RocksDB的元数据更新是在数据写之后做的。因而，日志是不需要的。在数据写完之后，元数据更新写入RocksDB；RocksDB本身支持事务，元数据更新作为RocksDB的事务提交即可。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>写到Blob中的新位置。同理，日志是不需要的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Deferred Writes（延迟写），只用于覆写（Overwrite）情况。从上面也可以看到，只有覆写需要考虑日志问题。如果新写比块大小（min_alloc_size）更小，那么会将其数据与元数据合并写入到RocksDB中，之后异步地把数据搬到实际落盘位置；这就是日志了。如果新写比块大小更大，那么分割它，整块的部分写入新分配块中，即按（1）处理，；不足的部分按（3）中上种情况处理。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>上述基本概述了BlueStore的写处理。可以看到其是如何解决FileStore的日志双写问题的。首先，没有Linux文件系统了，也就没有了多余的Journaling of Journal问题。然后，大部分写是写到新位置的，而不是覆写，因此不需要对它们使用日志；写仍然发生了两次，第一次是数据落盘，然后是RocksDB事务提交，但不再需要在日志中包含数据了。最后，小的覆写合并到日志中提交，一次写完即可返回用户，之后异步地把数据搬到实际位置（小数据合并到日志是个常用技巧）；大的覆写被分割，整块部分用Append-only方式处理，也绕开了日志的需要。至此，成为一个自然而正常的处理方式。（P.S.总之，个人感觉日志双写不是一个该存在的问题，不知为何成了一个问题，好在今天终于不是问题了。）&lt;/p>
&lt;p>更深入地，Ceph的开发文档中列出了所有的写策略处理方式。可以看到Inline Compression也是BlueStore的功能点之一；其中也有对Partial-write问题的处理。&lt;/p>
&lt;p>CDM的Slides中有BlueStore写的状态机图。状态机是存储中常用的处理方式，处理写路径，Ceph的PG Peering过程也有相应的状态机。数据落盘，对应的是PREPARE-&amp;gt;AIO_WAIT间的“Initiate some AIO”一步。之后经过多个队列，向RocksDB提交事务，以及完成Deferred Write和Cleanup。直到最终完成。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/03-13-56-33-2020-11-03-13-56-11-image.png?token=AARMIEVN2WIMILBRLCACEIC7UDYU6" alt="">&lt;/p>
&lt;p>另外，BlueStore使用Direct IO提交数据，这样数据会立即落盘，而不是在内核中缓存；从而，存储系统可以完全自主地控制写的持久化。这是一个如今常见的做法。但代价是，不能利用内核缓存，需要自己处理缓存问题；也必须处理好数据对齐，以及写小于一扇区时的Partial-write问题。&lt;/p>
&lt;h2 id="bluefs的架构">BlueFS的架构&lt;/h2>
&lt;p>BlueFS以尽量简单为目的设计，专门用于支持RocksDB；RocksDB总之还是需要一个文件系统来工作的。BlueFS不支持POSIX接口。总的来说，它有这些特点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>目录结构方面，BlueFS只有扁平的目录结构，没有树形层次关系；用于放置RocksDB的db.wal/，db/，db.slow/文件。这些文件可以被挂载到不同的硬盘上，例如db.wal/放在NVMRAM上；db/包含热SST数据，放在SSD上；db.slow/放在磁盘上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据写入方面，BlueFS不支持覆写，只支持追加（Append-only）。块分配粒度较粗，越1MB。有垃圾回收机制定期处理被浪费掉的空间。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对元数据的操作记录到日志，每次挂载时重放日志，来获得当前的元数据。元数据生存在内存中，并没有持久化在磁盘上，不需要存储诸如空闲块链表之类的。当日志过大时，会进行重写Compact。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Superblock用于存储整个文件系统级别的元数据，日志和数据本着尽量简单的设计，按照追加的方式不断写入。关于写放大的问题，这是Append-only式通有的，在Write Behaviors论文中有详述。&lt;/p>
&lt;h2 id="总结">&lt;strong>总结&lt;/strong>&lt;/h2>
&lt;p>BlueStore的设计考虑了FileStore中存在的一些硬伤，抛弃了传统的文件系统直接管理裸设备，缩短了IO路径，同时采用ROW的方式，避免了日志双写的问题，在写入性能上有了极大的提高。&lt;/p>
&lt;p>通过分析BlueStore的基本结构、考虑的问题以及设计思想，我们对于BlueStore有了大概的了解；BlueStore在设计时有考虑到未来存储的应用环境，是一种比较先进的本地文件系统，但也不可避免存在一些缺陷，例如较为复杂的元数据结构和IO逻辑，在大量小IO下可能存在的double write问题，较大的元数据内存占用等（当然有些问题在ceph的使用场景下可能不存在，但是我们如果希望借鉴BlueStore来设计本地文件系统就不得不考虑这些问题）。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/news/45599">Ceph BlueStore 和双写问题 - 云+社区 - 腾讯云&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/ceph/bluestore.html">https://cloud-atlas.readthedocs.io/zh_CN/latest/ceph/bluestore.html&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.cnblogs.com/fengjian2016/p/9747689.html">Ceph的BlueStore总体介绍 - fengjian1585 - 博客园&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/46362124">https://zhuanlan.zhihu.com/p/46362124&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/91015613">https://zhuanlan.zhihu.com/p/91015613&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.51cto.com/wendashuai/2500499">BlueStore源码分析之对象IO-Darren_Wen-51CTO博客&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.codeleading.com/article/8882498924/">ceph bluestore 磁盘空间管理源码解析 - 代码先锋网&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/68067068">https://zhuanlan.zhihu.com/p/68067068&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/91018497">BlueStore源码分析之BitMap分配器&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://docs.google.com/presentation/d/1_1Otkgv76fbCU2Zogjz748sEAG-1Nfiidbb6mgTON-A/edit#slide=id.p">https://docs.google.com/presentation/d/1_1Otkgv76fbCU2Zogjz748sEAG-1Nfiidbb6mgTON-A/edit#slide=id.p&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.e-learn.cn/topic/3434557">BlueStore源码分析之FreelistManager | 易学教程&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>BookKeeper</title><link>https://justice.bj.cn/post/30.architech/bookkeeper/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/bookkeeper/</guid><description>&lt;h1 id="bookkeeper">BookKeeper&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>BookKeeper是一个可靠的日志流记录系统，用于将系统产生的日志(也可以是其他数据)记录在BookKeeper集群上，由BookKeeper这个第三方Storage保证数据存储的可靠和一致性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>典型场景是系统写write-ahead log，即先把log写到BookKeeper上，BookKeeper诞生于Hadoop2.0的namenode HA，由yahoo于2009年创建，并在2011年开源。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/13-09-49-25-2019-12-10-19-03-10-image.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/13-09-49-44-2019-12-10-18-03-31-image.png" alt="Apache BookKeeper 架构图">&lt;/p>
&lt;h2 id="复制">复制&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>BookKeeper对所有数据都会复制和存储相同的多份拷贝——一般是三份或是五份——到不同的机器上，可以是同一数据中心，也可以是跨数据中心。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不像其他使用主/从或是管道复制算法在副本之间复制数据的分布式系统（例如Apache HDFS、Ceph、Kafka），Apache BookKeeper使用一种多数投票并行复制算法在确保可预测的低延时的基础上复制数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>BookKeeper复制基于以下核心思想：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>日志流的原子结构是记录而不是字节。也就是说，数据总是以不可分割的记录形式（包括了元数据）存放的，而不是一个个字节组成的数组。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志流中记录的顺序与实际记录的实际存储是解耦的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>一个 &lt;strong>BookKeeper 集群&lt;/strong>包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Bookies：一组独立的存储服务器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>元数据存储&lt;/strong>系统：用于服务发现和元数据管理&lt;/p>
&lt;p>BookKeeper 客户端可以使用较高级别的 DistributedLog API（也称为&lt;strong>日志流 API&lt;/strong>）或较低级别的** ledger API**。Ledger API 允许用户直接与 bookies 交互。下图即为 BookKeeper 安装的典型示例。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkHv9TACJCDicQzzDs25hTzvIgQ4axO43jbDOj4pFdicP1dgEhPwmUWnSg/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>典型的 BookKeeper 安装（通过多个 API 连接的应用程序）&lt;/p>
&lt;hr>
&lt;p>&lt;strong>流存储要求&lt;/strong>&lt;/p>
&lt;p>在上篇文章中已经提到，实时存储平台应该&lt;strong>同时&lt;/strong>满足以下要求：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>即使在强持久性条件下&lt;/strong>，客户端也能够以极低的延迟（小于 5 毫秒）读写 entry 流&lt;/p>
&lt;/li>
&lt;li>
&lt;p>能够持久、一致、容错地存储数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在写入时，客户端能够进行流式传输或追尾传输&lt;/p>
&lt;/li>
&lt;li>
&lt;p>有效存储数据，支持访问历史数据与实时数据&lt;/p>
&lt;p>BookKeeper 通过提供以下保证来&lt;strong>同时&lt;/strong>满足上述各项要求：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkPUekjeqBcL7WaPz8S1zgIcqW2pTDdaG0aOzQfp26Uyo2fJiaZrE2b8A/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>&lt;strong>多副本&lt;/strong>&lt;/p>
&lt;p>BookKeeper 在一个数据中心内的多个机器上，或是多个数据中心之间，复制每条数据记录并存储多个副本（通常是 3 个或 5 个副本）。&lt;/p>
&lt;p>一些分布式系统使用主/从或管道复制算法在副本之间复制数据（例如，Apache HDFS、Ceph、Kafka 等），BookKeeper 的不同之处在于使用 &lt;strong>quorum-vote 并行复制算法&lt;/strong>来复制数据，以确保可预测的低延迟。下图即为 BookKeeper 集成中的多副本。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkZNRjkWuaYuybA5bYIgbia5IGCScA5TnMYiaRsaWZdIeKwZLia4TOQpGag/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">BookKeeper 多副本中的 ensemble、写入、ack quorum&lt;/p>
&lt;p>在上图中：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;p>从 BookKeeper 集群中（自动）选择一组 bookies（图例中为 bookies 1-5）。这一组 bookies 即为给定 **ledger **上用于存储数据记录的 &lt;strong>ensemble&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ledger 中的数据分布在 bookies 的 ensemble 中。也就是说，每条记录都存有多个副本。用户可以在客户端级别配置副本数，即&lt;strong>写入 quorum 大小&lt;/strong>。在上图中，写入 quorum 大小为 3，即记录写入到 bookie 2、bookie 3 与 bookie 4。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>客户端向 ensemble 中写入数据记录时，需要等待直至有指定数量的副本发送确认（ack）。副本数即为 &lt;strong>ack quorum 大小&lt;/strong>。接收到指定数量的 ack 后，客户端默认写入成功。在上图中，ack quorum 大小为 2，也就是说，比如 bookie 3 和 bookie 4 存储数据记录，则向客户端发送一条确认。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当 bookie 发生故障时，ensemble 的组成会发生变化。正常的 bookies 会取代终止的 bookies，这种取代可能只是暂时的。例如：如果 &lt;strong>Bookie 5&lt;/strong> 终止，&lt;strong>Bookie x&lt;/strong> 可能会取代它。&lt;/p>
&lt;p>&lt;strong>多副本：核心理念&lt;/strong>&lt;/p>
&lt;p>BookKeeper 多副本基于以下核心理念：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志流面向记录而不是面向字节。这意味着，数据总是存储为不可分割的记录（包括元数据），而不是存储为单个字节数组。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志（流）中记录的顺序与记录副本的实际存储顺序分离。&lt;/p>
&lt;p>这两个核心理念确保 BookKeeper 多副本能够实现以下几项功能：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>为向 bookies 写入记录提供多种选择，从而确保即使集群中多个 bookies 终止或运行缓慢，写入操作仍然可以完成（只要有足够的容量来处理负载）。可以通过改变 ensemble 来实现。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过增加 ensemble 大小来最大化单个日志（流）的带宽，以使单个日志不受一台或一小组机器的限制。可以通过将 ensemble 大小配置为大于写入 quorum 大小来实现。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过调整 ack quorum 大小来改善追加时的延迟。这对于确保 BookKeeper 的低延迟十分重要，同时还可以提供一致性与持久性保证。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过多对多副本恢复提供快速&lt;strong>再复制&lt;/strong>（再复制为复制不足的记录创建更多副本，例如：副本数小于写入 quorum 大小）。所有的 bookies 都可以作为记录副本的提供者&lt;em>与&lt;/em>接受者。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="持久性">持久性&lt;/h2>
&lt;p>保证复制每条写入 BookKeeper 的数据记录，并持久化到指定数量的 bookies 中。可以通过使用磁盘 fsync 和写入确认来实现。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>在单个 bookie 上，将确认发送给客户端之前，数据记录已明确写入（启用 fsync）磁盘，以便在发生故障时能够持久保存数据。这样可以保证数据写入到持久化存储中不依赖电源，可以被重新读取使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在单个集群内，复制数据记录到多个 bookies，以实现容错。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>仅当客户端收到指定数量（通过 ack quorum 大小指定）的 bookies 响应时，才 ack 数据记录。&lt;/p>
&lt;p>最新的 NoSQL 类型数据库、分布式文件系统和消息系统（例如：Apache Kafka）都假定：保证最佳持久化的有效方式是将数据复制到多个节点的内存中。但问题是，这些系统允许潜在的数据丢失。&lt;/p>
&lt;p>&lt;strong>BookKeeper 旨在提供更强的持久性保证，完全防止数据丢失，从而满足企业的严格要求。&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="一致性">一致性&lt;/h3>
&lt;p>保证一致性是分布式系统中的常见问题，尤其是在引入多副本以确保持久性和高可用时。BookKeeper 为存储在日志中的数据提供了简单而强大的一致性保证（可重复读取的一致性）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果记录已被引用程序 ack，则必须&lt;strong>立即&lt;/strong>可读。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果记录被读取一次，则必须&lt;strong>始终&lt;/strong>可读。如果记录 &lt;strong>R&lt;/strong> 成功写入，则在 &lt;strong>R&lt;/strong> 之前的所有记录都已成功提交/保存，并且将始终可读。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在不同读者之间，存储记录的顺序必须完全相同且可重复。&lt;/p>
&lt;p>这种可重复读取的一致性由 BookKeeper 中的 LastAddConfirmed（LAC）协议实现。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="高可用">高可用&lt;/h3>
&lt;p>在 CAP（Consistency：一致性、Availability：高可用、Partition tolerance：分区容错）条件下，BookKeeper 是一个 CP 系统。&lt;/p>
&lt;p>但实际上，即使存在硬件、网络或其他故障，Apache BookKeeper 仍然可以提供高可用性。为保证写入与读取的高可用性能，BookKeeper 采用了以下机制：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkkeiauEpNQvI7UticfwQkH4NGerkEJkgLnEIO8Wpd0wEicOLX36SrhVN4Q/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="低延迟">低延迟&lt;/h3>
&lt;p>强持久性和一致性是分布式系统的复杂问题，特别是当分布式系统还需要满足企业级低延迟时。BookKeeper 通过以下方式满足这些要求：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在单个 bookie 上，bookie 服务器旨在用于不同工作负载（写入、追尾读、追赶读/随机读）之间的I/O 隔离。在 journal 上部署 &lt;strong>group-committing 机制&lt;/strong>以平衡延迟与吞吐量。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用 &lt;strong>quorum-vote 并行复制 schema&lt;/strong> 缓解由于网络故障、JVM 垃圾回收暂停和磁盘运行缓慢引起的延迟损失。这样不仅可以改善追尾延迟，还能保证可预测的 p99 低延迟。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用长轮询机制在 ack 并确认新记录后，立刻向追尾的写入者发出通知并发送记录。&lt;/p>
&lt;p>最后，值得一提的是，明确 fsync 和写入确认的持久性与可重复的读取一致性对于状态处理（尤其是流应用程序的 effectively-once 处理）非常重要。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="http://matt33.com/2019/01/28/bk-store-realize/">http://matt33.com/2019/01/28/bk-store-realize/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/news/339495">为何选择Apache BookKeeper？第一部分：一致性、持久性以及可用性 - 云+社区 - 腾讯云&lt;/a>&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>Ceph基础</title><link>https://justice.bj.cn/post/40.storage/ceph/ceph/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/ceph/ceph/</guid><description>&lt;h1 id="ceph基础">Ceph基础&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Ceph 最初是一项关于存储系统的 PhD 研究项目，由 Sage Weil 在 University of California, Santa Cruz（UCSC）实施。&lt;/p>
&lt;h2 id="特性">特性&lt;/h2>
&lt;h3 id="优点">优点&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Ceph支持对象存储、块存储和文件存储服务，故称为统一存储。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用CRUSH算法，数据分布均衡，并行度高，不需要维护固定的元数据结构；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据具有强一致，确保所有副本写入完成才返回确认，适合读多写少场景；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>去中心化，MDS之间地位相同，无固定的中心节点&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="缺点">缺点&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>去中心化的分布式解决方案，需要提前做好规划设计，对技术团队的要求能力比较高。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ceph扩容时，由于其数据分布均衡的特性，会导致整个存储系统性能的下降。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2019-12-19-16-33-55-image.png" alt="">&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;h3 id="crush算法">CRUSH算法&lt;/h3>
&lt;p>CEPH的数据分布算法，它是一个分层的，区分故障域的分布式算法。在CRUSH算法中，对于不同的物理设备统一抽象成了bucket，每个结点都是一个bucket，其对应的物理结构各不相同。例如下图中的root，row（机架）， cabinet（机柜）， disk都是bucket的一种&lt;/p>
&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2019-12-19-16-42-38-image.png" title="" alt="" data-align="center">
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://www.infoq.cn/article/BRjtISYrUdHgec4ODExH">分布式存储 Ceph 介绍及原理架构分享（上）-InfoQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://juejin.im/entry/5b208433518825137b50cd10">https://juejin.im/entry/5b208433518825137b50cd10&lt;/a>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>Docker</title><link>https://justice.bj.cn/post/32.cloudnaive/docker%E7%AE%80%E4%BB%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/32.cloudnaive/docker%E7%AE%80%E4%BB%8B/</guid><description>&lt;h1 id="docker">Docker&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>docker是一个开源的应用容器引擎，基于go语言开发并遵循了apache2.0协议开源。&lt;/p>
&lt;p>docker可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的linux服务器，也可以实现虚拟化。&lt;/p>
&lt;p>容器是完全使用沙箱机制，相互之间不会有任何接口（类iphone的app），并且容器开销极其低。&lt;/p>
&lt;h3 id="容器和虚拟机">容器和虚拟机&lt;/h3>
&lt;p>容器时在linux上本机运行，并与其他容器共享主机的内核，它运行的一个独立的进程，不占用其他任何可执行文件的内存，非常轻量。&lt;/p>
&lt;p>虚拟机运行的是一个完成的操作系统，通过虚拟机管理程序对主机资源进行虚拟访问，相比之下需要的资源更多。&lt;/p>
&lt;p> &lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQh5SJzVziaQ5Zhib5Z2hlSNS1xwjicXK5fra81kibukKyz2K8ZYRmBrtGdQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="8容器在内核中支持2种重要技术">&lt;strong>8、容器在内核中支持2种重要技术&lt;/strong>&lt;/h3>
&lt;p>docker本质就是宿主机的一个进程，docker是通过namespace实现资源隔离，通过cgroup实现资源限制，通过写时复制技术（copy-on-write）实现了高效的文件操作（类似虚拟机的磁盘比如分配500g并不是实际占用物理磁盘500g）&lt;/p>
&lt;p>1）namespaces 名称空间&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQiaXAsDznWiaTVMEgH9l7wg603nZJ7ia0yib2AtrvNwfLMIDWg1raSTBSBQ/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p> 2）control Group 控制组&lt;/p>
&lt;p>cgroup的特点是：　&lt;/p>
&lt;ul>
&lt;li>
&lt;p>cgroup的api以一个伪文件系统的实现方式，用户的程序可以通过文件系统实现cgroup的组件管理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cgroup的组件管理操作单元可以细粒度到线程级别，另外用户可以创建和销毁cgroup，从而实现资源载分配和再利用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>所有资源管理的功能都以子系统的方式实现，接口统一子任务创建之初与其父任务处于同一个cgroup的控制组&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>四大功能：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>资源限制：可以对任务使用的资源总额进行限制&lt;/p>
&lt;/li>
&lt;li>
&lt;p>优先级分配：通过分配的cpu时间片数量以及磁盘IO带宽大小，实际上相当于控制了任务运行优先级&lt;/p>
&lt;/li>
&lt;li>
&lt;p>资源统计：可以统计系统的资源使用量，如cpu时长，内存用量等&lt;/p>
&lt;/li>
&lt;li>
&lt;p>任务控制：cgroup可以对任务执行挂起、恢复等操作&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="heading">&lt;/h3>
&lt;h2 id="重要概念">重要概念&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>image镜像：docker镜像就是一个只读模板，比如，一个镜像可以包含一个完整的centos，里面仅安装apache或用户的其他应用，镜像可以用来创建docker容器，另外docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下周一个已经做好的镜像来直接使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>container容器：docker利用容器来运行应用，容器是从镜像创建的运行实例，它可以被启动，开始、停止、删除、每个容器都是互相隔离的，保证安全的平台，可以吧容器看做是要给简易版的linux环境（包括root用户权限、镜像空间、用户空间和网络空间等）和运行再其中的应用程序&lt;/p>
&lt;/li>
&lt;li>
&lt;p>repostory仓库：仓库是集中存储镜像文件的沧桑，registry是仓库主从服务器，实际上参考注册服务器上存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>仓库分为两种，公有参考，和私有仓库，最大的公开仓库是docker Hub，存放了数量庞大的镜像供用户下周，国内的docker pool，这里仓库的概念与Git类似，registry可以理解为github这样的托管服务&lt;/p>
&lt;h2 id="架构">架构&lt;/h2>
&lt;h3 id="1总体架构">&lt;strong>1、总体架构&lt;/strong>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQP05SZIegcKiaLYoSNMNVAKLiaGdaTWNg6u4bUdvcuNWcvusCkZ7wvibuA/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>distribution 负责与docker registry交互，上传洗澡镜像以及v2 registry 有关的源数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>registry负责docker registry有关的身份认证、镜像查找、镜像验证以及管理registry mirror等交互操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>image 负责与镜像源数据有关的存储、查找，镜像层的索引、查找以及镜像tar包有关的导入、导出操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>reference负责存储本地所有镜像的repository和tag名，并维护与镜像id之间的映射关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>layer模块负责与镜像层和容器层源数据有关的增删改查，并负责将镜像层的增删改查映射到实际存储镜像层文件的graphdriver模块&lt;/p>
&lt;/li>
&lt;li>
&lt;p>graghdriver是所有与容器镜像相关操作的执行者&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="2docker架构2">&lt;strong>2、docker架构2&lt;/strong>&lt;/h3>
&lt;p>如果觉得上面架构图比较乱可以看这个架构：&lt;/p>
&lt;h3 id="httpsmmbizqpiccnmmbiz_jpgicnyeyk3vqgm9yicn3viaynt53xk9vanziqbqbbbiugyerdnepf9jkodiafzjgqg9zmxi0qy0empg0iaonaicmdy5vqq640wx_fmtjpegtpwebpwxfrom5wx_lazy1wx_co1">&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQbQbBBIUgYerDNepf9jkoDiafzJGQg9zMXI0qY0eMPg0iaoNaicMDY5VQQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/h3>
&lt;p>从上图不难看出，用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。&lt;/p>
&lt;p>而Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；而后Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。&lt;/p>
&lt;p>Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。&lt;/p>
&lt;p>而libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。当执行完运行容器的命令后，一个实际的Docker容器就处于运行状态，该容器拥有独立的文件系统，独立并且安全的运行环境等。&lt;/p>
&lt;h3 id="3docker架构3">&lt;strong>3、docker架构3&lt;/strong>&lt;/h3>
&lt;p>再来看看另外一个架构，这个个架构就简单清晰指明了server/client交互，容器和镜像、数据之间的一些联系。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQPo5rennpNHVZaT2ceMmGrP5icFBVD94LiaP15ib8L2zHVIj0kkpyvy87A/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>这个架构图更加清晰了架构&lt;/p>
&lt;p>docker daemon就是docker的守护进程即server端，可以是远程的，也可以是本地的，这个不是C/S架构吗，客户端Docker client 是通过rest api进行通信。&lt;/p>
&lt;p>docker cli 用来管理容器和镜像，客户端提供一个只读镜像，然后通过镜像可以创建多个容器，这些容器可以只是一个RFS（Root file system根文件系统），也可以ishi一个包含了用户应用的RFS，容器再docker client中只是要给进程，两个进程之间互不可见。&lt;/p>
&lt;p>用户不能与server直接交互，但可以通过与容器这个桥梁来交互，由于是操作系统级别的虚拟技术，中间的损耗几乎可以不计。&lt;/p>
&lt;h2 id="三docker架构2各个模块的功能带完善">&lt;strong>三、docker架构2各个模块的功能（带完善）&lt;/strong>&lt;/h2>
&lt;p>主要的模块有：Docker Client、Docker Daemon、Docker Registry、Graph、Driver、libcontainer以及Docker container。　　　&lt;/p>
&lt;h3 id="heading-1">&lt;/h3>
&lt;h3 id="1docker-client">&lt;strong>1、docker client&lt;/strong>&lt;/h3>
&lt;p>docker client 是docker架构中用户用来和docker daemon建立通信的客户端，用户使用的可执行文件为docker，通过docker命令行工具可以发起众多管理container的请求。&lt;/p>
&lt;p>docker client可以通过一下三宗方式和docker daemon建立通信：tcp://host:port;unix:path_to_socket;fd://socketfd。，docker client可以通过设置命令行flag参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性&lt;/p>
&lt;p>docker client发送容器管理请求后，由docker daemon接受并处理请求，当docker client 接收到返回的请求相应并简单处理后，docker client 一次完整的生命周期就结束了，当需要继续发送容器管理请求时，用户必须再次通过docker可以执行文件创建docker client。&lt;/p>
&lt;h3 id="heading-2">&lt;/h3>
&lt;h3 id="2docker-daemon">&lt;strong>2、docker daemon&lt;/strong>&lt;/h3>
&lt;p>docker daemon 是docker架构中一个常驻在后台的系统进程，功能是：接收处理docker client发送的请求。该守护进程在后台启动一个server，server负载接受docker client发送的请求；接受请求后，server通过路由与分发调度，找到相应的handler来执行请求。&lt;/p>
&lt;p>docker daemon启动所使用的可执行文件也为docker，与docker client启动所使用的可执行文件docker相同，在docker命令执行时，通过传入的参数来判别docker daemon与docker client。&lt;/p>
&lt;p>docker daemon的架构可以分为：docker server、engine、job。daemon&lt;/p>
&lt;h3 id="heading-3">&lt;/h3>
&lt;h3 id="3docker-server">&lt;strong>3、docker server&lt;/strong>&lt;/h3>
&lt;p>docker server在docker架构中时专门服务于docker client的server，该server的功能时：接受并调度分发docker client发送的请求，架构图如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>在Docker的启动过程中，通过包gorilla/mux（golang的类库解析），创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。&lt;/p>
&lt;p>若Docker Client通过HTTP的形式访问Docker Daemon，创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。&lt;/p>
&lt;p>在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。&lt;/p>
&lt;p>需要注意的是：Docker Server的运行在Docker的启动过程中，是靠一个名为”serveapi”的job的运行来完成的。原则上，Docker Server的运行是众多job中的一个，但是为了强调Docker Server的重要性以及为后续job服务的重要特性，将该”serveapi”的job单独抽离出来分析，理解为Docker Server。&lt;/p>
&lt;h3 id="heading-4">&lt;/h3>
&lt;h3 id="4engine">&lt;strong>4、engine&lt;/strong>&lt;/h3>
&lt;p>Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。&lt;/p>
&lt;p>在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为”create”的job在运行时，执行的是daemon.ContainerCreate的handler。&lt;/p>
&lt;h3 id="heading-5">&lt;/h3>
&lt;h3 id="5job">&lt;strong>5、job&lt;/strong>&lt;/h3>
&lt;p>一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job，从Internet上下载一个文档，这是一个job；包括之前在Docker Server部分说过的，创建Server服务于HTTP的API，这也是一个job，等等。&lt;/p>
&lt;p>Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。&lt;/p>
&lt;h3 id="heading-6">&lt;/h3>
&lt;h3 id="6docker-registry">&lt;strong>6、docker registry&lt;/strong>&lt;/h3>
&lt;p>Docker Registry是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。&lt;/p>
&lt;p>在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为”search”，”pull” 与 “push”。&lt;/p>
&lt;p>其中，在Docker架构中，Docker可以使用公有的Docker Registry，即大家熟知的Docker Hub，如此一来，Docker获取容器镜像文件时，必须通过互联网访问Docker Hub；同时Docker也允许用户构建本地私有的Docker Registry，这样可以保证容器镜像的获取在内网完成。&lt;/p>
&lt;h3 id="heading-7">&lt;/h3>
&lt;h3 id="7graph">&lt;strong>7、Graph&lt;/strong>&lt;/h3>
&lt;p>Graph在Docker架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph存储着本地具有版本信息的文件系统镜像，另一方面也通过GraphDB记录着所有文件系统镜像彼此之间的关系。&lt;/p>
&lt;p>Graph的架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQDticrbs6fTR1ZvAvK5ufFO1MicOjiaClM6SIXpA2EiaFqhBvjlnzdhb4YQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>其中，GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录。它仅仅实现了大多数图数据库所拥有的一个小的子集，但是提供了简单的接口表示节点之间的关系。&lt;/p>
&lt;p>同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。&lt;/p>
&lt;p>&lt;strong>8、driver&lt;/strong>&lt;/p>
&lt;p>Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。由于Docker运行的生命周期中，并非用户所有的操作都是针对Docker容器的管理，另外还有关于Docker运行信息的获取，Graph的存储与记录等。因此，为了将Docker容器的管理从Docker Daemon内部业务逻辑中区分开来，设计了Driver层驱动来接管所有这部分请求。&lt;/p>
&lt;p>在Docker Driver的实现中，可以分为以下三类驱动：graphdriver、networkdriver和execdriver。&lt;/p>
&lt;p>graphdriver主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的rootfs时，graphdriver从本地镜像存储目录中获取指定的容器镜像。&lt;/p>
&lt;p>在graphdriver的初始化过程之前，有4种文件系统或类文件系统在其内部注册，它们分别是aufs、btrfs、vfs和devmapper。而Docker在初始化之时，通过获取系统环境变量”DOCKER_DRIVER”来提取所使用driver的指定类型。而之后所有的graph操作，都使用该driver来执行。&lt;/p>
&lt;p>graphdriver的架构如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>networkdriver的用途是完成Docker容器网络环境的配置，其中包括Docker启动时为Docker环境创建网桥；Docker容器创建时为其创建专属虚拟网卡设备；以及为Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。networkdriver的架构如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在execdriver的实现过程中，原先可以使用LXC驱动调用LXC的接口，来操纵容器的配置以及生命周期，而现在execdriver默认使用native驱动，不依赖于LXC。具体体现在Daemon启动过程中加载的ExecDriverflag参数，该参数在配置文件已经被设为”native”。这可以认为是Docker在1.2版本上一个很大的改变，或者说Docker实现跨平台的一个先兆。execdriver架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQ2mENqgVLbqicwOZs40ibSm9FbCu5GKiaM7VicliaNWKKH1WYm4ibfkh8V98w/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="9libcontainer">&lt;strong>9、libcontainer&lt;/strong>&lt;/h3>
&lt;p>libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。&lt;/p>
&lt;p>正是由于libcontainer的存在，Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。libcontainer架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQgial97GT7cIBa4wxBXwhKicjoH7PnjMx6bNFtavDz6jSjofmwEYRw9TA/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>另外，libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。又由于libcontainer使用Go这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，因此很难说，未来的Docker就一定会紧紧地和Linux捆绑在一起。而于此同时，Microsoft在其著名云计算平台Azure中，也添加了对Docker的支持，可见Docker的开放程度与业界的火热度。&lt;/p>
&lt;p>暂不谈Docker，由于libcontainer的功能以及其本身与系统的松耦合特性，很有可能会在其他以容器为原型的平台出现，同时也很有可能催生出云计算领域全新的项目。&lt;/p>
&lt;h3 id="heading-8">&lt;/h3>
&lt;h3 id="10docker-container">&lt;strong>10、docker container&lt;/strong>&lt;/h3>
&lt;p>Docker container（Docker容器）是Docker架构中服务交付的最终体现形式。&lt;/p>
&lt;p>Docker按照用户的需求与指令，订制相应的Docker容器：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过指定运行的命令，使得Docker容器执行指定的工作。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;h2 id="四docker简单使用">&lt;strong>四、docker简单使用&lt;/strong>&lt;/h2>
&lt;h3 id="1安装">&lt;strong>1、安装&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">yum install docker -ysystemctl enable dockersystemctl start docker
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意：启动前应当设置源&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">vim /usr/lib/systemd/system/docker.service
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>2、docker版本查询&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 ~]# docker version
Client:Version: 1.13.1
API version: 1.26
Package version: docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64
Go version: go1.10.3
Git commit: b2f74b2/1.13.1
Built: Wed May 1 14:55:20 2019OS/
Arch: linux/amd64
Server:Version: 1.13.1
API version: 1.26 (minimum version 1.12)
Package version: docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64
Go version: go1.10.3
Git commit: b2f74b2/1.13.1
Built: Wed May 1 14:55:20 2019OS/
Arch: linux/amd64
Experimental: false
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>3、搜索下载镜像&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker pull alpine    #下载镜像
docker search nginx   #查看镜像docker pull nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="4查看已经下载的镜像">&lt;strong>4、查看已经下载的镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 ~]# docker images
REPOSITORY         TAG      IMAGE  ID     CREATED      SIZEzxg/my_nginx      v1       b164f4c07c64  8 days ago    126 MBzxg/my_nginx      latest   f07837869dfc  8 days ago    126 MBdocker.io/nginx   latest   e445ab08b2be  2 weeks ago   126 MBdocker.io/alpine  latest   b7b28af77ffe  3 weeks ago   5.58 MBdocker.io/centos  latest   9f38484d220f  4 months ago  202 MB
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="5导出镜像">&lt;strong>5、导出镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker save nginx &amp;gt;/tmp/nginx.tar.gz
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="6删除镜像">&lt;strong>6、删除镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker rmi -f nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="7导入镜像">&lt;strong>7、导入镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker load &amp;lt;/tmp/nginx.tar.gz
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="8默认配置文件">&lt;strong>8、默认配置文件&lt;/strong>&lt;/h3>
&lt;p>vim /usr/lib/systemd/system/docker.service &lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[Unit]
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果更改存储目录就添加　　&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">--graph=/opt/docker
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果更改DNS——默认采用宿主机的dns&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">--dns=xxxx的方式指定
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="heading-9">&lt;/h3>
&lt;h3 id="9运行hello-world">&lt;strong>9、运行hello world&lt;/strong>&lt;/h3>
&lt;p>这里用centos镜像echo一个hello word&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker images
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="10运行一个容器-run">&lt;strong>10、运行一个容器-run&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it alpine sh   #运行并进入alpine
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>后台运行（-d后台运行）（&amp;ndash;name添加一个名字）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it -d --name test1 alpine
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>还有一种-rm参数，ctrl+c后就删除，可以测试环境用，生成环境用的少&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it --rm --name centos nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="11如何进入容器">&lt;strong>11、如何进入容器&lt;/strong>&lt;/h3>
&lt;p>三种方法，上面已经演示了一种&lt;/p>
&lt;p>第一种，需要容器本身的pid及util-linux，不推荐，暂时不演示了&lt;/p>
&lt;p>第二种，不分配bash终端的一种实施操作，不推荐，这种操作如果在开一个窗口也能看到操作的指令，所有人都能看到&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker ps
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第三种：exec方式，终端时分开的，推荐&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker exec -it mynginx sh
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="12查看docker进程及删除容器">&lt;strong>12、查看docker进程及删除容器&lt;/strong>&lt;/h3>
&lt;p>上面已经演示：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ docker ps
$ docker ps -a 　　　　　　&lt;span class="c1">#-a :显示所有的容器，包括未运行的&lt;/span>
&lt;span class="c1"># 查看容器详细信息&lt;/span>
$ docker inspect mynginx
&lt;span class="c1"># 查看日志**&lt;/span>
$ docker logs -f mynginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://www.cnblogs.com/zhangxingeng/p/11236968.html">Docker1 架构原理及简单使用 - 乐章 - 博客园&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.yuanshuli.com/post-64.html">【解决】docker启动报错：Running modprobe xt_conntrack failed with message: `modprobe: ERROR: could not insert &amp;lsquo;xt_conntrack&amp;rsquo;&amp;hellip;&amp;hellip; - 碧海长天&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch内核解析 - 查询篇</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E8%AF%BB%E6%B5%81%E7%A8%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E8%AF%BB%E6%B5%81%E7%A8%8B/</guid><description>&lt;h1 id="elasticsearch内核解析---查询篇">Elasticsearch内核解析 - 查询篇&lt;/h1>
&lt;h2 id="读操作">读操作&lt;/h2>
&lt;p>实时性和《&lt;a href="https://zhuanlan.zhihu.com/p/34669354">Elasticsearch内核解析 - 写入篇&lt;/a>》中的“写操作”一样，对于搜索而言是近实时的，延迟在100ms以上，对于NoSQL则需要是实时的。&lt;/p>
&lt;p>一致性指的是写入成功后，下次读操作一定要能读取到最新的数据。对于搜索，这个要求会低一些，可以有一些延迟。但是对于NoSQL数据库，则一般要求最好是强一致性的。&lt;/p>
&lt;p>结果匹配上，NoSQL作为数据库，查询过程中只有符合不符合两种情况，而搜索里面还有是否相关，类似于NoSQL的结果只能是0或1，而搜索里面可能会有0.1，0.5，0.9等部分匹配或者更相关的情况。&lt;/p>
&lt;p>结果召回上，搜索一般只需要召回最满足条件的Top N结果即可，而NoSQL一般都需要返回满足条件的所有结果。&lt;/p>
&lt;p>搜索系统一般都是两阶段查询，第一个阶段查询到对应的Doc ID，也就是PK；第二阶段再通过Doc ID去查询完整文档，而NoSQL数据库一般是一阶段就返回结果。在Elasticsearch中两种都支持。&lt;/p>
&lt;p>目前NoSQL的查询，聚合、分析和统计等功能上都是要比搜索弱的。&lt;/p>
&lt;h2 id="lucene的读">Lucene的读&lt;/h2>
&lt;p>Elasticsearch使用了Lucene作为搜索引擎库，通过Lucene完成特定字段的搜索等功能，在Lucene中这个功能是通过IndexSearcher的下列接口实现的：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="n">TopDocs&lt;/span> &lt;span class="nf">search&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Query&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Document&lt;/span> &lt;span class="nf">doc&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">docID&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="nf">count&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Query&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">......(&lt;/span>&lt;span class="n">其他&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第一个search接口实现搜索功能，返回最满足Query的N个结果；第二个doc接口通过doc id查询Doc内容；第三个count接口通过Query获取到命中数。&lt;/p>
&lt;p>这三个功能是搜索中的最基本的三个功能点，对于大部分Elasticsearch中的查询都是比较复杂的，直接用这个接口是无法满足需求的，比如分布式问题。这些问题都留给了Elasticsearch解决，我们接下来看Elasticsearch中相关读功能的剖析。&lt;/p>
&lt;h2 id="elasticsearch的读">Elasticsearch的读&lt;/h2>
&lt;p>Elasticsearch中每个Shard都会有多个Replica，主要是为了保证数据可靠性，除此之外，还可以增加读能力，因为写的时候虽然要写大部分Replica Shard，但是查询的时候只需要查询Primary和Replica中的任何一个就可以了。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-1ad1351408bdf0ce7f76f251d6ef8bc4_720w.jpg" alt="">&lt;/p>
&lt;p>Search On Replicas&lt;/p>
&lt;p>在上图中，该Shard有1个Primary和2个Replica Node，当查询的时候，从三个节点中根据Request中的preference参数选择一个节点查询。preference可以设置_local，_primary，_replica以及其他选项。如果选择了primary，则每次查询都是直接查询Primary，可以保证每次查询都是最新的。如果设置了其他参数，那么可能会查询到R1或者R2，这时候就有可能查询不到最新的数据。&lt;/p>
&lt;blockquote>
&lt;p>上述代码逻辑在OperationRouting.Java的searchShards方法中。&lt;/p>
&lt;/blockquote>
&lt;p>接下来看一下，Elasticsearch中的查询是如何支持分布式的。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-737f6cb48ccf22c50c2e630433c6ad48_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中通过分区实现分布式，数据写入的时候根据_routing规则将数据写入某一个Shard中，这样就能将海量数据分布在多个Shard以及多台机器上，已达到分布式的目标。这样就导致了查询的时候，潜在数据会在当前index的所有的Shard中，所以Elasticsearch查询的时候需要查询所有Shard，同一个Shard的Primary和Replica选择一个即可，查询请求会分发给所有Shard，每个Shard中都是一个独立的查询引擎，比如需要返回Top 10的结果，那么每个Shard都会查询并且返回Top 10的结果，然后在Client Node里面会接收所有Shard的结果，然后通过优先级队列二次排序，选择出Top 10的结果返回给用户。&lt;/p>
&lt;p>这里有一个问题就是请求膨胀，用户的一个搜索请求在Elasticsearch内部会变成Shard个请求，这里有个优化点，虽然是Shard个请求，但是这个Shard个数不一定要是当前Index中的Shard个数，只要是当前查询相关的Shard即可，这个需要基于业务和请求内容优化，通过这种方式可以优化请求膨胀数。&lt;/p>
&lt;p>Elasticsearch中的查询主要分为两类，Get请求：通过ID查询特定Doc；Search请求：通过Query查询匹配Doc。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-1f4c1cf921049b841ae2612b4734cdb1_720w.jpg" alt="">&lt;/p>
&lt;blockquote>
&lt;p>上图中内存中的Segment是指刚Refresh Segment，但是还没持久化到磁盘的新Segment，而非从磁盘加载到内存中的Segment。&lt;/p>
&lt;/blockquote>
&lt;p>对于Search类请求，查询的时候是一起查询内存和磁盘上的Segment，最后将结果合并后返回。这种查询是近实时（Near Real Time）的，主要是由于内存中的Index数据需要一段时间后才会刷新为Segment。&lt;/p>
&lt;p>对于Get类请求，查询的时候是先查询内存中的TransLog，如果找到就立即返回，如果没找到再查询磁盘上的TransLog，如果还没有则再去查询磁盘上的Segment。这种查询是实时（Real Time）的。这种查询顺序可以保证查询到的Doc是最新版本的Doc，这个功能也是为了保证NoSQL场景下的实时性要求。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-c5455432442548d1f12c975684fc4a00_720w.jpg" alt="">&lt;/p>
&lt;p>多阶段查询&lt;/p>
&lt;p>所有的搜索系统一般都是两阶段查询，第一阶段查询到匹配的DocID，第二阶段再查询DocID对应的完整文档，这种在Elasticsearch中称为query_then_fetch，还有一种是一阶段查询的时候就返回完整Doc，在Elasticsearch中称作query_and_fetch，一般第二种适用于只需要查询一个Shard的请求。&lt;/p>
&lt;p>除了一阶段，两阶段外，还有一种三阶段查询的情况。搜索里面有一种算分逻辑是根据TF（Term Frequency）和DF（Document Frequency）计算基础分，但是Elasticsearch中查询的时候，是在每个Shard中独立查询的，每个Shard中的TF和DF也是独立的，虽然在写入的时候通过_routing保证Doc分布均匀，但是没法保证TF和DF均匀，那么就有会导致局部的TF和DF不准的情况出现，这个时候基于TF、DF的算分就不准。为了解决这个问题，Elasticsearch中引入了DFS查询，比如DFS_query_then_fetch，会先收集所有Shard中的TF和DF值，然后将这些值带入请求中，再次执行query_then_fetch，这样算分的时候TF和DF就是准确的，类似的有DFS_query_and_fetch。这种查询的优势是算分更加精准，但是效率会变差。另一种选择是用BM25代替TF/DF模型。&lt;/p>
&lt;p>在新版本Elasticsearch中，用户没法指定DFS_query_and_fetch和query_and_fetch，这两种只能被Elasticsearch系统改写。&lt;/p>
&lt;h2 id="elasticsearch查询流程">Elasticsearch查询流程&lt;/h2>
&lt;p>Elasticsearch中的大部分查询，以及核心功能都是Search类型查询，上面我们了解到查询分为一阶段，二阶段和三阶段，这里我们就以最常见的的二阶段查询为例来介绍查询流程。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-10acab5576a2359ca279331e81adc1e2_720w.jpg" alt="">&lt;/p>
&lt;p>查询流程&lt;/p>
&lt;p>&lt;strong>注册Action&lt;/strong>&lt;/p>
&lt;p>Elasticsearch中，查询和写操作一样都是在ActionModule.java中注册入口处理函数的。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">registerHandler.accept(new RestSearchAction(settings, restController));
......
actions.register(SearchAction.INSTANCE, TransportSearchAction.class);
......
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果请求是Rest请求，则会在RestSearchAction中解析请求，检查查询类型，不能设置为dfs_query_and_fetch或者query_and_fetch，这两个目前只能用于Elasticsearch中的优化场景，然后将请求发给后面的TransportSearchAction处理。然后构造SearchRequest，将请求发送给TransportSearchAction处理。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-02fdc9375a9d6af62ac6c2035b5a9730_720w.jpg" alt="">&lt;/p>
&lt;p>如果是第一阶段的Query Phase请求，则会调用SearchService的executeQueryPhase方法。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-be3a37c7fe08d4f1559b9eb0aaa8d37a_720w.jpg" alt="">&lt;/p>
&lt;p>如果是第二阶段的Fetch Phase请求，则会调用SearchService的executeFetchPhase方法。&lt;/p>
&lt;h2 id="client-node">&lt;strong>Client Node&lt;/strong>&lt;/h2>
&lt;p>Client Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。&lt;/p>
&lt;p>&lt;strong>1. Get Remove Cluster Shard&lt;/strong>&lt;/p>
&lt;p>判断是否需要跨集群访问，如果需要，则获取到要访问的Shard列表。&lt;/p>
&lt;p>&lt;strong>2. Get Search Shard Iterator&lt;/strong>&lt;/p>
&lt;p>获取当前Cluster中要访问的Shard，和上一步中的Remove Cluster Shard合并，构建出最终要访问的完整Shard列表。&lt;/p>
&lt;p>这一步中，会根据Request请求中的参数从Primary Node和多个Replica Node中选择出一个要访问的Shard。&lt;/p>
&lt;p>&lt;strong>3. For Every Shard:Perform&lt;/strong>&lt;/p>
&lt;p>遍历每个Shard，对每个Shard执行后面逻辑。&lt;/p>
&lt;p>&lt;strong>4. Send Request To Query Shard&lt;/strong>&lt;/p>
&lt;p>将查询阶段请求发送给相应的Shard。&lt;/p>
&lt;p>&lt;strong>5. Merge Docs&lt;/strong>&lt;/p>
&lt;p>上一步将请求发送给多个Shard后，这一步就是异步等待返回结果，然后对结果合并。这里的合并策略是维护一个Top N大小的优先级队列，每当收到一个shard的返回，就把结果放入优先级队列做一次排序，直到所有的Shard都返回。&lt;/p>
&lt;p>翻页逻辑也是在这里，如果需要取Top 30~ Top 40的结果，这个的意思是所有Shard查询结果中的第30到40的结果，那么在每个Shard中无法确定最终的结果，每个Shard需要返回Top 40的结果给Client Node，然后Client Node中在merge docs的时候，计算出Top 40的结果，最后再去除掉Top 30，剩余的10个结果就是需要的Top 30~ Top 40的结果。&lt;/p>
&lt;p>上述翻页逻辑有一个明显的缺点就是每次Shard返回的数据中包括了已经翻过的历史结果，如果翻页很深，则在这里需要排序的Docs会很多，比如Shard有1000，取第9990到10000的结果，那么这次查询，Shard总共需要返回1000 * 10000，也就是一千万Doc，这种情况很容易导致OOM。&lt;/p>
&lt;p>另一种翻页方式是使用search_after，这种方式会更轻量级，如果每次只需要返回10条结构，则每个Shard只需要返回search_after之后的10个结果即可，返回的总数据量只是和Shard个数以及本次需要的个数有关，和历史已读取的个数无关。这种方式更安全一些，推荐使用这种。&lt;/p>
&lt;p>如果有aggregate，也会在这里做聚合，但是不同的aggregate类型的merge策略不一样，具体的可以在后面的aggregate文章中再介绍。&lt;/p>
&lt;p>&lt;strong>6. Send Request To Fetch Shard&lt;/strong>&lt;/p>
&lt;p>选出Top N个Doc ID后发送给这些Doc ID所在的Shard执行Fetch Phase，最后会返回Top N的Doc的内容。&lt;/p>
&lt;h2 id="query-phase">Query Phase&lt;/h2>
&lt;p>接下来我们看第一阶段查询的步骤：&lt;/p>
&lt;p>&lt;strong>1. Create Search Context&lt;/strong>&lt;/p>
&lt;p>创建Search Context，之后Search过程中的所有中间状态都会存在Context中，这些状态总共有50多个，具体可以查看DefaultSearchContext或者其他SearchContext的子类。&lt;/p>
&lt;p>&lt;strong>2. Parse Query&lt;/strong>&lt;/p>
&lt;p>解析Query的Source，将结果存入Search Context。这里会根据请求中Query类型的不同创建不同的Query对象，比如TermQuery、FuzzyQuery等，最终真正执行TermQuery、FuzzyQuery等语义的地方是在Lucene中。&lt;/p>
&lt;p>这里包括了dfsPhase、queryPhase和fetchPhase三个阶段的preProcess部分，只有queryPhase的preProcess中有执行逻辑，其他两个都是空逻辑，执行完preProcess后，所有需要的参数都会设置完成。&lt;/p>
&lt;p>由于Elasticsearch中有些请求之间是相互关联的，并非独立的，比如scroll请求，所以这里同时会设置Context的生命周期。&lt;/p>
&lt;p>同时会设置lowLevelCancellation是否打开，这个参数是集群级别配置，同时也能动态开关，打开后会在后面执行时做更多的检测，检测是否需要停止后续逻辑直接返回。&lt;/p>
&lt;p>&lt;strong>3. Get From Cache&lt;/strong>&lt;/p>
&lt;p>判断请求是否允许被Cache，如果允许，则检查Cache中是否已经有结果，如果有则直接读取Cache，如果没有则继续执行后续步骤，执行完后，再将结果加入Cache。&lt;/p>
&lt;p>&lt;strong>4. Add Collectors&lt;/strong>&lt;/p>
&lt;p>Collector主要目标是收集查询结果，实现排序，对自定义结果集过滤和收集等。这一步会增加多个Collectors，多个Collector组成一个List。&lt;/p>
&lt;ol>
&lt;li>FilteredCollector*：*先判断请求中是否有Post Filter，Post Filter用于Search，Agg等结束后再次对结果做Filter，希望Filter不影响Agg结果。如果有Post Filter则创建一个FilteredCollector，加入Collector List中。&lt;/li>
&lt;li>PluginInMultiCollector：判断请求中是否制定了自定义的一些Collector，如果有，则创建后加入Collector List。&lt;/li>
&lt;li>MinimumScoreCollector：判断请求中是否制定了最小分数阈值，如果指定了，则创建MinimumScoreCollector加入Collector List中，在后续收集结果时，会过滤掉得分小于最小分数的Doc。&lt;/li>
&lt;li>EarlyTerminatingCollector：判断请求中是否提前结束Doc的Seek，如果是则创建EarlyTerminatingCollector，加入Collector List中。在后续Seek和收集Doc的过程中，当Seek的Doc数达到Early Terminating后会停止Seek后续倒排链。&lt;/li>
&lt;li>CancellableCollector：判断当前操作是否可以被中断结束，比如是否已经超时等，如果是会抛出一个TaskCancelledException异常。该功能一般用来提前结束较长的查询请求，可以用来保护系统。&lt;/li>
&lt;li>EarlyTerminatingSortingCollector：如果Index是排序的，那么可以提前结束对倒排链的Seek，相当于在一个排序递减链表上返回最大的N个值，只需要直接返回前N个值就可以了。这个Collector会加到Collector List的头部。EarlyTerminatingSorting和EarlyTerminating的区别是，EarlyTerminatingSorting是一种对结果无损伤的优化，而EarlyTerminating是有损的，人为掐断执行的优化。&lt;/li>
&lt;li>TopDocsCollector：这个是最核心的Top N结果选择器，会加入到Collector List的头部。TopScoreDocCollector和TopFieldCollector都是TopDocsCollector的子类，TopScoreDocCollector会按照固定的方式算分，排序会按照分数+doc id的方式排列，如果多个doc的分数一样，先选择doc id小的文档。而TopFieldCollector则是根据用户指定的Field的值排序。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>5. lucene::search&lt;/strong>&lt;/p>
&lt;p>这一步会调用Lucene中IndexSearch的search接口，执行真正的搜索逻辑。每个Shard中会有多个Segment，每个Segment对应一个LeafReaderContext，这里会遍历每个Segment，到每个Segment中去Search结果，然后计算分数。&lt;/p>
&lt;p>搜索里面一般有两阶段算分，第一阶段是在这里算的，会对每个Seek到的Doc都计算分数，为了减少CPU消耗，一般是算一个基本分数。这一阶段完成后，会有个排序。然后在第二阶段，再对Top 的结果做一次二阶段算分，在二阶段算分的时候会考虑更多的因子。二阶段算分在后续操作中。&lt;/p>
&lt;p>具体请求，比如TermQuery、WildcardQuery的查询逻辑都在Lucene中，后面会有专门文章介绍。&lt;/p>
&lt;p>&lt;strong>6. rescore&lt;/strong>&lt;/p>
&lt;p>根据Request中是否包含rescore配置决定是否进行二阶段排序，如果有则执行二阶段算分逻辑，会考虑更多的算分因子。二阶段算分也是一种计算机中常见的多层设计，是一种资源消耗和效率的折中。&lt;/p>
&lt;p>Elasticsearch中支持配置多个Rescore，这些rescore逻辑会顺序遍历执行。每个rescore内部会先按照请求参数window选择出Top window的doc，然后对这些doc排序，排完后再合并回原有的Top 结果顺序中。&lt;/p>
&lt;p>&lt;strong>7. suggest::execute()&lt;/strong>&lt;/p>
&lt;p>如果有推荐请求，则在这里执行推荐请求。如果请求中只包含了推荐的部分，则很多地方可以优化。推荐不是今天的重点，这里就不介绍了，后面有机会再介绍。&lt;/p>
&lt;p>&lt;strong>8. aggregation::execute()&lt;/strong>&lt;/p>
&lt;p>如果含有聚合统计请求，则在这里执行。Elasticsearch中的aggregate的处理逻辑也类似于Search，通过多个Collector来实现。在Client Node中也需要对aggregation做合并。aggregate逻辑更复杂一些，就不在这里赘述了，后面有需要就再单独开文章介绍。&lt;/p>
&lt;p>上述逻辑都执行完成后，如果当前查询请求只需要查询一个Shard，那么会直接在当前Node执行Fetch Phase。&lt;/p>
&lt;h2 id="fetch-phase">Fetch Phase&lt;/h2>
&lt;p>Elasticsearch作为搜索系统时，或者任何搜索系统中，除了Query阶段外，还会有一个Fetch阶段，这个Fetch阶段在数据库类系统中是没有的，是搜索系统中额外增加的阶段。搜索系统中额外增加Fetch阶段的原因是搜索系统中数据分布导致的，在搜索中，数据通过routing分Shard的时候，只能根据一个主字段值来决定，但是查询的时候可能会根据其他非主字段查询，那么这个时候所有Shard中都可能会存在相同非主字段值的Doc，所以需要查询所有Shard才能不会出现结果遗漏。同时如果查询主字段，那么这个时候就能直接定位到Shard，就只需要查询特定Shard即可，这个时候就类似于数据库系统了。另外，数据库中的二级索引又是另外一种情况，但类似于查主字段的情况，这里就不多说了。&lt;/p>
&lt;p>基于上述原因，第一阶段查询的时候并不知道最终结果会在哪个Shard上，所以每个Shard中管都需要查询完整结果，比如需要Top 10，那么每个Shard都需要查询当前Shard的所有数据，找出当前Shard的Top 10，然后返回给Client Node。如果有100个Shard，那么就需要返回100 * 10 = 1000个结果，而Fetch Doc内容的操作比较耗费IO和CPU，如果在第一阶段就Fetch Doc，那么这个资源开销就会非常大。所以，一般是当Client Node选择出最终Top N的结果后，再对最终的Top N读取Doc内容。通过增加一点网络开销而避免大量IO和CPU操作，这个折中是非常划算的。&lt;/p>
&lt;p>Fetch阶段的目的是通过DocID获取到用户需要的完整Doc内容。这些内容包括了DocValues，Store，Source，Script和Highlight等，具体的功能点是在SearchModule中注册的，系统默认注册的有：&lt;/p>
&lt;ul>
&lt;li>ExplainFetchSubPhase&lt;/li>
&lt;li>DocValueFieldsFetchSubPhase&lt;/li>
&lt;li>ScriptFieldsFetchSubPhase&lt;/li>
&lt;li>FetchSourceSubPhase&lt;/li>
&lt;li>VersionFetchSubPhase&lt;/li>
&lt;li>MatchedQueriesFetchSubPhase&lt;/li>
&lt;li>HighlightPhase&lt;/li>
&lt;li>ParentFieldSubFetchPhase&lt;/li>
&lt;/ul>
&lt;p>除了系统默认的8种外，还有通过插件的形式注册自定义的功能，这些SubPhase中最重要的是Source和Highlight，Source是加载原文，Highlight是计算高亮显示的内容片断。&lt;/p>
&lt;p>上述多个SubPhase会针对每个Doc顺序执行，可能会产生多次的随机IO，这里会有一些优化方案，但是都是针对特定场景的，不具有通用性。&lt;/p>
&lt;p>Fetch Phase执行完后，整个查询流程就结束了。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Elasticsearch中的查询流程比较简单，更多的查询原理都在Lucene中，后续我们会有针对不同请求的Lucene原理介绍性文章。&lt;/p></description></item><item><title>Elasticsearch写流程</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</guid><description>&lt;h1 id="elasticsearch写流程">Elasticsearch写流程&lt;/h1>
&lt;h2 id="lucene的写操作及其问题">lucene的写操作及其问题&lt;/h2>
&lt;p>Elasticsearch底层使用Lucene来实现doc的读写操作，Lucene通过&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">addDocument&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">deleteDocuments&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">updateDocument&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>三个方法来实现文档的写入，更新和删除操作。但是存在如下问题&lt;/p>
&lt;ol>
&lt;li>&lt;strong>没有并发设计&lt;/strong>。 lucene只是一个搜索引擎库，并没有涉及到分布式相关的设计，因此要想使用Lucene来处理海量数据，并利用分布式的能力，就必须在其之上进行分布式的相关设计。&lt;/li>
&lt;li>&lt;strong>非实时&lt;/strong>。 将文件写入lucence后并不能立即被检索，需要等待lucene生成一个完整的segment才能被检索&lt;/li>
&lt;li>&lt;strong>数据存储不可靠&lt;/strong>。 写入lucene的数据不会立即被持久化到磁盘，如果服务器宕机，那存储在内存中的数据将会丢失&lt;/li>
&lt;li>&lt;strong>不支持部分更新&lt;/strong> 。lucene中提供的updateDocuments仅支持对文档的全量更新，对部分更新不支持&lt;/li>
&lt;/ol>
&lt;h2 id="2-elasticsearch的写入方案">2. Elasticsearch的写入方案&lt;/h2>
&lt;p>针对Lucene的问题，ES做了如下设计&lt;/p>
&lt;h3 id="21-分布式设计">2.1 分布式设计：&lt;/h3>
&lt;p>为了支持对海量数据的存储和查询，Elasticsearch引入分片的概念，一个索引被分成多个分片，每个分片可以有一个主分片和多个副本分片，每个分片副本都是一个具有完整功能的lucene实例。分片可以分配在不同的服务器上，同一个分片的不同副本不能分配在相同的服务器上。&lt;/p>
&lt;p>在进行写操作时，ES会根据传入的_routing参数（或mapping中设置的_routing, 如果参数和设置中都没有则默认使用_id), 按照公式 &lt;code>shard_num=hash(\routing)%num_primary_shards&lt;/code>,计算出文档要分配到的分片，在从集群元数据中找出对应主分片的位置，将请求路由到该分片进行文档写操作。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-8ce7b65ede7b511a2d6d02530ed501d6_720w.jpg" alt="">&lt;/p>
&lt;h3 id="22-近实时性-refresh操作">2.2 近实时性-refresh操作&lt;/h3>
&lt;p>当一个文档写入Lucene后是不能被立即查询到的，Elasticsearch提供了一个refresh操作，会定时地调用lucene的reopen(新版本为openIfChanged)为内存中新写入的数据生成一个新的segment，此时被处理的文档均可以被检索到。refresh操作的时间间隔由 &lt;code>refresh_interval&lt;/code>参数控制，默认为1s, 当然还可以在写入请求中带上refresh表示写入后立即refresh，另外还可以调用refresh API显式refresh。&lt;/p>
&lt;h3 id="23-数据存储可靠性">2.3 数据存储可靠性&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>引入translog&lt;/strong> 当一个文档写入Lucence后是存储在内存中的，即使执行了refresh操作仍然是在文件系统缓存中，如果此时服务器宕机，那么这部分数据将会丢失。为此ES增加了translog， 当进行文档写操作时会先将文档写入Lucene，然后写入一份到translog，写入translog是落盘的(如果对可靠性要求不是很高，也可以设置异步落盘，可以提高性能，由配置 &lt;code>index.translog.durability&lt;/code>和 &lt;code>index.translog.sync_interval&lt;/code>控制)，这样就可以防止服务器宕机后数据的丢失。由于translog是追加写入，因此性能要比随机写入要好。与传统的分布式系统不同，这里是先写入Lucene再写入translog，原因是写入Lucene可能会失败，为了减少写入失败回滚的复杂度，因此先写入Lucene.&lt;/li>
&lt;li>&lt;strong>flush操作&lt;/strong> 另外每30分钟或当translog达到一定大小(由 &lt;code>index.translog.flush_threshold_size&lt;/code>控制，默认512mb), ES会触发一次flush操作，此时ES会先执行refresh操作将buffer中的数据生成segment，然后调用lucene的commit方法将所有内存中的segment fsync到磁盘。此时lucene中的数据就完成了持久化，会清空translog中的数据(6.x版本为了实现sequenceIDs,不删除translog)&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-9418f89986e3a141b3acf83cf158885a_720w.jpg" alt="">&lt;/p>
&lt;ol>
&lt;li>&lt;strong>merge操作&lt;/strong> 由于refresh默认间隔为1s中，因此会产生大量的小segment，为此ES会运行一个任务检测当前磁盘中的segment，对符合条件的segment进行合并操作，减少lucene中的segment个数，提高查询速度，降低负载。不仅如此，merge过程也是文档删除和更新操作后，旧的doc真正被删除的时候。用户还可以手动调用_forcemerge API来主动触发merge，以减少集群的segment个数和清理已删除或更新的文档。&lt;/li>
&lt;li>&lt;strong>多副本机制&lt;/strong> 另外ES有多副本机制，一个分片的主副分片不能分片在同一个节点上，进一步保证数据的可靠性。&lt;/li>
&lt;/ol>
&lt;h3 id="24-部分更新">2.4 部分更新&lt;/h3>
&lt;p>lucene仅支持对文档的整体更新，ES为了支持局部更新，在Lucene的Store索引中存储了一个_source字段，该字段的key值是文档ID， 内容是文档的原文。当进行更新操作时先从_source中获取原文，与更新部分合并后，再调用lucene API进行全量更新， 对于写入了ES但是还没有refresh的文档，可以从translog中获取。另外为了防止读取文档过程后执行更新前有其他线程修改了文档，ES增加了版本机制，当执行更新操作时发现当前文档的版本与预期不符，则会重新获取文档再更新。&lt;/p>
&lt;h2 id="3-es的写入流程">3. ES的写入流程&lt;/h2>
&lt;p>ES的任意节点都可以作为协调节点(coordinating node)接受请求，当协调节点接受到请求后进行一系列处理，然后通过_routing字段找到对应的primary shard，并将请求转发给primary shard, primary shard完成写入后，将写入并发发送给各replica， raplica执行写入操作后返回给primary shard， primary shard再将请求返回给协调节点。大致流程如下图：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-b0b0c96aaeadfde4da0685dae4b9908f_720w.jpg" alt="">&lt;/p>
&lt;h3 id="31-coordinating节点">3.1 coordinating节点&lt;/h3>
&lt;p>ES中接收并转发请求的节点称为coordinating节点，ES中所有节点都可以接受并转发请求。当一个节点接受到写请求或更新请求后，会执行如下操作：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>ingest pipeline&lt;/strong> 查看该请求是否符合某个ingest pipeline的pattern, 如果符合则执行pipeline中的逻辑，一般是对文档进行各种预处理，如格式调整，增加字段等。如果当前节点没有ingest角色，则需要将请求转发给有ingest角色的节点执行。&lt;/li>
&lt;li>&lt;strong>自动创建索引&lt;/strong> 判断索引是否存在，如果开启了自动创建则自动创建，否则报错&lt;/li>
&lt;li>&lt;strong>设置routing&lt;/strong> 获取请求URL或mapping中的_routing，如果没有则使用_id, 如果没有指定_id则ES会自动生成一个全局唯一ID。该_routing字段用于决定文档分配在索引的哪个shard上。&lt;/li>
&lt;li>&lt;strong>构建BulkShardRequest&lt;/strong> 由于Bulk Request中包含多种(Index/Update/Delete)请求，这些请求分别需要到不同的shard上执行，因此协调节点，会将请求按照shard分开，同一个shard上的请求聚合到一起，构建BulkShardRequest&lt;/li>
&lt;li>&lt;strong>将请求发送给primary shard&lt;/strong> 因为当前执行的是写操作，因此只能在primary上完成，所以需要把请求路由到primary shard所在节点&lt;/li>
&lt;li>&lt;strong>等待primary shard返回&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>目前的Elasticsearch有两个明显的身份，一个是分布式搜索系统，另一个是分布式NoSQL数据库，对于这两种不同的身份，读写语义基本类似，但也有一点差异。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-aced78c779161b2a5baa366f63d86883_720w.jpg" alt="">&lt;/p>
&lt;h2 id="写操作">&lt;strong>写操作&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>实时性：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>搜索系统的Index一般都是NRT（Near Real Time），近实时的，比如Elasticsearch中，Index的实时性是由refresh控制的，默认是1s，最快可到100ms，那么也就意味着Index doc成功后，需要等待一秒钟后才可以被搜索到。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NoSQL数据库的Write基本都是RT（Real Time），实时的，写入成功后，立即是可见的。Elasticsearch中的Index请求也能保证是实时的，因为Get请求会直接读内存中尚未Flush到存储介质的TransLog。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>可靠性：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>搜索系统对可靠性要求都不高，一般数据的可靠性通过将原始数据存储在另一个存储系统来保证，当搜索系统的数据发生丢失时，再从其他存储系统导一份数据过来重新rebuild就可以了。在Elasticsearch中，通过设置TransLog的Flush频率可以控制可靠性，要么是按请求，每次请求都Flush；要么是按时间，每隔一段时间Flush一次。一般为了性能考虑，会设置为每隔5秒或者1分钟Flush一次，Flush间隔时间越长，可靠性就会越低。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NoSQL数据库作为一款数据库，必须要有很高的可靠性，数据可靠性是生命底线，决不能有闪失。如果把Elasticsearch当做NoSQL数据库，此时需要设置TransLog的Flush策略为每个请求都要Flush，这样才能保证当前Shard写入成功后，数据能尽量持久化下来。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>上面简单介绍了下NoSQL数据库和搜索系统的一些异同，我们会在后面有一篇文章，专门用来介绍Elasticsearch作为NoSQL数据库时的一些局限和特点。&lt;/p>
&lt;h2 id="读操作">读操作&lt;/h2>
&lt;p>下一篇《Elasticsearch内核解析 - 查询篇》中再详细介绍。&lt;/p>
&lt;p>上面大概对比了下搜索和NoSQL在写方面的特点，接下来，我们看一下Elasticsearch 6.0.0版本中写入流程都做了哪些事情，希望能对大家有用。&lt;/p>
&lt;h2 id="关键点">&lt;strong>关键点&lt;/strong>&lt;/h2>
&lt;p>在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑：&lt;/p>
&lt;ul>
&lt;li>可靠性：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。&lt;/li>
&lt;li>一致性：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。&lt;/li>
&lt;li>原子性：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。&lt;/li>
&lt;li>隔离性：多个写入操作相互不影响。&lt;/li>
&lt;li>实时性：写入后是否可以立即被查询到。&lt;/li>
&lt;li>性能：写入性能，吞吐量到底怎么样。&lt;/li>
&lt;/ul>
&lt;p>Elasticsearch作为分布式系统，也需要在写入的时候满足上述的四个特点，我们在后面的写流程介绍中会涉及到上述四个方面。&lt;/p>
&lt;p>接下来,我们一层一层剖析Elasticsearch内部的写机制。&lt;/p>
&lt;h2 id="lucene的写">&lt;strong>Lucene的写&lt;/strong>&lt;/h2>
&lt;p>众所周知，Elasticsearch内部使用了Lucene完成索引创建和搜索功能，Lucene中写操作主要是通过IndexWriter类实现，IndexWriter提供三个接口：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">addDocument&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">updateDocuments&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">deleteDocuments&lt;/span>&lt;span class="o">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>通过这三个接口可以完成单个文档的写入，更新和删除功能，包括了分词，倒排创建，正排创建等等所有搜索相关的流程。只要Doc通过IndesWriter写入后，后面就可以通过IndexSearcher搜索了，看起来功能已经完善了，但是仍然有一些问题没有解：&lt;/p>
&lt;ol>
&lt;li>上述操作是单机的，而不是我们需要的分布式。&lt;/li>
&lt;li>文档写入Lucene后并不是立即可查询的，需要生成完整的Segment后才可被搜索，如何保证实时性？&lt;/li>
&lt;li>Lucene生成的Segment是在内存中，如果机器宕机或掉电后，内存中的Segment会丢失，如何保证数据可靠性 ？&lt;/li>
&lt;li>Lucene不支持部分文档更新，但是这又是一个强需求，如何支持部分更新？&lt;/li>
&lt;/ol>
&lt;p>上述问题，在Lucene中是没有解决的，那么就需要Elasticsearch中解决上述问题。&lt;/p>
&lt;p>Elasticsearch在解决上述问题时，除了我们在上一篇《Elasticsearch数据模型简介》中介绍的几种系统字段外，在引擎架构上也引入了多重机制来解决问题。我们再来看Elasticsearch中的写机制。&lt;/p>
&lt;h2 id="elasticsearch的写">&lt;strong>Elasticsearch的写&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。&lt;/p>
&lt;p>此外，Elasticsearch整体架构上采用了一主多副的方式：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-8203d235d8cfc14849012e6ea229fa89_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch一主多副&lt;/p>
&lt;p>每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。&lt;/p>
&lt;p>请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。&lt;/p>
&lt;p>这种模式下，写入操作的延时就等于latency = Latency(Primary Write) + Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。&lt;/p>
&lt;p>采用多个副本后，避免了单机或磁盘故障发生时，对已经持久化后的数据造成损害，但是Elasticsearch里为了减少磁盘IO保证读写性能，一般是每隔一段时间（比如5分钟）才会把Lucene的Segment写入磁盘持久化，对于写入内存，但还未Flush到磁盘的Lucene数据，如果发生机器宕机或者掉电，那么内存中的数据也会丢失，这时候如何保证？&lt;/p>
&lt;p>对于这种问题，Elasticsearch学习了数据库中的处理方式：增加CommitLog模块，Elasticsearch中叫TransLog。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-20a780ddd33a74b37a81e18d3baf8983_720w.jpg" alt="">&lt;/p>
&lt;p>Refresh &amp;amp;&amp;amp; Flush&lt;/p>
&lt;p>在每一个Shard中，写入流程分为两部分，先写入Lucene，再写入TransLog。&lt;/p>
&lt;p>写入请求到达Shard后，先写Lucene文件，创建好索引，此时索引还在内存里面，接着去写TransLog，写完TransLog后，刷新TransLog数据到磁盘上，写磁盘成功后，请求返回给用户。这里有几个关键点，一是和数据库不同，数据库是先写CommitLog，然后再写内存，而Elasticsearch是先写内存，最后才写TransLog，一种可能的原因是Lucene的内存写入会有很复杂的逻辑，很容易失败，比如分词，字段长度超过限制等，比较重，为了避免TransLog中有大量无效记录，减少recover的复杂度和提高速度，所以就把写Lucene放在了最前面。二是写Lucene内存后，并不是可被搜索的，需要通过Refresh把内存的对象转成完整的Segment后，然后再次reopen后才能被搜索，一般这个时间设置为1秒钟，导致写入Elasticsearch的文档，最快要1秒钟才可被从搜索到，所以Elasticsearch在搜索方面是NRT（Near Real Time）近实时的系统。三是当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。四是每隔一段比较长的时间，比如30分钟后，Lucene会把内存中生成的新Segment刷新到磁盘上，刷新后索引文件已经持久化了，历史的TransLog就没用了，会清空掉旧的TransLog。&lt;/p>
&lt;p>上面介绍了Elasticsearch在写入时的两个关键模块，Replica和TransLog，接下来，我们看一下Update流程：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-e728bc042a75f8798925d708dc61b1ef_720w.jpg" alt="">&lt;/p>
&lt;p>Update&lt;/p>
&lt;p>Lucene中不支持部分字段的Update，所以需要在Elasticsearch中实现该功能，具体流程如下：&lt;/p>
&lt;ol>
&lt;li>收到Update请求后，从Segment或者TransLog中读取同id的完整Doc，记录版本号为V1。&lt;/li>
&lt;li>将版本V1的全量Doc和请求中的部分字段Doc合并为一个完整的Doc，同时更新内存中的VersionMap。获取到完整Doc后，Update请求就变成了Index请求。&lt;/li>
&lt;li>加锁。&lt;/li>
&lt;li>再次从versionMap中读取该id的最大版本号V2，如果versionMap中没有，则从Segment或者TransLog中读取，这里基本都会从versionMap中获取到。&lt;/li>
&lt;li>检查版本是否冲突(V1==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的Add请求。&lt;/li>
&lt;li>在Index Doc阶段，首先将Version + 1得到V3，再将Doc加入到Lucene中去，Lucene中会先删同id下的已存在doc id，然后再增加新Doc。写入Lucene成功后，将当前V3更新到versionMap中。&lt;/li>
&lt;li>释放锁，部分更新的流程就结束了。&lt;/li>
&lt;/ol>
&lt;p>介绍完部分更新的流程后，大家应该从整体架构上对Elasticsearch的写入有了一个初步的映象，接下来我们详细剖析下写入的详细步骤。&lt;/p>
&lt;h2 id="elasticsearch写入请求类型">&lt;strong>Elasticsearch写入请求类型&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中的写入请求类型，主要包括下列几个：Index(Create)，Update，Delete和Bulk，其中前3个是单文档操作，后一个Bulk是多文档操作，其中Bulk中可以包括Index(Create)，Update和Delete。&lt;/p>
&lt;p>在6.0.0及其之后的版本中，前3个单文档操作的实现基本都和Bulk操作一致，甚至有些就是通过调用Bulk的接口实现的。估计接下来几个版本后，Index(Create)，Update，Delete都会被当做Bulk的一种特例化操作被处理。这样，代码和逻辑都会更清晰一些。&lt;/p>
&lt;p>下面，我们就以Bulk请求为例来介绍写入流程。&lt;/p>
&lt;h2 id="elasticsearch写入流程图">&lt;strong>Elasticsearch写入流程图&lt;/strong>&lt;/h2>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-4e32cd77e69ae4932665d110d6bf13a1_720w.jpg" alt="">&lt;/p>
&lt;p>写入流程图&lt;/p>
&lt;ul>
&lt;li>红色：Client Node。&lt;/li>
&lt;li>绿色：Primary Node。&lt;/li>
&lt;li>蓝色：Replica Node。&lt;/li>
&lt;/ul>
&lt;h2 id="注册action">&lt;strong>注册Action&lt;/strong>&lt;/h2>
&lt;p>在Elasticsearch中，所有action的入口处理方法都是注册在ActionModule.java中，比如Bulk Request有两个注册入口，分别是Rest和Transport入口：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-7506f5d87f80ec63cbd2030b785441ec_720w.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-ac3d0e6ee82d507d38110565d121febf_720w.jpg" alt="">&lt;/p>
&lt;p>如果请求是Rest请求，则会在RestBulkAction中Parse Request，构造出BulkRequest，然后发给后面的TransportAction处理。&lt;/p>
&lt;p>TransportShardBulkAction的基类TransportReplicationAction中注册了对Primary，Replica等的不同处理入口:&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-4ac69cd72079de47adfb2d81906579db_720w.jpg" alt="">&lt;/p>
&lt;p>这里对原始请求，Primary Node请求和Replica Node请求各自注册了一个handler处理入口。&lt;/p>
&lt;h2 id="client-node">&lt;strong>Client Node&lt;/strong>&lt;/h2>
&lt;p>Client Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。&lt;/p>
&lt;p>&lt;strong>1. Ingest Pipeline&lt;/strong>&lt;/p>
&lt;p>在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。&lt;/p>
&lt;p>如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node。&lt;/p>
&lt;p>&lt;strong>2. Auto Create Index&lt;/strong>&lt;/p>
&lt;p>判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能。&lt;/p>
&lt;p>&lt;strong>3. Set Routing&lt;/strong>&lt;/p>
&lt;p>设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。&lt;/p>
&lt;p>在这一步中，如果没有指定&lt;em>id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID。&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>4. Construct BulkShardRequest&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard。&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>5. Send Request To Primary&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node。&lt;/em>&lt;/p>
&lt;h2 id="primary-node">&lt;strong>Primary Node&lt;/strong>&lt;/h2>
&lt;p>Primary 请求的入口是在PrimaryOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。&lt;/p>
&lt;p>&lt;strong>1. Index or Update or Delete&lt;/strong>&lt;/p>
&lt;p>循环执行每个Single Write Request，对于每个Request，根据操作类型(&lt;em>CREATE/INDEX/UPDATE/DELETE&lt;/em>)选择不同的处理逻辑。&lt;/p>
&lt;p>其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍。&lt;/p>
&lt;p>&lt;strong>2. Translate Update To Index or Delete&lt;/strong>&lt;/p>
&lt;p>这一步是Update操作的特有步骤，在这里，会将Update请求转换为Index或者Delete请求。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1。&lt;/p>
&lt;p>&lt;strong>3. Parse Doc&lt;/strong>&lt;/p>
&lt;p>这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，_uid = type # _id，对用户，_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument。&lt;/p>
&lt;p>&lt;strong>4. Update Mapping&lt;/strong>&lt;/p>
&lt;p>Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping。&lt;/p>
&lt;p>&lt;strong>5. Get Sequence Id and Version&lt;/strong>&lt;/p>
&lt;p>由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增1。&lt;/p>
&lt;p>&lt;strong>6. Add Doc To Lucene&lt;/strong>&lt;/p>
&lt;p>这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。&lt;/p>
&lt;p>如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。&lt;/p>
&lt;p>这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。&lt;/p>
&lt;p>Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，后续会有文章专门介绍Lucene中的写入。&lt;/p>
&lt;p>&lt;strong>7. Write Translog&lt;/strong>&lt;/p>
&lt;p>写完Lucene的Segment后，会以keyvalue的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。&lt;/p>
&lt;p>需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。&lt;/p>
&lt;p>这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint。&lt;/p>
&lt;p>&lt;strong>8. Renew Bulk Request&lt;/strong>&lt;/p>
&lt;p>这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样。&lt;/p>
&lt;p>&lt;strong>9. Flush Translog&lt;/strong>&lt;/p>
&lt;p>这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大。&lt;/p>
&lt;p>&lt;strong>10. Send Requests To Replicas&lt;/strong>&lt;/p>
&lt;p>这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。&lt;/p>
&lt;p>这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。&lt;/p>
&lt;p>发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler。&lt;/p>
&lt;p>&lt;strong>11. Receive Response From Replicas&lt;/strong>&lt;/p>
&lt;p>Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint。&lt;/p>
&lt;h2 id="replica-node">&lt;strong>Replica Node&lt;/strong>&lt;/h2>
&lt;p>Replica 请求的入口是在ReplicaOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。&lt;/p>
&lt;p>&lt;strong>1. Index or Delete&lt;/strong>&lt;/p>
&lt;p>根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了。&lt;/p>
&lt;p>&lt;strong>2. Parse Doc&lt;/strong>&lt;/p>
&lt;p>&lt;strong>3. Update Mapping&lt;/strong>&lt;/p>
&lt;p>以上都和Primary Node中逻辑一致。&lt;/p>
&lt;p>&lt;strong>4. Get Sequence Id and Version&lt;/strong>&lt;/p>
&lt;p>Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行。&lt;/p>
&lt;p>&lt;strong>5. Add Doc To Lucene&lt;/strong>&lt;/p>
&lt;p>由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些。&lt;/p>
&lt;p>&lt;strong>6. Write Translog&lt;/strong>&lt;/p>
&lt;p>&lt;strong>7. Flush Translog&lt;/strong>&lt;/p>
&lt;p>以上都和Primary Node中逻辑一致。&lt;/p>
&lt;h2 id="最后">&lt;strong>最后&lt;/strong>&lt;/h2>
&lt;p>上面详细介绍了Elasticsearch的写入流程及其各个流程的工作机制，我们在这里再次总结下之前提出的分布式系统中的六大特性：&lt;/p>
&lt;ol>
&lt;li>可靠性：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。&lt;/li>
&lt;li>一致性：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现最终一致性。&lt;/li>
&lt;li>原子性：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。&lt;/li>
&lt;li>隔离性：仍然采用Version和局部锁来保证更新的是特定版本的数据。&lt;/li>
&lt;li>实时性：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。&lt;/li>
&lt;li>性能：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。后面我们也会有文章专门介绍Lucene中的优化思路。&lt;/li>
&lt;/ol></description></item><item><title>ElasticSearch基础</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/elasticsearch%E5%9F%BA%E7%A1%80/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/elasticsearch%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="elasticsearch基础">ElasticSearch基础&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch 是一个基于lucene的分布式可扩展的实时搜索和分析引擎。&lt;/p>
&lt;h2 id="特点">特点&lt;/h2>
&lt;ul>
&lt;li>分布式存储&lt;/li>
&lt;li>近实时检索&lt;/li>
&lt;/ul>
&lt;h2 id="核心概念">核心概念&lt;/h2>
&lt;ul>
&lt;li>索引(index)：&lt;/li>
&lt;li>分片(shard):&lt;/li>
&lt;li>分段(segment):&lt;/li>
&lt;li>Translog:&lt;/li>
&lt;/ul>
&lt;h3 id="写流程">写流程&lt;/h3>
&lt;h2 id="常用操作">常用操作&lt;/h2>
&lt;ul>
&lt;li>清空index数据&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># create index&lt;/span>
curl -X PUT http://192.168.0.10:20000/test6 --header &lt;span class="s2">&amp;#34;Content-Type: application/json&amp;#34;&lt;/span> -d index.json
cat index.json
&lt;span class="c1"># delete index&lt;/span>
curl -X DELETE http://192.168.0.10:20000/test6
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Elasticsearch数据模型</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="elasticsearch数据模型">Elasticsearch数据模型&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch是一个建立在全文搜索引擎库Apache Lucene 基础上的分布式搜索引擎，先来简单看一下Lucene中的一些数据模型：&lt;/p>
&lt;h2 id="lucene数据模型">Lucene数据模型&lt;/h2>
&lt;p>Lucene中包含了四种基本数据类型，分别是：&lt;/p>
&lt;ul>
&lt;li>Index：索引，由很多的Document组成。&lt;/li>
&lt;li>Document：由很多的Field组成，是Index和Search的最小单位。&lt;/li>
&lt;li>Field：由很多的Term组成，包括Field Name和Field Value。&lt;/li>
&lt;li>Term：由很多的字节组成，可以分词。&lt;/li>
&lt;/ul>
&lt;p>上述四种类型在Elasticsearch中同样存在，意思也一样。&lt;/p>
&lt;p>Lucene中存储的索引主要分为三种类型：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Invert Index：倒排索引，或者简称Index，通过Term可以查询到拥有该Term的文档。可以配置为是否分词，如果分词可以配置不同的分词器。索引存储的时候有多种存储类型，分别是：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS：只存储DocID。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS：存储DocID和词频（Term Freq）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS_AND_POSITIONS：存储DocID、词频（Term Freq）和位置。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：存储DocID、词频（Term Freq）、位置和偏移。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DocValues：正排索引，采用列式存储。通过DocID可以快速读取到该Doc的特定字段的值。由于是列式存储，性能会比较好。一般用于sort，agg等需要高频读取Doc字段值的场景。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Store：字段原始内容存储，同一篇文章的多个Field的Store会存储在一起，适用于一次读取少量且多个字段内存的场景，比如摘要等。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Lucene中提供索引和搜索的最小组织形式是Segment，Segment中按照索引类型不同，分成了Invert Index，Doc Values和Store这三大类（还有一些辅助类，这里省略），每一类里面都是按照Doc为最小单位存储。Invert Index中存储的Key是Term，Value是Doc ID的链表；Doc Value中Key 是Doc ID和Field Name，Value是Field Value；Store的Key是Doc ID，Value是Filed Name和Filed Value。&lt;/p>
&lt;p>由于Lucene中没有主键概念和更新逻辑，所有对Lucene的更新都是Append一个新Doc，类似于一个只能Append的队列，所有Doc都被同等对等，同样的处理方式。其中的Doc由众多Field组成，没有特殊Field，每个Field也都被同等对待，同样的处理方式。&lt;/p>
&lt;p>从上面介绍来看，Lucene只是提供了一个索引和查询的最基本的功能，距离一个完全可用的完整搜索引擎还有一些距离：&lt;/p>
&lt;h2 id="lucene的不足">Lucene的不足&lt;/h2>
&lt;ol>
&lt;li>Lucene是一个单机的搜索库，如何能以分布式形式支持海量数据?&lt;/li>
&lt;li>Lucene中没有更新，每次都是Append一个新文档，如何做部分字段的更新？&lt;/li>
&lt;li>Lucene中没有主键索引，如何处理同一个Doc的多次写入？&lt;/li>
&lt;li>在稀疏列数据中，如何判断某些文档是否存在特定字段？&lt;/li>
&lt;li>Lucene中生成完整Segment后，该Segment就不能再被更改，此时该Segment才能被搜索，这种情况下，如何做实时搜索？&lt;/li>
&lt;/ol>
&lt;p>上述几个问题，对于搜索而言都是至关重要的功能诉求，我们接下来看看Elasticsearch中是如何来解这些问题的。&lt;/p>
&lt;h2 id="elasticsearch怎么做">Elasticsearch怎么做&lt;/h2>
&lt;p>在Elasticsearch中，为了支持分布式，增加了一个系统字段_routing（路由），通过_routing将Doc分发到不同的Shard，不同的Shard可以位于不同的机器上，这样就能实现简单的分布式了。&lt;/p>
&lt;p>采用类似的方式，Elasticsearch增加了_id、_version、_source和_seq_no等等多个系统字段，通过这些Elasticsearch中特有的系统字段可以有效解决上述的几个问题，新增的系统字段主要是下列几个：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-02d4de08ebd1f37d1b99cc84728f1cf3_720w.jpg" alt="">&lt;/p>
&lt;p>下面我们逐个字段的剖析下上述系统字段的作用，先来看第一个_id字段：&lt;/p>
&lt;h2 id="1-_id">&lt;strong>1. _id&lt;/strong>&lt;/h2>
&lt;p>Doc的主键，在写入的时候，可以指定该Doc的ID值，如果不指定，则系统自动生成一个唯一的UUID值。&lt;/p>
&lt;p>Lucene中没有主键索引，要保证系统中同一个Doc不会重复，Elasticsearch引入了_id字段来实现主键。每次写入的时候都会先查询id，如果有，则说明已经有相同Doc存在了。&lt;/p>
&lt;p>通过_id值（ES内部转换成_uid）可以唯一在Elasticsearch中确定一个Doc。&lt;/p>
&lt;p>Elasticsearch中，_id只是一个用户级别的虚拟字段，在Elasticsearch中并不会映射到Lucene中，所以也就不会存储该字段的值。&lt;/p>
&lt;p>_id的值可以由_uid解析而来（_uid =type + &amp;lsquo;#&amp;rsquo; + id），Elasticsearch中会存储_uid。&lt;/p>
&lt;h2 id="2-_uid">&lt;strong>2. _uid&lt;/strong>&lt;/h2>
&lt;p>_uid的格式是：type + &amp;lsquo;#&amp;rsquo; + id。&lt;/p>
&lt;p>_uid会存储在Lucene中，在Lucene中的映射关系如下：dex下可能存在多个id值相同的Doc，而6.0.0之后只支持单Type，同Index下id值是唯一的。&lt;/p>
&lt;p>uid会存储在Lucene中，在Lucene中的映射关系如下：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-b854f5d9505615613184bba5fe760088_720w.jpg" alt="">&lt;/p>
&lt;p>_uid 只是存储了倒排Index和原文store：倒排Index的目的是可以通过_id快速查询到文档；原文store用来在返回的Response里面填充完整的_id值。&lt;/p>
&lt;p>在Lucene中存储_uid，而不是_id的原因是，在6.0.0之前版本里面，_uid可以比_id表示更多的信息，比如Type。在6.0.0版本之后，同一个Index只能有一个Type，这时候Type就没多大意义了，后面Type应该会消失，那时候_id就会和_uid概念一样，到时候两者会合二为一，也能简化大家的理解。&lt;/p>
&lt;h2 id="3-_version">&lt;strong>3. _version&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中每个Doc都会有一个Version，该Version可以由用户指定，也可以由系统自动生成。如果是系统自动生成，那么每次Version都是递增1。&lt;/p>
&lt;p>_version是实时的，不受搜索的近实时性影响，原因是可以通过_uid从内存中versionMap或者TransLog中读取到。&lt;/p>
&lt;p>Version在Lucene中也是映射为一个特殊的Field存在。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-a529eeb14094626f3523a92e0dfdb299_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中Version字段的主要目的是通过doc_id读取Version，所以Version只要存储为DocValues就可以了，类似于KeyValue存储。&lt;/p>
&lt;p>Elasticsearch通过使用version来保证对文档的变更能以正确的顺序执行，避免乱序造成的数据丢失：&lt;/p>
&lt;ol>
&lt;li>首次写入Doc的时候，会为Doc分配一个初始的Version：V0，该值根据VersionType不同而不同。&lt;/li>
&lt;li>再次写入Doc的时候，如果Request中没有指定Version，则会先加锁，然后去读取该Doc的最大版本V1，然后将V1+1后的新版本写入Lucene中。&lt;/li>
&lt;li>再次写入Doc的时候，如果Request中指定了Version：V1，则继续会先加锁，然后去读该Doc的最大版本V2，判断V1==V2，如果不相等，则发生版本冲突。否则版本吻合，继续写入Lucene。&lt;/li>
&lt;li>当做部分更新的时候，会先通过GetRequest读取当前id的完整Doc和V1，接着和当前Request中的Doc合并为一个完整Doc。然后执行一些逻辑后，加锁，再次读取该Doc的最大版本号V2，判断V1==V2，如果不相等，则在刚才执行其他逻辑时被其他线程更改了当前文档，需要报错后重试。如果相等，则期间没有其他线程修改当前文档，继续写入Lucene中。这个过程就是一个典型的read-then-update事务。&lt;/li>
&lt;/ol>
&lt;h2 id="4-_source">&lt;strong>4. _source&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中有一个重要的概念是source，存储原始文档，也可以通过过滤设置只存储特定Field。&lt;/p>
&lt;p>Source在Lucene中也是映射为了一个特殊的Field存在：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-5faeffae9ed270c1030c83499287dfc7_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_source字段的主要目的是通过doc_id读取该文档的原始内容，所以只需要存储Store即可。&lt;/p>
&lt;p>_source其实是名为_source的虚拟Store Field。&lt;/p>
&lt;p>Elasticsearch中使用_source字段可以实现以下功能：&lt;/p>
&lt;ul>
&lt;li>Update：部分更新时，需要从读取文档保存在_source字段中的原文，然后和请求中的部分字段合并为一个完整文档。如果没有_source，则不能完成部分字段的Update操作。&lt;/li>
&lt;li>Rebuild：最新的版本中新增了rebuild接口，可以通过Rebuild API完成索引重建，过程中不需要从其他系统导入全量数据，而是从当前文档的_source中读取。如果没有_source，则不能使用Rebuild API。&lt;/li>
&lt;li>Script：不管是Index还是Search的Script，都可能用到存储在Store中的原始内容，如果禁用了_source，则这部分功能不再可用。&lt;/li>
&lt;li>Summary：摘要信息也是来源于_source字段。&lt;/li>
&lt;/ul>
&lt;h2 id="5-_seq_no">&lt;strong>5. _seq_no&lt;/strong>&lt;/h2>
&lt;p>严格递增的顺序号，每个文档一个，Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。&lt;/p>
&lt;p>任何类型的写操作，包括index、create、update和Delete，都会生成一个_seq_no。&lt;/p>
&lt;p>_seq_no在Primary Node中由SequenceNumbersService生成，但其实真正产生这个值的是LocalCheckpointTracker，每次递增1：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="cm">/**
&lt;/span>&lt;span class="cm"> * The next available sequence number.
&lt;/span>&lt;span class="cm"> */&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">volatile&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">nextSeqNo&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="cm">/**
&lt;/span>&lt;span class="cm"> * Issue the next sequence number.
&lt;/span>&lt;span class="cm"> *
&lt;/span>&lt;span class="cm"> * @return the next assigned sequence number
&lt;/span>&lt;span class="cm"> */&lt;/span>
&lt;span class="kd">synchronized&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">generateSeqNo&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">nextSeqNo&lt;/span>&lt;span class="o">++;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>每个文档在使用Lucene的document操作接口之前，会获取到一个_seq_no，这个_seq_no会以系统保留Field的名义存储到Lucene中，文档写入Lucene成功后，会标记该seq_no为完成状态，这时候会使用当前seq_no更新local_checkpoint。&lt;/p>
&lt;p>checkpoint分为local_checkpoint和global_checkpoint，主要是用于保证有序性，以及减少Shard恢复时数据拷贝的数据拷贝量，更详细的介绍可以看这篇文章：&lt;a href="https://link.zhihu.com/?target=https%3A//www.elastic.co/blog/elasticsearch-sequence-ids-6-0">Sequence IDs: Coming Soon to an Elasticsearch Cluster Near You&lt;/a>。&lt;/p>
&lt;p>_seq_no在Lucene中的映射：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-a90804c654226608954774a639768afd_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_seq_no的作用有两个，一是通过doc_id查询到该文档的seq_no，二是通过seq_no范围查找相关文档，所以也就需要存储为Index和DocValues（或者Store）。由于是在冲突检测时才需要读取文档的_seq_no，而且此时只需要读取_seq_no，不需要其他字段，这时候存储为列式存储的DocValues比Store在性能上更好一些。&lt;/p>
&lt;p>_seq_no是严格递增的，写入Lucene的顺序也是递增的，所以DocValues存储类型可以设置为Sorted。&lt;/p>
&lt;p>另外，_seq_no的索引应该仅需要支持存储DocId就可以了，不需要FREQS、POSITIONS和分词。如果多存储了这些，对功能也没影响，就是多占了一点资源而已。&lt;/p>
&lt;h2 id="6-_primary_term">&lt;strong>6. _primary_term&lt;/strong>&lt;/h2>
&lt;p>_primary_term也和_seq_no一样是一个整数，每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。&lt;/p>
&lt;p>_primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-4a4184826fdd0a3d51001d3de3a57fb2_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_primary_term只需要通过doc_id读取到即可，所以只需要保存为DocValues就可以了.&lt;/p>
&lt;h2 id="7-_routing">&lt;strong>7. _routing&lt;/strong>&lt;/h2>
&lt;p>路由规则，写入和查询的routing需要一致，否则会出现写入的文档没法被查到情况。&lt;/p>
&lt;p>在mapping中，或者Request中可以指定按某个字段路由。默认是按照_Id值路由。&lt;/p>
&lt;p>_routing在Lucene中映射为：&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-a401f30a09d5f75b4adf79150544f536_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中文档级别的_routing主要有两个目的，一是可以查询到使用某种_routing的文档有哪些，当发生_routing变化时，可以对历史_routing的文档重新读取再Index，这个需要倒排Index。另一个是查询到文档后，在Response里面展示该文档使用的_routing规则，这里需要存储为Store。&lt;/p>
&lt;h2 id="8-_field_names">&lt;strong>8. _field_names&lt;/strong>&lt;/h2>
&lt;p>该字段会索引某个Field的名称，用来判断某个Doc中是否存在某个Field，用于exists或者missing请求。&lt;/p>
&lt;p>_field_names在Lucene中的映射：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-81d5dbc1f9e816879ad063b2fc1e9ba9_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_field_names的目的是查询哪些Doc的这个Field是否存在，所以只需要倒排Index即可。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2></description></item><item><title>esrally for es on cfs</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/esrally/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/esrally/</guid><description>&lt;h1 id="esrally-for-es-on-cfs">esrally for es on cfs&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>esrally 是 elastic 官方开源的一款基于 python3 实现的针对 es 的压测工具，主要功能如下：&lt;/p>
&lt;ul>
&lt;li>自动创建、压测和销毁 es 集群&lt;/li>
&lt;li>可分 es 版本管理压测数据和方案&lt;/li>
&lt;li>完善的压测数据展示，支持不同压测之间的数据对比分析，也可以将数据存储到指定的es中进行二次分析&lt;/li>
&lt;li>支持收集 JVM 详细信息，比如内存、GC等数据来定位性能问题&lt;/li>
&lt;/ul>
&lt;h2 id="安装测试">安装测试&lt;/h2>
&lt;h3 id="测试环境">测试环境&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>节点类型&lt;/th>
&lt;th>节点数&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>内存&lt;/th>
&lt;th>存储&lt;/th>
&lt;th>网络&lt;/th>
&lt;th>备注&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>管理节点&lt;/td>
&lt;td>3&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>120 GB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>元数据节点&lt;/td>
&lt;td>10&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>16 x 1TB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>混合部署&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>数据节点&lt;/td>
&lt;td>10&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>16 x 1TB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>混合部署&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="cfs配置">cfs配置&lt;/h3>
&lt;ul>
&lt;li>创建cfs vol&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="cp">#!/bin/sh
&lt;/span>&lt;span class="cp">&lt;/span>&lt;span class="c1"># ./create_vol.sh&lt;/span>
&lt;span class="nv">VolNames&lt;/span>&lt;span class="o">=&lt;/span>estest
&lt;span class="nv">leader&lt;/span>&lt;span class="o">=&lt;/span>10.194.139.42:8080 &lt;span class="c1">#cfs master leader节点的ip&lt;/span>
&lt;span class="nv">Capacity&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">1024&lt;/span> &lt;span class="c1">#unit GB&lt;/span>
&lt;span class="nv">Owner&lt;/span>&lt;span class="o">=&lt;/span>es01
&lt;span class="nv">DpCount&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">500&lt;/span>
&lt;span class="c1"># 创建vol&lt;/span>
curl &lt;span class="s2">&amp;#34;http://&lt;/span>&lt;span class="nv">$leader&lt;/span>&lt;span class="s2">/admin/createVol?name=&lt;/span>&lt;span class="nv">$VolName&lt;/span>&lt;span class="s2">&amp;amp;replicas=3&amp;amp;type=extent&amp;amp;capacity=&lt;/span>&lt;span class="nv">$Capacity&lt;/span>&lt;span class="s2">&amp;amp;owner=&lt;/span>&lt;span class="nv">$Owner&lt;/span>&lt;span class="s2">&amp;amp;followerRead=true&amp;#34;&lt;/span>
&lt;span class="c1"># 创建dp&lt;/span>
curl &lt;span class="s2">&amp;#34;http://&lt;/span>&lt;span class="nv">$leader&lt;/span>&lt;span class="s2">/dataPartition/create?count=&lt;/span>&lt;span class="nv">$DpCount&lt;/span>&lt;span class="s2">&amp;amp;name=&lt;/span>&lt;span class="nv">$VolName&lt;/span>&lt;span class="s2">&amp;amp;type=extent&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>
&lt;p>挂载cfs vol：&lt;/p>
&lt;p>在es运行节点运行cfs-client，挂载cfs vol到指定目录：&lt;code>/mnt/cfs&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ &lt;span class="nb">cd&lt;/span> &lt;span class="nv">$CFS_ROOT&lt;/span>
$ bin/cfs-client -c conf/cfs-client.json
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>cfs client 配置文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="err">//&lt;/span> &lt;span class="err">cfs-client.json&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;mountPoint&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/mnt/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;volName&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;estest&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;owner&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;es01&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;masterAddr&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;10.194.139.42:8080,10.194.139.44:8080,10.194.139.45:8080&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;logDir&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/export/Logs/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;warnLogDir&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/export/Logs/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;logLevel&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;debug&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;consulAddr&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;http://cbconsul-cfs01.cbmonitor.svc.ht7.n.jd.local&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;exporterPort&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">9613&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;profPort&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;11094&amp;#34;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="es配置">es配置&lt;/h3>
&lt;p>修改es配置文件&lt;code>elasticsearch.yml&lt;/code>中配置项&lt;code>path.data&lt;/code>为cfs挂载目录:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="c"># elasticsearch.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="c">#...&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">path.data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/mnt/cfs/es/&amp;lt;HOST_NAME&amp;gt; &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">## HOST_NAME为节点主机名，如果节点运行多个es，每个es需配置不同的目录&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="c">#...&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="esrally">esrally&lt;/h3>
&lt;ul>
&lt;li>centos7&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ yum install -y python3 python3-devel
&lt;span class="c1"># install git&lt;/span>
$ yum install -y http://opensource.wandisco.com/centos/6/git/x86_64/wandisco-git-release-6-1.noarch.rpm
$ yum install -y git
$ pip3 install esrally
$ &lt;span class="nv">target_hosts&lt;/span>&lt;span class="o">=&lt;/span>10.194.132.2:20000,10.194.132.5:20000,10.194.132.71:20000,10.194.134.196:20000
$ esrally --track&lt;span class="o">=&lt;/span>pmc &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --target-hosts&lt;span class="o">=&lt;/span>&lt;span class="nv">$target_hosts&lt;/span> &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --pipeline&lt;span class="o">=&lt;/span>benchmark-only
$ esrally --pipeline&lt;span class="o">=&lt;/span>benchmark-only &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --track&lt;span class="o">=&lt;/span>http_logs &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --target-hosts&lt;span class="o">=&lt;/span>&lt;span class="nv">$target_hosts&lt;/span> &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --report-file&lt;span class="o">=&lt;/span>/tmp/report_http_logs.md
&lt;span class="c1"># 指定集群，运行测试，test-mode参数只会运行1000条文档 es集群必须处理green状态，否则会被禁止race&lt;/span>
&lt;span class="c1"># 去掉--offline --test-mode可以让其把相关文件夹创建，然后结束掉&lt;/span>
esrally --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
esrally race --track&lt;span class="o">=&lt;/span>geonames --challenge&lt;span class="o">=&lt;/span>append-no-conflicts --user-tag&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;car:1g&amp;#34;&lt;/span> --car&lt;span class="o">=&lt;/span>1gheap --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
esrally race --track&lt;span class="o">=&lt;/span>geonames --challenge&lt;span class="o">=&lt;/span>append-no-conflicts --user-tag&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;car:2g&amp;#34;&lt;/span> --car&lt;span class="o">=&lt;/span>2gheap --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
&lt;span class="c1"># 对比2次的测试结果，根据esrally list races显示的时间戳当参数值，如果报错就使用Race ID&lt;/span>
esrally compare --baseline&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Race ID&amp;#39;&lt;/span> --contender&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Race ID&amp;#39;&lt;/span>
&lt;span class="c1"># 修改集群的分片数和副本数&lt;/span>
vim /home/esrally/.rally/benchmarks/tracks/default/geonames/index.json
vim /home/esrally/.rally/benchmarks/tracks/default/geonames/challenges/default.json
&lt;span class="c1"># 常用命令&lt;/span>
esrally list tracks
esrally list cars
esrally list races
esrally list pipelines
$ esrally list races
$ esrally compare --baseline 30889a15-66d3-4336-b6cb-0304834d853a --contender 1d1d5a98-54cb-486d-8339-98fe72ff054c
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="测试结果">测试结果&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>metric&lt;/th>
&lt;th>local-r0&lt;/th>
&lt;th>local-r1&lt;/th>
&lt;th>local-r2&lt;/th>
&lt;th>cfs-r0&lt;/th>
&lt;th>cfs-r1&lt;/th>
&lt;th>cfs-r2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Cumulative indexing time of primary&lt;br> shards&lt;/td>
&lt;td>48.1956&lt;/td>
&lt;td>50.1214&lt;/td>
&lt;td>53.6981&lt;/td>
&lt;td>61.514&lt;/td>
&lt;td>60.6874&lt;/td>
&lt;td>61.6843&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative merge time of primary shards&lt;/td>
&lt;td>19.3734&lt;/td>
&lt;td>19.759&lt;/td>
&lt;td>16.3446&lt;/td>
&lt;td>20.7753&lt;/td>
&lt;td>15.1702&lt;/td>
&lt;td>5.61673&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative refresh time of primary shard&lt;/td>
&lt;td>6.52128&lt;/td>
&lt;td>5.99413&lt;/td>
&lt;td>5.61948&lt;/td>
&lt;td>12.7984&lt;/td>
&lt;td>10.6824&lt;/td>
&lt;td>9.8606&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative flush time of primary shards&lt;/td>
&lt;td>0.00513&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1.67E-05&lt;/td>
&lt;td>0.0028&lt;/td>
&lt;td>0.0147333&lt;/td>
&lt;td>0.03075&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2020-06-03-13-36-05-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>local-r0、local-r1, local-r2: 分别表示es    &lt;code>path.data&lt;/code>使用本地磁盘，replica分别为0，1，2时的数据；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cfs-r0，cfs-r1, cfs-r2分别表示es &lt;code>path.data&lt;/code>使用cfs 卷，replica分别为0,1,2时esrally的数据；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://esrally.readthedocs.io/en/latest/">https://esrally.readthedocs.io/en/latest/&lt;/a>&lt;u>summary_report&lt;/u>.html&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/c89975b50447">https://www.jianshu.com/p/c89975b50447&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://esrally.lyremelody.org/zh_CN/latest/quickstart.html">快速入门 — Rally 0.9.0 文档&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>ES倒排索引原理</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/</guid><description>&lt;h1 id="es倒排索引原理">ES倒排索引原理&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch通过Lucene的倒排索引技术实现比关系型数据库更快的过滤。它对多条件的过滤支持非常好，比如年龄在18和30之间，性别为女性这样的组合查询。倒排索引很多地方都有介绍，但是其比关系型数据库的b-tree索引快在哪里？到底为什么快呢？&lt;/p>
&lt;p>笼统的来说，b-tree索引是为写入优化的索引结构。当我们不需要支持快速的更新的时候，可以用预先排序等方式换取更小的存储空间，更快的检索速度等好处，其代价就是更新慢。要进一步深入的化，还是要看一下Lucene的倒排索引是怎么构成的。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-378bc62acf1a493c402291a8f8e99e6a_720w.jpg" alt="">&lt;/p>
&lt;p>这里有好几个概念。我们来看一个实际的例子，假设有如下的数据：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-e6b81003803254b1d11b3384626c93ab_720w.jpg" alt="">&lt;/p>
&lt;p>这里每一行是一个document。每个document都有一个docid。那么给这些document建立的倒排索引就是：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-c1cf40e4c4218fd3e992258c08e4e334_720w.jpg" alt="">&lt;/p>
&lt;p>可以看到，倒排索引是per field的，一个字段由一个自己的倒排索引。18,20这些叫做 term，而[1,3]就是posting list。Posting list就是一个int的数组，存储了所有符合某个term的文档id。那么什么是term dictionary 和 term index？&lt;/p>
&lt;p>假设我们有很多个term，比如：&lt;/p>
&lt;p>Carla,Sara,Elin,Ada,Patty,Kate,Selena&lt;/p>
&lt;p>如果按照这样的顺序排列，找出某个特定的term一定很慢，因为term没有排序，需要全部过滤一遍才能找出特定的term。排序之后就变成了：&lt;/p>
&lt;p>Ada,Carla,Elin,Kate,Patty,Sara,Selena&lt;/p>
&lt;p>这样我们可以用二分查找的方式，比全遍历更快地找出目标的term。这个就是 term dictionary。有了term dictionary之后，可以用 logN 次磁盘查找得到目标。但是磁盘的随机读操作仍然是非常昂贵的（一次random access大概需要10ms的时间）。所以尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个term dictionary本身又太大了，无法完整地放到内存里。于是就有了term index。term index有点像一本字典的大的章节表。比如：&lt;/p>
&lt;p>A开头的term ……………. Xxx页&lt;/p>
&lt;p>C开头的term ……………. Xxx页&lt;/p>
&lt;p>E开头的term ……………. Xxx页&lt;/p>
&lt;p>如果所有的term都是英文字符的话，可能这个term index就真的是26个英文字符表构成的了。但是实际的情况是，term未必都是英文字符，term可以是任意的byte数组。而且26个英文字符也未必是每一个字符都有均等的term，比如x字符开头的term可能一个都没有，而s开头的term又特别多。实际的term index是一棵trie 树：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-e4632ac1392b01f7a39d963fddb1a1e0_720w.jpg" alt="">&lt;/p>
&lt;p>例子是一个包含 &amp;ldquo;A&amp;rdquo;, &amp;ldquo;to&amp;rdquo;, &amp;ldquo;tea&amp;rdquo;, &amp;ldquo;ted&amp;rdquo;, &amp;ldquo;ten&amp;rdquo;, &amp;ldquo;i&amp;rdquo;, &amp;ldquo;in&amp;rdquo;, 和 &amp;ldquo;inn&amp;rdquo; 的 trie 树。这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。再加上一些压缩技术（搜索 Lucene Finite State Transducers） term index 的尺寸可以只有所有term的尺寸的几十分之一，使得用内存缓存整个term index变成可能。整体上来说就是这样的效果。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-e4599b618e270df9b64a75eb77bfb326_720w.jpg" alt="">&lt;/p>
&lt;p>现在我们可以回答“为什么Elasticsearch/Lucene检索可以比mysql快了。Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。&lt;/p>
&lt;p>额外值得一提的两点是：term index在内存中是以FST（finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary在磁盘上是以分block的方式保存的，一个block内部利用公共前缀压缩，比如都是Ab开头的单词就可以把Ab省去。这样term dictionary可以比b-tree更节约磁盘空间。&lt;/p>
&lt;h2 id="如何联合索引查询">如何联合索引查询？&lt;/h2>
&lt;p>所以给定查询过滤条件 age=18 的过程就是先从term index找到18在term dictionary的大概位置，然后再从term dictionary里精确地找到18这个term，然后得到一个posting list或者一个指向posting list位置的指针。然后再查询 gender=女 的过程也是类似的。最后得出 age=18 AND gender=女 就是把两个 posting list 做一个“与”的合并。&lt;/p>
&lt;p>这个理论上的“与”合并的操作可不容易。对于mysql来说，如果你给age和gender两个字段都建立了索引，查询的时候只会选择其中最selective的来用，然后另外一个条件是在遍历行的过程中在内存中计算之后过滤掉。那么要如何才能联合使用两个索引呢？有两种办法：&lt;/p>
&lt;ul>
&lt;li>使用skip list数据结构。同时遍历gender和age的posting list，互相skip；&lt;/li>
&lt;li>使用bitset数据结构，对gender和age两个filter分别求出bitset，对两个bitset做AN操作。&lt;/li>
&lt;/ul>
&lt;p>PostgreSQL 从 8.4 版本开始支持通过bitmap联合使用两个索引，就是利用了bitset数据结构来做到的。当然一些商业的关系型数据库也支持类似的联合索引的功能。Elasticsearch支持以上两种的联合索引方式，如果查询的filter缓存到了内存中（以bitset的形式），那么合并就是两个bitset的AND。如果查询的filter没有缓存，那么就用skip list的方式去遍历两个on disk的posting list。&lt;/p>
&lt;h2 id="利用-skip-list-合并">利用 Skip List 合并&lt;/h2>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-eafa46683272ff1b2081edbc8db5469f_720w.jpg" alt="">&lt;/p>
&lt;p>以上是三个posting list。我们现在需要把它们用AND的关系合并，得出posting list的交集。首先选择最短的posting list，然后从小到大遍历。遍历的过程可以跳过一些元素，比如我们遍历到绿色的13的时候，就可以跳过蓝色的3了，因为3比13要小。&lt;/p>
&lt;p>整个过程如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">Next -&amp;gt; 2
Advance(2) -&amp;gt; 13
Advance(13) -&amp;gt; 13
Already on 13
Advance(13) -&amp;gt; 13 MATCH!!!
Next -&amp;gt; 17
Advance(17) -&amp;gt; 22
Advance(22) -&amp;gt; 98
Advance(98) -&amp;gt; 98
Advance(98) -&amp;gt; 98 MATCH!!!
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后得出的交集是[13,98]，所需的时间比完整遍历三个posting list要快得多。但是前提是每个list需要指出Advance这个操作，快速移动指向的位置。什么样的list可以这样Advance往前做蛙跳？skip list：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-a8b78c8e861c34a1afd7891284852b34_720w.jpg" alt="">&lt;/p>
&lt;p>从概念上来说，对于一个很长的posting list，比如：&lt;/p>
&lt;p>[1,3,13,101,105,108,255,256,257]&lt;/p>
&lt;p>我们可以把这个list分成三个block：&lt;/p>
&lt;p>[1,3,13] [101,105,108] [255,256,257]&lt;/p>
&lt;p>然后可以构建出skip list的第二层：&lt;/p>
&lt;p>[1,101,255]&lt;/p>
&lt;p>1,101,255分别指向自己对应的block。这样就可以很快地跨block的移动指向位置了。&lt;/p>
&lt;p>Lucene自然会对这个block再次进行压缩。其压缩方式叫做Frame Of Reference编码。示例如下：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-9c03d3e449e3f8fb8182287048ad6db7_720w.jpg" alt="">&lt;/p>
&lt;p>考虑到频繁出现的term（所谓low cardinality的值），比如gender里的男或者女。如果有1百万个文档，那么性别为男的posting list里就会有50万个int值。用Frame of Reference编码进行压缩可以极大减少磁盘占用。这个优化对于减少索引尺寸有非常重要的意义。当然mysql b-tree里也有一个类似的posting list的东西，是未经过这样压缩的。&lt;/p>
&lt;p>因为这个Frame of Reference的编码是有解压缩成本的。利用skip list，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的block的过程，从而节省了cpu。&lt;/p>
&lt;h2 id="利用bitset合并">利用bitset合并&lt;/h2>
&lt;p>Bitset是一种很直观的数据结构，对应posting list如：&lt;/p>
&lt;p>[1,3,4,7,10]&lt;/p>
&lt;p>对应的bitset就是：&lt;/p>
&lt;p>[1,0,1,1,0,0,1,0,0,1]&lt;/p>
&lt;p>每个文档按照文档id排序对应其中的一个bit。Bitset自身就有压缩的特点，其用一个byte就可以代表8个文档。所以100万个文档只需要12.5万个byte。但是考虑到文档可能有数十亿之多，在内存里保存bitset仍然是很奢侈的事情。而且对于个每一个filter都要消耗一个bitset，比如age=18缓存起来的话是一个bitset，18&amp;lt;=age&amp;lt;25是另外一个filter缓存起来也要一个bitset。&lt;/p>
&lt;p>所以秘诀就在于需要有一个数据结构：&lt;/p>
&lt;ul>
&lt;li>可以很压缩地保存上亿个bit代表对应的文档是否匹配filter；&lt;/li>
&lt;li>这个压缩的bitset仍然可以很快地进行AND和 OR的逻辑操作。&lt;/li>
&lt;/ul>
&lt;p>Lucene使用的这个数据结构叫做 Roaring Bitmap。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-9482b84c4aa3fb77a959c1ead553037e_720w.jpg" alt="">&lt;/p>
&lt;p>其压缩的思路其实很简单。与其保存100个0，占用100个bit。还不如保存0一次，然后声明这个0重复了100遍。&lt;/p>
&lt;p>这两种合并使用索引的方式都有其用途。Elasticsearch对其性能有详细的对比（&lt;a href="https://link.zhihu.com/?target=https%3A//www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps">https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps&lt;/a>）。简单的结论是：因为Frame of Reference编码是如此 高效，对于简单的相等条件的过滤缓存成纯内存的bitset还不如需要访问磁盘的skip list的方式要快。&lt;/p>
&lt;h2 id="如何减少文档数">如何减少文档数？&lt;/h2>
&lt;p>一种常见的压缩存储时间序列的方式是把多个数据点合并成一行。Opentsdb支持海量数据的一个绝招就是定期把很多行数据合并成一行，这个过程叫compaction。类似的vivdcortext使用mysql存储的时候，也把一分钟的很多数据点合并存储到mysql的一行里以减少行数。&lt;/p>
&lt;p>这个过程可以示例如下：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-252d8f8ebe62e508f62049e80a9b9468_720w.jpg" alt="">&lt;/p>
&lt;p>可以看到，行变成了列了。每一列可以代表这一分钟内一秒的数据。&lt;/p>
&lt;p>Elasticsearch有一个功能可以实现类似的优化效果，那就是Nested Document。我们可以把一段时间的很多个数据点打包存储到一个父文档里，变成其嵌套的子文档。示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">{timestamp:12:05:01, idc:sz, value1:10,value2:11}
{timestamp:12:05:02, idc:sz, value1:9,value2:9}
{timestamp:12:05:02, idc:sz, value1:18,value:17}
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以打包成：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>
    &lt;span class="err">max_timestamp:12:05:02,&lt;/span>
    &lt;span class="err">min_timestamp:&lt;/span> &lt;span class="err">1205:01,&lt;/span>
    &lt;span class="err">idc:sz,&lt;/span>
    &lt;span class="err">records:&lt;/span> &lt;span class="err">[&lt;/span>
&lt;span class="err">{timestamp:12:05:01,&lt;/span> &lt;span class="err">value1:10,value2:11&lt;/span>&lt;span class="p">}&lt;/span>
        &lt;span class="p">{&lt;/span>&lt;span class="err">timestamp:12:05:02,&lt;/span> &lt;span class="err">value1:9,value2:9&lt;/span>&lt;span class="p">}&lt;/span>
        &lt;span class="p">{&lt;/span>&lt;span class="err">timestamp:12:05:02,&lt;/span> &lt;span class="err">value1:18,value:17&lt;/span>&lt;span class="p">}&lt;/span>
    &lt;span class="err">]&lt;/span>
&lt;span class="err">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这样可以把数据点公共的维度字段上移到父文档里，而不用在每个子文档里重复存储，从而减少索引的尺寸。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-917578288797efab8f67e7b74d5ec6a3_720w.jpg" alt="">&lt;/p>
&lt;p>在存储的时候，无论父文档还是子文档，对于Lucene来说都是文档，都会有文档Id。但是对于嵌套文档来说，可以保存起子文档和父文档的文档id是连续的，而且父文档总是最后一个。有这样一个排序性作为保障，那么有一个所有父文档的posting list就可以跟踪所有的父子关系。也可以很容易地在父子文档id之间做转换。把父子关系也理解为一个filter，那么查询时检索的时候不过是又AND了另外一个filter而已。前面我们已经看到了Elasticsearch可以非常高效地处理多filter的情况，充分利用底层的索引。&lt;/p>
&lt;p>使用了嵌套文档之后，对于term的posting list只需要保存父文档的doc id就可以了，可以比保存所有的数据点的doc id要少很多。如果我们可以在一个父文档里塞入50个嵌套文档，那么posting list可以变成之前的1/50。&lt;/p></description></item><item><title>ES冷热分离</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB/</guid><description>&lt;h1 id="es冷热分离">ES冷热分离&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>elasticsearch 从6.6版本增加了冷热(hot-warm)特性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>该特性可以将同一个es集群中的不同es节点根据硬件性能分为&lt;code>hot&lt;/code>/&lt;code>warm&lt;/code>不同的类型(node type)。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="配置">配置&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>条件： elasticsearch version &amp;gt;= 6.6.0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>配置&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="c"># $ES_HOME/config/elasticsearch.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># $ES_HOME/start.sh&lt;/span>
&lt;span class="nv">OPTS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34; -Enode.attr.box_type=hot -Enode.attr.resource_level=high -p &lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">PID_FILE&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> -d &amp;#34;&lt;/span>
bin/elasticsearch &lt;span class="nv">$OPTS&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>迁移节点到温节点&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">curl -X PUT /&amp;lt;INDEX_NAME&amp;gt;/_settings
&lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;settings&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;index.routing.allocation.require.box_type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;warm&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://www.elastic.co/cn/blog/hot-warm-architecture-in-elasticsearch-5-x">Elasticsearch Hot Warm Architecture | Elastic Blog&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.elastic.co/cn/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management">使用索引生命周期管理在 Elasticsearch 中实现热温冷架构 | Elastic Blog&lt;/a>&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>ES存储详解</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%AD%98%E5%82%A8%E8%AF%A6%E8%A7%A3/</guid><description>&lt;h1 id="es存储详解">ES存储详解&lt;/h1>
&lt;h2 id="elasticsearch路径">Elasticsearch路径&lt;/h2>
&lt;p>Elasticsearch配置了多个路径：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>path.home&lt;/code>：运行Elasticsearch进程的用户的主目录。默认为Java系统属性user.dir，它是进程所有者的默认主目录。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>path.conf&lt;/code>：包含配置文件的目录。这通常通过设置Java系统属性es.config来设置，因为在找到配置文件之前它必然会被解析。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>path.plugins&lt;/code>：子文件夹为Elasticsearch插件的目录。这里支持Sym-links，当从同一个可执行文件运行多个Elasticsearch实例时，可以使用它来有选择地启用/禁用某个Elasticsearch实例的一组插件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>path.logs&lt;/code>：存储生成的日志的位置。如果其中一个卷的磁盘空间不足，则将它放在与数据目录不同的卷上可能是有意义的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>path.data&lt;/code>：包含Elasticsearch存储的数据的文件夹的路径。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>在本文中，我们将仔细研究数据目录（path.data）的实际内容，并尝试了解所有文件的用途。&lt;/p>
&lt;h3 id="2文件从哪里来">2、文件从哪里来？&lt;/h3>
&lt;p>由于Elasticsearch使用Lucene来处理分片级别的索引和查询，因此数据目录中的文件由Elasticsearch和Lucene写入。&lt;br>
两者的职责都非常明确：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Lucene负责写和维护Lucene索引文件，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而Elasticsearch在Lucene之上写与功能相关的元数据，例如字段映射，索引设置和其他集群元数据。最终用户和支持功能&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在低级Lucene中不存在，由Elasticsearch提供。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>在深入研究并最终找到Lucene索引文件之前，让我们看看Elasticsearch编写的外部级别数据。&lt;/p>
&lt;h3 id="3节点数据">3、节点数据&lt;/h3>
&lt;p>只需从空数据目录启动Elasticsearch即可生成以下目录树：&lt;/p>
&lt;p>&lt;img src="https://img0.tuicool.com/AbYBVra.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>node.lock文件用于确保一次只能从一个数据目录读取/写入一个Elasticsearch相关安装信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>有趣的是global-0.st文件。global-前缀表示这是一个全局状态文件，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而.st扩展名表示这是一个包含元数据的状态文件。您可能已经猜到，此二进制文件包含有关您的集群的全局元数据，前缀后的数字表示集群元数据版本，遵循跟随您的集群严格增加的版本控制方案。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>注意：虽然在紧急情况下使用十六进制编辑器在技术上可以编辑这些文件，但强烈建议不要这样做，因为它很快就会导致数据丢失。&lt;/p>
&lt;h3 id="4索引数据">4、索引数据&lt;/h3>
&lt;p>让我们创建一个分片索引并查看Elasticsearch更改的文件。&lt;/p>
&lt;p>&lt;img src="https://img2.tuicool.com/J3uuInA.png" alt="">&lt;/p>
&lt;p>我们看到已经创建了与索引名称对应的新目录。此目录有两个子文件夹：_state和0.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>前者&lt;em>state包含所谓的索引状态文件（indices / {index-name} /&lt;/em> state / state- {version} .st），&lt;br>
其中包含有关索引的元数据，例如它的创建时间戳。它还包含唯一标识符以及索引的设置和映射。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>后者0包含与索引的第一个（也是唯一的）分片相关的数据（分片0）。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>接下来，我们将仔细研究一下。&lt;/p>
&lt;h3 id="5分片数据">5、分片数据&lt;/h3>
&lt;p>分片数据目录包含分片的状态文件，其中包括版本控制以及有关分片是主分片还是副本的信息。&lt;/p>
&lt;p>&lt;img src="https://img2.tuicool.com/JVvmymQ.png" alt="">&lt;/p>
&lt;p>在早期的Elasticsearch版本中，还在分片数据目录中找到了单独的{shard_id} / index / _checksums-文件（和.cks-files）。在当前版本中，这些校验和现在可以在Lucene文件的页脚中找到，因为Lucene已经为其所有索引文件添加了端到端校验和。&lt;/p>
&lt;p>{shard_id} / index目录包含Lucene拥有的文件。Elasticsearch通常不直接写入此文件夹（除了早期版本中的旧校验和实现）。这些目录中的文件构成了任何Elasticsearch数据目录的大小。&lt;/p>
&lt;p>在我们进入Lucene的世界之前，我们将看一下Elasticsearch 事务日志，这在每个分片translog目录中的前缀translog-中存在。Translog日志对于Elasticsearch的功能和性能非常重要，因此我们将在下一节中更详细地解释它的用法。&lt;/p>
&lt;h3 id="6每个分片的事务日志transaction-log">6、每个分片的事务日志（Transaction Log）&lt;/h3>
&lt;p>Elasticsearch事务日志确保可以安全地将数据索引到Elasticsearch，而无需为每个文档执行低级Lucene提交。提交Lucene索引会在Lucene级别创建一个新的segment，即执行fsync（），会导致大量磁盘I / O影响性能。&lt;/p>
&lt;p>为了接受索引文档并使其可搜索而不需要完整的Lucene提交，Elasticsearch将其添加到Lucene IndexWriter并将其附加到事务日志中。在每个refresh_interval之后，它将在Lucene索引上调用reopen（），这将使数据可以在不需要提交的情况下进行搜索。这是Lucene Near Real Time API的一部分。当IndexWriter最终由于自动刷新事务日志或由于显式刷新操作而提交时，先前的事务日志将被丢弃并且新的事务日志将取代它。&lt;/p>
&lt;p>如果需要恢复，将首先恢复在Lucene中写入磁盘的segments，然后重放事务日志，以防止丢失尚未完全提交到磁盘的操作。&lt;/p>
&lt;h3 id="7lucene索引文件">7、Lucene索引文件&lt;/h3>
&lt;p>Lucene在记录Lucene索引目录中的文件方面做得很好，为了方便起见，这里重现了这些文件（Lucene中的链接文档也详细介绍了这些文件从Lucene 2.1返回后所经历的变化，所以检查一下出来）：&lt;/p>
&lt;p>&lt;img src="https://img1.tuicool.com/jymmIjJ.jpg" alt="">&lt;/p>
&lt;p>通常，您还会在Lucene索引目录中看到一个&lt;code>segments.gen&lt;/code>文件，该文件是一个帮助文件，其中包含有关当前/最新segments_N文件的信息，并用于可能无法通过目录列表返回足够信息的文件系统，以确定最新一代段文件。在较旧的Lucene版本中，您还可以找到带有.del后缀的文件。它们与Live Documents（.liv）文件的用途相同- 换句话说，这些是删除列表。&lt;/p>
&lt;h3 id="8修复有问题的碎片">8、修复有问题的碎片&lt;/h3>
&lt;p>由于Elasticsearch分片包含Lucene索引，我们可以使用Lucene的强大的CheckIndex工具（http://t.cn/Rs0gKjCl），这使我们能够扫描和修复有问题的段，通常只需要很少的数据丢失。我们通常会建议Elasticsearch用户简单地重新索引数据（re-index），但如果由于某种原因这是不可能的并且数据非常重要，那么这是一条可以采取的路线，即使它需要相当多的手工工作和时间， 取决于碎片的数量和它们的大小。&lt;/p>
&lt;p>Lucene CheckIndex工具包含在默认的Elasticsearch发行版中，无需额外下载。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback"># change this to reflect your shard path, the format is
# {path.data}/{cluster_name}/nodes/{node_id}/indices/{index_name}/{shard_id}/index/
$ export SHARD_PATH=data /elasticsearch/nodes/ 0 /indices/foo/ 0 /index/ 5
$ java -cp lib/elasticsearch-*. jar: lib/* :lib/sigar/* - ea: org.apache.lucene... org. apache.lucene.index.CheckIndex $SHARD_PATH
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果CheckIndex检测到问题并且其建议修复它看起来很合理，您可以通过添加-fix命令行参数吿诉CheckIndex应用修复程序。&lt;/p>
&lt;h3 id="9存储快照">9、存储快照&lt;/h3>
&lt;p>您可能想知道所有这些文件如何转换为快照存储库使用的存储。不用再想了：拿这个集群，将它作为我的快照快照到基于文件系统的网关，并检查存储库中的文件，我们会找到这些文件（为简洁起见省略了一些文件）：&lt;/p>
&lt;p>&lt;img src="https://img2.tuicool.com/ye6NnaQ.png" alt="">&lt;/p>
&lt;p>在根目录下，我们有一个索引文件，其中包含有关此存储库中所有快照的信息，每个快照都有一个关联的快照和元数据文件。 &lt;/p>
&lt;p>根目录下的快照文件包含有关快照状态，快照包含的索引等信息。根目录下的元数据文件包含快照时的群集元数据。&lt;/p>
&lt;p>当设置compress：true时，使用LZF压缩元数据和快照文件，LZF专注于压缩和解压缩速度，这使其非常适合Elasticsearch。&lt;/p>
&lt;p>数据存储有标题：ZV + 1字节，指示数据是否被压缩。在标题之后，格式上将存在一个或多个压缩的64K块：2字节块长度+2字节未压缩大小+压缩数据。使用此信息，您可以使用任何兼容LibLZF的解压缩程序。&lt;/p>
&lt;p>在索引级别，还有另一个文件indices / {index_name} / snapshot- {snapshot_name}，其中包含索引元数据，例如快照时索引的设置和映射。&lt;/p>
&lt;p>在分片级别，您将找到两种文件：重命名的Lucene索引文件和分片快照文件：indices / {index_name} / {shard_id} / snapshot- {snapshot_name}。此文件包含有关快照中使用的分片目录中的哪些文件的信息，以及从快照中的逻辑文件名到具体文件名的映射，这些文件名在还原时应存储为磁盘。它还包含可用于检测和防止数据损坏的所有相关文件的校验和，Lucene版本控制和大小信息。.&lt;/p>
&lt;p>您可能想知道为什么这些文件已被重命名而不是仅保留其原始文件名，这可能更容易直接在磁盘上使用。&lt;/p>
&lt;p>原因很简单：可以在再次快照之前对索引进行快照，删除并重新创建它。在这种情况下，几个文件最终会有相同的名称，但内容不同。&lt;/p>
&lt;h3 id="10小结">10、小结&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>在本文中，我们查看了各种级别的Elasticsearch写入数据目录的文件：节点，索引和分片级别。&lt;br>
我们已经看到了Lucene索引存储在磁盘上的位置，并简要描述了如何使用Lucene CheckIndex工具来验证和修复有问题的碎片。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>希望您不需要对Elasticsearch数据目录的内容执行任何操作，但是了解您最喜欢的基于搜索的数据库将哪种数据写入文件系统总是有帮助的！&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不需要完整记住每个文件的确切含义，关键的时候知道去哪里更快的查找最重要。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="11补充认知">11、补充认知&lt;/h3>
&lt;p>一份数据写入es会产生多份数据用于不同查询方式，会比原数据占用更多磁盘空间。而索引setting里&amp;quot;codec&amp;quot;: &amp;ldquo;best_compression&amp;quot;是针对_source进行压缩的，压缩算法是deflate压缩比为6。&lt;/p>
&lt;p>对照上面的lucene表进行如下的关联。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>原文_source 的文件: .fdt .fdm .fdx;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>倒排索引 的文件: .tim .tip .doc;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>聚合排序 的列存文件: .dvd .dvm;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>全文检索文件: . pos .pay .nvd .nvm等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>加载到内存 中的文件有: .fdx .tip .dvm，&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>其中.tip占用内存最大，而.fdt . tim .dvd文件占用磁盘最大，例如&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">11 .5M _ ap9_1w3 .liv
25 .0G _ ap9 .fdt
31 .9M _ ap9 .fdx
444 K _ ap9 .fnm
53 .1G _ ap9_Lucene50_0 .doc
64 .2G _ ap9_Lucene50_0 .tim
781 M _ ap9_Lucene50_0 .tip
87 .7G _ ap9_Lucene54_0 .dvd
920 K _ ap9_Lucene54_0 .dvm
104 .0K _ ap9 .si
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>另外segment较小时文件内容是保存在.cfs文件中，.cfe文件保存Lucene各文件在.cfs文件的位置信息，这是为了减少Lucene打开的文件句柄数。&lt;/p>
&lt;p>es节点上shard过多了会导致内存不够，可以对静态的索引进行&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">POST {indexName}/_forcemerge?max_num_segments=1
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item></channel></rss>