<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Justice的小站</title><link>https://justice.bj.cn/</link><description>Recent content on Justice的小站</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 10 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://justice.bj.cn/index.xml" rel="self" type="application/rss+xml"/><item><title>Justice's Blog</title><link>https://justice.bj.cn/homepage/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/about/</guid><description>&lt;h2 id="self-introduction">Self Introduction&lt;/h2>
&lt;p>Cras ex dui, tristique a libero eget, consectetur semper ligula. Nunc augue arcu, malesuada a nisi et, molestie finibus metus. Sed lacus odio, ultricies a nisl vitae, sollicitudin tempor ipsum. Vivamus quis feugiat arcu. Sed mi nunc, efficitur quis tellus vitae, posuere mattis metus. Phasellus in mattis dui. Nullam blandit, augue non ullamcorper dapibus, lacus dui molestie massa, in iaculis purus lectus eu lectus. Duis hendrerit lacinia tellus, sit amet feugiat dolor placerat id. Aenean ac velit massa. Vivamus feugiat dui at magna viverra, ut dictum nunc rutrum. Duis eget sapien finibus, lobortis orci id, vestibulum tellus. Maecenas lobortis urna libero, quis fermentum lectus lobortis nec. Nullam laoreet volutpat libero, ac mattis magna ullamcorper quis. Duis eget ipsum eu nisi mattis cursus et vitae turpis.&lt;/p>
&lt;p>Aliquam pretium diam eget leo feugiat finibus. Donec malesuada commodo ipsum. Aenean a massa in lacus venenatis vestibulum. Duis vel sem quis elit iaculis consectetur et quis dolor. Morbi eu ipsum hendrerit, malesuada ante sed, dapibus est. Suspendisse feugiat nulla ut gravida convallis. Phasellus id massa posuere, rhoncus justo ut, porttitor dolor. Nulla ultrices malesuada egestas. Nunc fermentum tincidunt sem ac vulputate. Donec mollis sollicitudin justo eget varius. Donec ornare velit et felis blandit, id molestie sapien lobortis. Morbi eget tristique justo. Mauris posuere, nibh eu laoreet ultricies, ligula erat iaculis sapien, vel dapibus lacus libero ut diam. Etiam viverra ante felis, et scelerisque nunc pellentesque vitae. Praesent feugiat dictum molestie.&lt;/p>
&lt;h2 id="details">Details&lt;/h2>
&lt;p>Nunc pellentesque vitae:&lt;/p>
&lt;ul>
&lt;li>Morbi accumsan nibh efficitur diam molestie, non dignissim diam facilisis.&lt;/li>
&lt;li>Donec dignissim leo in mollis faucibus.&lt;/li>
&lt;li>Donec blandit lacus a pellentesque fermentum.&lt;/li>
&lt;/ul>
&lt;p>Donec mollis sollicitudin:&lt;/p>
&lt;ul>
&lt;li>Nunc dictum purus ornare purus consectetur, eu pellentesque massa ullamcorper.&lt;/li>
&lt;li>Aliquam eu leo vitae justo aliquam tincidunt.&lt;/li>
&lt;li>Fusce non massa id augue interdum feugiat sed et nulla.&lt;/li>
&lt;li>Vivamus molestie augue in tristique laoreet.&lt;/li>
&lt;/ul></description></item><item><title>Pages</title><link>https://justice.bj.cn/homepage/pages/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/pages/</guid><description/></item><item><title>Experiences</title><link>https://justice.bj.cn/homepage/experiences/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/experiences/</guid><description/></item><item><title>Vintage</title><link>https://justice.bj.cn/homepage/vintage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/vintage/</guid><description/></item><item><title>Blank</title><link>https://justice.bj.cn/homepage/blank/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/homepage/blank/</guid><description>
&lt;div style="text-align:center">
&lt;p>Write anything you like here!&lt;/p>
&lt;/div></description></item><item><title/><link>https://justice.bj.cn/post/30.architech/deltalake/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/deltalake/</guid><description>&lt;h2 id="全面介绍数砖开发-delta-lake-的第一篇论文">全面介绍数砖开发 Delta Lake 的第一篇论文&lt;/h2>
&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>云对象存储如Amazon S3，作为目前最大且最节约成本的存储系统，用于实现数据仓库和数据湖的存储非常具有吸引力。但由于其实现的本质是键值存储，保证ACID事务性和高性能具有很大的挑战：元数据操作，比如list对象是很昂贵的操作；一致性保证也受限。&lt;/p>
&lt;p>在本论文中，我们向大家介绍Delta Lake，一个由Databricks开源的基于云对象存储的ACID表存储层技术。Delta Lake通过使用压缩至Apache Parquent格式的事务性日志来提供ACID，Time Travel以及海量数据集的高性能元数据操作（比如快速搜索查询相关的上亿个表分区）。同时Delta Lake也提供一些高阶的特性，比如自动数据布局优化，upsert，缓存以及审计日志等。Delta Lake表可以通过Apache Spark， Hive， Presto， Redshift等系统访问。Delta Lake目前已被上千个Databriks用户使用，每天处理exabytes级数据，最大的应用实例管理EB级数据集以及上亿对象。&lt;/p>
&lt;h3 id="1引言">1.引言&lt;/h3>
&lt;p>云对象存储如Amazon S3以及Azure Blob存储已成为最大且最广泛使用的存储系统，为上百万用户存储EB级数据。除了云服务传统的优点，如按需付费，规模效益，专业的管理等，云对象存储特别具有吸引力的原因是允许用户对存储和计算资源进行分离；举例来说，用户可以存储PB数据，但是仅运行一个集群执行几个小时的查询。&lt;/p>
&lt;p>因此，目前许多组织使用云存储来管理数据仓库以及数据湖上的大型结构化数据。主流的开源大数据系统，包括Apache Spark， Hive以及Presto支持Apache Parquet，ORC格式云对象存储的读写。商业服务包括AWS Athena， Google BigQuery和Redshift Spectrum 查询也支持以上这些系统及这些文件格式。&lt;/p>
&lt;p>不幸的是，尽管许多系统支持云对象的读写，实现高性能及可变的表存储非常有挑战，同时也使得在其上构建数仓很困难。与分布式文件系统如HDFS， 或者DBMS的定制存储引擎不同，大多数云存储对象都仅仅只是键值存储，并没有跨键的一致性保证。它们的性能特点也与分布式文件系统非常不同因此需要获得特殊的关注。&lt;/p>
&lt;p>在云存储对象中存储关系型数据最常见的方式是使用列存储格式，比如Parquet和ORC，每张表都被存储为一系列对象（parquet或者ORC文件）的集合，通过某些列做分区。这种方式在对象文件的数量适中时，扫描文件的性能尚可接受。但对于更复杂的扫描工作，正确性以及性能都将受到挑战。首先，多对象的更新并不是原子的，查询之间没有隔离：举例来说，如果一个查询需要更新表中的多个对象（比如从表的所有parquet文件中删除某个用户的相关记录），由于是逐个object更新，因此读客户端将会看到部分更新。另外，写回滚也很困难：如果一个更新失败，那么表将处于被污染的状态。第二，对于有上百万对象的大表，元数据操作非常昂贵。比如，parquet文件中footer包含了min/max等统计信息在查询时用来帮助跳过读文件。在HDFS上读footer信息只需要几毫秒，云对象存储的延迟非常高使得跳过读操作甚至比实际的查询花费时间还要长。&lt;/p>
&lt;p>从我们与云客户工作的经验来看，这些一致性以及性能方面的问题对企业的数据团队产生了很大的挑战。大多数的企业数据是持续更新的，所以需要原子写的解决方案；多数涉及到用户信息的数据需要表范围的更新以满足GDPR这样的合规要求。即使是内部的数据也需要更新操作来修正错误数据以及集成延迟到达的记录。有趣的是，在Databricks提供云服务最初的几年，我们收到的客户服务支持升级中，约有一半都是由于云存储策略导致的数据损毁，一致性以及性能等方面的问题。(比如，取消更新任务失败造成的影响，或者改进读取上万个对象的查询性能)。&lt;/p>
&lt;p>为了解决这些挑战，我们设计了Delta Lake，基于云存储对象的ACID表存储层。Delta Lake从2017年开始服务于客户，并于2019年开源。Delta Lake的核心概念很简单：我们使用存储在云对象中的预写日志，以ACID的方式维护了哪些对象属于Delta table这样的信息。对象本身写在parquet文件中，使已经能够处理Parquet格式的引擎可以方便地开发相应的connectors。这样的设计可以让客户端以串行的方式一次更新多个对象，替换一些列对象的子集，同时保持与读写parquet文件本身相同的高并发读写性能。日志包含了为每一个数据文件维护的元数据，如min/max 统计信息。相比“对象存储中的文件”这样的方式，元数据搜索相关数据文件速度有了数量级的提升。 最关键的是，我们设计Delta Lake使所有元数据都在底层对象存储中，并且事务是通过针对对象存储的乐观并发协议实现的（具体细节因云厂商而异）。这意味着不需要单独的服务来维护Delta table的状态；用户只需要在运行查询时启动服务器，享受存储计算扩展分离带来的好处。&lt;/p>
&lt;p>基于这样的事务性设计，我们能够加入在传统云数据湖上无法提供的解决用户痛点的特性，包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Time travel：允许用户查询具体时间点的数据快照或者回滚错误的数据更新。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Upsert，delete以及merge操作：高效重写相关对象实现对存储数据的更新以及合规工作流（比如GDPR）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>高效的流I/O：流作业以低延迟将小对象写入表中，然后以事务形式将它们合并到大对象中来提高查询性能。支持快速“tail”读取表中新加入数据，因此作业可以将Delta表作为一个消息队列。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>缓存：由于Delta表中的对象以及日志是不可变的，集群节点可以安全地将他们缓存在本地存储中。我们在Databricks云服务中利用这个特性为Delta表实现透明的SSD缓存。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据布局优化：我们的云服务包括一个特性，能够在不影响查询的情况下，自动优化表中对象的大小，以及数据记录的聚类（clustering）（将记录存储成Zorder实现多维度的本地化）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Schema演进：当表的schema变化时，允许在不重写parquet文件的情况下读取旧的parquet文件。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志审计：基于事务日志的审计功能。&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>这些特性改进了数据在云对象存储上的可管理性和性能，并且结合了数仓和数据湖的关键特性创造了“湖仓”的典范：直接在廉价的对象存储上使用标准的DBMS管理功能。事实上，我们发现很多Databricks的客户希望使用Delta Lake简化他们整体的数据架构，替换之前分离的数据湖，数仓，以及流存储系统，用Delta表来为所有用例提供适用的功能。表格1展示了一个例子，数据管道包括对象存储，消息队列以及为两个不同商业智能服务的数仓（每一个使用独立的计算资源），替换为只包含云存储对象上的Delta表，使用Delta的流I/O以及性能特性来执行ETL和BI。这种新的管道只用到了廉价的对象存储并产生了更少数量的数据备份，在存储和运维方面降低了成本。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77csdibuLB2ez2OshfIKyibpR8tzHh2OnfENrEKw7MdGzo5yibpQ3eXnsv9w/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图1: 使用3个存储系统实现的数据pipeline(1个消息队列, 1个对象存储 和 1个数据仓库), 或者使用 Delta Lake去同时实现流和表存储. Delta Lake的实现使得不需要维护多份copy，只需要使用最便宜的对象存储.&lt;/p>
&lt;p>Delta Lake目前被大多数Databricks的大客户采用，每天处理exabyte数据（约占我们整体工作需求的一半），其他云厂商如Google Cloud，Alibaba，Tencent，Fivetran，Informatica，Qlik，Talend以及其他产品也支持Delta Lake。在Databricks的客户中，Delta Lake的使用用例非常多样化，从传统的ETL和数仓的工作流，到生物信息学，实时网络安全分析（每天处理数百TB流事件数据），GDPR合规性以及用于机器学习的数据管理（管理数以百万计的图像作为Delta表中的记录而不是S3对象，以获取ACID并提高性能）。我们将在Section 5具体讨论这些使用用例。&lt;/p>
&lt;p>有趣的是，Delta Lake将Databricks的云存储相关支持问题比例从一半降为接近于零。Detla Lake为大多数客户改善了负载性能，在某些极端用例下，使用Delta Lake的数据布局以及快速访问统计信息对高维数据集（比如网络安全和生物信息学等场景）查询，甚至可以获得100倍的速度提升。开源的Delta Lake项目包含了Apache Spark（流批）， Hive， Presto，AWS Athena，Redshift以及Snowflake的连接器，能够运行在多种云对象存储或者HDFS之上。在本文中，我们将展示Delta Lake的设计初衷，设计理念，用户使用案例以及推动设计的性能测试。&lt;/p>
&lt;h3 id="2动机云对象存储的特点及挑战">2.动机：云对象存储的特点及挑战&lt;/h3>
&lt;p>本节，我们将对云对象存储的API及性能特点进行描述，以阐述在云对象存储之上为什么高效的表存储非常具有挑战，并介绍一些现有的管理表数据集的方法。&lt;/p>
&lt;p>&lt;strong>2.1 对象存储API&lt;/strong>&lt;/p>
&lt;p>云对象存储，比如Amazon S3，Azure Blob存储，Google云存储，以及OpenStack Swift，都提供了简单但容易扩展的键值存储接口。这些系统允许用户创建桶，每个桶存储多个对象，每个对象都是一个二进制blob，大小可到几TB（比如，在S3上对象最大为5TB），每个对象都由一个字符串作为key来标识。通常的方式是将文件系统的路径作为云对象存储的key。但云对象存储不能像文件系统那样，提供廉价的对“文件”或者对象的重命名操作。云对象存储提供元数据API，比如S3的List操作，根据给定的起始键，列出在某个桶中按键的字典序排序的所有对象。这使得通过发起一个LIST请求，给定一个代表目录前缀的key（比如 warehouse/table1/）有效地列出目录下的所有对象成为可能。但很可惜的是，元数据的API操作通常很昂贵，比如S3的LIST调用每次只能返回1000个key，每次调用花费几十至上百毫秒，所以当以顺序的方式列出一个有数百万对象的数据集可能需要好几分钟。&lt;/p>
&lt;p>读取对象时，云对象存储支持字节范围的请求，读取某个大对象的某字节范围（比如，从10000字节到20000字节）通常是高效的。这样就可以利用对常用值进行聚类的存储格式。&lt;/p>
&lt;p>更新对象通常需要重写整个对象。这些更新需要是原子的，以使读对整个新版本对象或者老版本对象可见。有些系统也支持对象的追加写。&lt;/p>
&lt;p>一些云厂商也在blob存储上实现了分布式文件系统接口，比如Azure的ADLS Gen2与Hadoop的HDFS具有相似的语义（比如目录的原子rename）。然而，Delta Lake解决的许多问题，如小文件问题，对多个目录的原子更新问题即使在分布式系统中也依然存在。事实上，有很多用户是在HDFS上使用Delta Lake。&lt;/p>
&lt;h2 id="heading">&lt;/h2>
&lt;p>&lt;strong>2.2 一致性属性&lt;/strong>&lt;/p>
&lt;p>如引言中所述，大多数云对象存储对单个key提供最终一致性保证，对跨key不提供一致性保证， 这对包含多对象的数据集管理提出了挑战。特别是当客户端提交了新的对象，其他客户端不能够保证在LIST或者读操作中立即看到这个对象。类似地，对现有对象的更新对其他客户端也不能够立即可见。更严重的是，有些对象存储系统，即使同一客户端执行了写操作也不能够立即读到新对象。&lt;/p>
&lt;p>精确的一致性模型因不同的云厂商而异，且相当复杂。举个具体的例子，Amazon S3提供了写后读的一致性，S3的客户端在PUT操作后可以通过GET返回这个对象的内容。一个例外是：如果客户端在PUT之前对不存在的Key先调用了GET，那么后续的GET操作可能由于S3的逆向缓存机制在一段时间内读不到这个对象。S3的LIST操作是最终一致的，这意味着在PUT之后LIST操作可能无法返回新的对象。其他的云对象存储提供更强的一致性保证，但在跨key的情况下仍然无法提供原子性操作。&lt;/p>
&lt;h2 id="heading-1">&lt;/h2>
&lt;p>&lt;strong>2.3 性能特点&lt;/strong>&lt;/p>
&lt;p>根据我们的经验，通过对象存储实现高吞吐量需要在大型顺序I / O和并行性之间取得平衡。&lt;/p>
&lt;p>对于读取，如前所述，最小粒度的操作是读取连续字节范围。每个读取操作通常会有至少5–10 ms的延迟，然后以大约50–100 MB / s的速度读取数据，因此，一个操作需要读取至少数百KB，才能达到顺序读取的峰值吞吐量的一半；读取数MB才能以接近峰值吞吐量。此外，在典型的VM配置上，应用程序需要并行运行多个读取以最大化吞吐量。例如，在AWS上最常用于分析的VM类型具有至少10 Gbps的网络带宽，因此它们需要并行运行8-10次读取才能充分利用此带宽。&lt;/p>
&lt;p>LIST操作也需要高并行度才能快速列出大数量的对象。比如S3的LIST操作每个请求只能返回1000个对象，耗时十到数百毫秒，因此客户端对大桶或者目录进行list时需要并行发出上百个LIST请求。在针对云上Apache Spark的优化运行时中，除了在Spark集群的driver节点中并行执行线程外，有时我们还会在worker节点上并行执行LIST操作以使它们更快地运行。在Delta Lake中，可用对象的元数据（包括它们的名称和数据统计信息）是存储在Delta日志中的，但我们还是会并行从该日志中读取数据。&lt;/p>
&lt;p>如2.1节所述，写操作通常要求必须重写整个对象（或者追加），这意味着如果一张表期望得到点更新，那么对象文件必须小一些，这与大量读对文件大小的要求是矛盾的。一种替代方案是使用日志结构的存储格式。&lt;/p>
&lt;p>表存储的含义。对于分析型工作负载，对象存储的性能特征引出的三点考虑：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>将需要经常访问的数据就近连续存储，这通常要求选择列存储格式。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使对象较大，但不能过大。大对象增加了更新数据的成本（例如，删除某个用户的所有数据），因为需要全部重写。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>避免使用LIST操作，并在可能的情况下按字典顺序的键范围发送请求。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="heading-2">&lt;/h2>
&lt;p>&lt;strong>2.4 现有的表存储方法&lt;/strong>&lt;/p>
&lt;p>基于对象存储的特征，目前主要有三种方法在对象存储之上管理表格数据集。我们将简述这些方法及其面临的挑战。&lt;/p>
&lt;p>**1.**目录文件 目前开源大数据技术栈以及云服务支持的最通用的方式是将表存储为对象集合，通常采用列存，比如Parquet。作为一种改进，可以基于一个或多个属性将记录“分区”到目录中。例如，对于具有日期字段的表，我们可以为每个日期创建一个单独的对象目录，例如，mytable / date = 2020-01-01 / obj1 以及mytable / date = 2020-01-01 / obj2用于记录从1月1日的数据，mytable / date = 2020-01-02 / obj1，1月2日的数据，依此类推，然后根据该字段将传入的数据拆分为多个对象。这样的分区减少了LIST操作以及仅访问几个分区的查询读操作的成本。&lt;/p>
&lt;p>这种方式具有吸引力是因为整个表仅由一些对象组成，可以通过许多工具访问 而无需运行任何其他数据存储或系统。这种方式起源于HDFS之上的Apache Hive，并且与Parquet，Hive和文件系统上的其他大数据软件配合使用。&lt;/p>
&lt;p>如引言中所述，这种方式的挑战是 “一堆文件”在云对象存储上有性能和一致性方面的问题。客户遇到的最常见挑战是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>跨多个对象没有原子性：任何需要写入或更新多个对象的事务都可能导致其他客户端只可见部分写入。此外，如果事务失败，数据将处于损坏状态。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最终一致性：即使事务成功，客户端也有可能只看到部分更新对象。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>性能差：查找与查询相关对象时的LIST操作很昂贵，即使它们被键划分到分区目录中。此外，访问存储在Parquet或ORC文件中的对象统计信息很昂贵，因为它需要对每个文件的统计信息进行额外的高延迟读取。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>没有管理功能：对象存储没有实现数据仓库中常用的标准工具，例如表版本控制或审核日志。&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>2.&lt;/strong> 自定义存储引擎. 为云构建的“闭源”存储引擎，例如Snowake数据仓库[23]，可以通过在单独的，高度一致的服务中管理元数据来绕过云对象存储的许多一致性挑战。这种服务保存着哪些对象构成了表这样的事实。在这些引擎中，可以将云对象存储视为笨拙的块设备，并且可以使用标准技术在云对象上实现有效的元数据存储，搜索，更新等。但是，此方法需要运行一个高可用性服务来管理元数据，这可能很昂贵，在使用外部计算引擎查询数据时可能会增加成本，而且有可能将用户锁定在某个特定厂商。&lt;/p>
&lt;p>这种方式的挑战：尽管这种从头开始的“闭源”设计是有好处的，但使用这种方法遇到的一些具体挑战是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>所有对表的I / O操作都需要连接元数据服务联系，增加资源成本并降低性能和可用性。例如，当用Spark访问Snow ﬂake数据集时，使用Snow ﬂake的Spark连接器通过Snow service的服务读取数据，与直接从云对象存储中读取数据相比降低了性能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>与重用现有开放格式（例如Parquet）的方法相比，开发现有计算引擎的连接器需要更多的工作量。根据我们的经验，数据团队希望在数据上使用多种计算引擎（例如Spark，TensorFlow，PyTorch等），因此使连接器易于实现非常重要。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>专有的元数据服务将用户与特定厂商绑定，相比之下，基于直接访问云存储的方式使用户总是能够使用各种技术访问他们的数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77ccXk5wDSjPN79YnOF0OkCyGKWsyJic6EJ9JbwBIj9AmcqMR7IG7edwLw/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图2:一个Delta table的对象layout案例&lt;/p>
&lt;p>Apache Hive ACID 使用Hive Metastore（一种事务性关系型数据库，例如MySQL）跟踪每张表相关的更新，更新以多个文件的形式存储在表的元数据信息中，一般为ORC格式。但是，这种方法受限于metastore的性能，根据我们的经验，它可能成为具有数百万个对象的表的瓶颈。&lt;/p>
&lt;p>**3.**在对象存储中保存元数据 Delta Lake的方法是将事务日志和元数据直接存储在云对象存储中，并在对象存储操作上使用一组协议来实现可序列化。。表中的数据以Parquet格式存储，只要实现一个最基本的连接器去发现要读取的对象集，就可以使用任何已经支持Parquet的软件访问数据。尽管我们认为Delta Lake是第一个使用该设计的系统（从2016年开始），但现在另外两个软件Apache Hudi 和Apache Iceberg 也支持这种方式。Delta Lake提供了一系列这些系统不支持的独特功能，例如Z序聚类，缓存和后台优化。我们将在第8节中详细讨论这些系统之间的异同。&lt;/p>
&lt;h1 id="heading-3">&lt;/h1>
&lt;ol start="3">
&lt;li>DELTA LAKE存储格式及访问协议&lt;/li>
&lt;/ol>
&lt;p>Delta Lake表是云对象存储或文件系统上的一个目录，其中包含具有表内容的数据对象和事务操作日志（包含检查点）。客户端使用我们根据云对象存储的特性量身定制的乐观并发控制协议来更新这些数据结构。在本节中，我们描述了Delta Lake的存储格式以及访问协议。我们还描述了Delta Lake的事务隔离级别，包括序列化（serializable）和快照（snapshot）隔离级别。&lt;/p>
&lt;h2 id="heading-4">&lt;/h2>
&lt;p>&lt;strong>3.1 存储格式&lt;/strong>&lt;/p>
&lt;p>图2展示了Delta Table的存储格式。每个表都存储在一个文件系统目录中（本例中是mytable）或者在对象存储中以相同目录作为key前缀的一些对象。&lt;/p>
&lt;h3 id="heading-5">&lt;/h3>
&lt;p>3.1.1 数据对象&lt;/p>
&lt;p>表的内容存储在Apache Parquet对象中，可以使用Hive的分区命名规范将其组织到目录中。&lt;/p>
&lt;p>例如在图2中，该表按日期列分区，因此对于每个日期，数据对象位于单独的目录中。我们选择Parquet作为我们的数据格式，因为Parquet面向列，提供多种压缩更新，支持半结构化数据的嵌套数据类型，并且已经在许多引擎中实现了高性能。基于现有的开放文件格式，还使Delta Lake可以持续利用Parquet库最新发布的更新并简化其他引擎的连接器开发（第4.8节）。其他开放文件格式，例如ORC [12]，也可以类似地工作，但是Parquet在Spark中拥有的支持最为成熟。&lt;/p>
&lt;p>每个数据对象在Delta中拥有唯一名字，通常是由writer生成的GUID。哪些对象是表的哪的版本是由事务日志决定的。&lt;/p>
&lt;h3 id="heading-6">&lt;/h3>
&lt;p>3.1.2 日志&lt;/p>
&lt;p>日志存储在表的_delta_log子目录中。它包含一系列以零填充的递增数字作为ID的JSON对象用于存储日志记录，并包含对某些特定日志对象的检查点，这些检查点将检查点之前的日志合并为Parquet格式。如3.2节中讨论的，一些简单的访问协议（取决于每个对象存储中可用的原子操作）用于创建新的日志条目或检查点，并使客户端在此基础上支持事务。&lt;/p>
&lt;p>每个日志记录对象（比如000003.json）包含了在前一个版本的表基础上进行的操作数组，以产生下一个版本。可用的操作包括：&lt;/p>
&lt;p>更改元数据 metaData操作更改表的当前元数据。表的第一个版本必须包含metaData操作。后续的metaData操作将完全覆盖表的当前元数据。元数据是一种数据结构，其中包含模式，分区列名称（如示例中的日期），数据文件的存储格式（通常为Parquet，但提供了可扩展性）以及其他配置选项，例如将表标记为仅追加。&lt;/p>
&lt;p>添加或删除文件   添加和删除操作用于通过添加或删除单个数据对象来修改表中的数据。因此，客户可以搜索日志以查找所有尚未删除的已添加对象，以确定组成表的对象集。&lt;/p>
&lt;p>数据对象的添加记录还可以包括数据统计信息，例如总记录条数以及每列的最小/最大值和空计数。当表中已存在的路径遇到添加操作时，最新版本的统计信息将替换任何先前版本的统计信息。这样可以在新版Delta Lake中“升级”旧表使其具有更多统计信息。&lt;/p>
&lt;p>删除操作包括表明删除发生时间的时间戳。在用户指定的保留时间阈值之后，数据对象会被进行惰性延迟物理删除。此延迟使并发的读取器可以继续对过期的数据快照执行操作。删除操作应作为墓碑保留在日志和所有日志检查点中，直到数据对象被删除为止。&lt;/p>
&lt;p>可以将添加或删除操作上的dataChange标志设置为false，以指示当与同一日志记录对象中的其他操作结合使用时，此添加或删除操作仅对现有数据重新排列或添加统计信息。例如，跟踪事务日志的流查询可以使用此标志来跳过不会影响其结果的操作，例如在早期数据文件中更改排序顺序。&lt;/p>
&lt;p>协议演进 协议操作用于增加Delta协议的版本号，在读取或写入给定表时需要此版本号。我们使用此操作向存储格式添加新功能，同时指出哪些客户端仍然兼容。&lt;/p>
&lt;p>添加来源信息 每个日志记录对象还可以在commitInfo操作中包括来源信息，例如，记录执行操作的用户。&lt;/p>
&lt;p>更新应用事务ID。Delta Lake为应用程序提供了一种将应用程序的数据包括在日志记录中的方法，这对于实现端到端事务性应用很有用。例如，写入Delta表的流处理系统需要知道先前已经提交了哪些写入，才能实现“精确一次性”的语义：如果流作业崩溃，则需要知道其哪些写入先前已写入表中，以便它可以从输入流中的正确偏移处开始重播后续写入。为了支持该用例，Delta Lake允许应用程序在其日志记录对象中写入带有appId和版本字段的自定义txn操作，这样该日志对象就可以用来跟踪应用程序特定的信息，例如本示例中输入流的对应偏移量。将此信息与相应的Delta添加和删除操作放置在相同的日志记录中（原子地插入到日志中），应用程序可以确保Delta Lake以原子方式添加新数据并存储其版本字段。每个应用程序可以简单地随机生成其appId来获得唯一的ID。我们在Spark Structured Streaming的Delta Lake connector中中使用此特性。&lt;/p>
&lt;h3 id="heading-7">&lt;/h3>
&lt;p>3.1.3日志检查点&lt;/p>
&lt;p>出于性能考虑，有必要定期将日志压缩到检查点中。检查点存储了直到特定日志记录ID的所有非冗余操作，以Parquet格式存储在表的日志中。某些冗余的操作是可以删除的。这些操作包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>对同一数据对象先执行添加操作，然后执行删除操作。可以删除添加项，因为数据对象不再是表的一部分。根据表的数据retention配置，应将删除操作保留为墓碑具体来说，客户端使用在删除操作中的时间戳来决定何时从存储中删除对象。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>同一对象的多个添加项可以被最后一个替换，因为新添加项只能添加统计信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>来自同一appId的多个txn操作可以被最新的替换，因为最新的txn操作包含其最新版本字段&lt;/p>
&lt;/li>
&lt;li>
&lt;p>changeMetadata以及协议操作可以进行合并操作以仅保留最新的元数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>检查点过程的最终结果是一个Parquet文件，其中包含仍在表中的每个对象的添加记录，需要保留直到retention period到期的对象删除记录，以及如txn，协议和changeMetadata等操作的少量记录。这种面向列的文件对于查询表的元数据以及基于数据统计信息查找哪些对象可能包含与选择性查询相关的数据来说是非常理想的存储格式。根据我们的经验，使用Delta Lake检查点查找对象集几乎总是比使用LIST操作和读取对象存储上的Parquet文件的Footer要快得多。&lt;/p>
&lt;p>任何客户端都可以尝试创建至指定日志记录ID的检查点，如果成功，则应将其写为对应ID的.parquet文件。例如，000003.parquet将代表直到并包括000003.json记录的检查点。默认情况下，我们的客户端每10个事务会写入一个检查点。&lt;/p>
&lt;p>最后，访问Delta Lake表的客户端需要高效地找到最后一个检查点（以及检查点之后的日志），而不需要列出_delta_log目录中的所有对象。检查点writer将会把最新的检查点ID写入_delta_log / _last_checkpoint文件中，前提是写入的检查点ID比该文件中当前的ID更大。请注意，由于云对象存储库最终的一致性问题，即使_last_checkpoint文件不是最新的也没有关系，因为客户端仍会在该文件中的ID之后搜索新的检查点。&lt;/p>
&lt;h2 id="heading-8">&lt;/h2>
&lt;p>&lt;strong>3.2 访问协议&lt;/strong>&lt;/p>
&lt;p>Delta Lake的访问协议是为了让用户能依托“对象存储”的接口实现“序列化”级别事务，尽管大部分公有云的“对象存储”只提供“最终一致性”保障。这个选择关键在于需要有一个“日志记录”对象，例如000003.json，此“日志对象”会作为客户端读取数据表的某个版本时使用的核心数据结构。读取了这个“日志对象”的内容，用户就能够从“对象存储”中定位到本张数据表中其他对象，完成后续对数据表中数据的查询，当然由于“对象存储”最终一致性，读取时可能数据对象还不可见，客户端可能需要等一个delay的小段时间。对于“写入”事务，用户需要一种机制去保障只有一个用户能创建下一个“日志记录”(比如，000003.json),这种机制可以理解为一种类似“乐观锁”的控制能力。&lt;/p>
&lt;h3 id="heading-9">&lt;/h3>
&lt;p>3.2.1 读表操作&lt;/p>
&lt;p>我们先描述Delta table的read-only读事务。读事务会安全的读到数据表的某个版本。Read-only的读事务有5个步骤：&lt;/p>
&lt;p>1.在table的log目录读取_last_checkpoint 对象，如果对象存在，读取最近一次的checkpoint ID&lt;/p>
&lt;p>2.在对象存储table的log目录中执行一次LIST操作，如果“最近一次checkpoint ID”存在，则以此ID做start key；如果它不存在，则找到最新的.parquet文件以及其后面的所有.json文件。这个操作提供了数据表从最近一次“快照”去恢复整张表所有状态所需要的所有文件清单。（需注意：因为对象存储是最终一致性语义，这个LIST操作返回的文件清单可能不连续，比如清单中有000004.json和000006.json但是没有000005.json . 这个问题Delta Lake有考虑到，客户端可以使用从表中读取的最大的ID，这里是000006.json，等待所有确实的对象可见后再完成计算）&lt;/p>
&lt;p>3.使用“快照”(如果存在)和后续的“日志记录”去重新组成数据表的状态（即，包含add records，没有相关remove records的数据对象）和这些数据对象的统计信息。Delta数据格式被涉及可以并行读取：比如，在使用Spark读取delta格式时，可以使用Spark job去并行读取.parquet的快照文件和.json的”日志记录“。&lt;/p>
&lt;p>4.使用统计信息去定位读事务的query相关的数据对象集合。&lt;/p>
&lt;p>5.可以在启动的spark cluster或其他计算集群中，并行的读取这些相关数据对象。 需注意，因为对象存储的最终一致性，一些worker节点可能读不到driver在制定执行计划后下发任务的相关数据文件，目前的设计是如果worker读不到，就等一段时间然后retry。&lt;/p>
&lt;p>我们注意到这个访问协议的每一步中都有相关的设计去规避对象存储的最终一致性。比如，客户端可能会读取到一个过期的_last_checkpoint文件，仍然可以用它的内容，通过LIST命令去定位新的“日志记录”文件清单，生产最新版本的数据表状态。这个_last_checkpoint文件主要是提供一个最新的快照ID，帮助减少LIST操作的开销。同样的，客户端能容忍在LIST最近对象清单时的不一致（比如，日志记录ID之间的gap），也能容忍在读取日志记录中的数据对象时，还不可见，通过等一等的方式去规避。&lt;/p>
&lt;h3 id="heading-10">&lt;/h3>
&lt;p>3.2.2 写事务&lt;/p>
&lt;p>一个写入数据的事务处理，一般会涉及最多5个步骤，具体有几步取决与事务中的具体操作：&lt;/p>
&lt;p>1.找到一个最近的日志记录ID，比如r，使用读事务协议的1-2步（比如，从最近的一次checkpoint ID开始往前找）.事务会读取表数据的第r个版本（按需），然后尝试去写一个r+1版本的日志记录文件。&lt;/p>
&lt;p>2.读取表数据的r版本数据，如果需要，使用读事务相同的步骤（比如，合并最新的checkpoint .parquet 和 较新的所有.json 日志记录文件，生成数据表的最新状态，然后读取数据表相关的数据对象清单）&lt;/p>
&lt;p>3.写入事务相关的数据对象到正确的数据表路径，使用GUID生成对象名。这一步可以并行化。最后这些数据对象会被最新的日志记录对象所引用。&lt;/p>
&lt;p>4.尝试去写本次写事务的日志记录到r+1版本的.json日志记录对象中，如果没有其他客户端在尝试写入这个对象（乐观锁）。这一步需要是原子的（atomic），我们稍后会探讨在不同的对象存储中如何实现原子性。如果这一步失败了，事务是要重试的；这取决于事务query的语义，在一定情况下客户端还是可以在重试中复用step3产生的数据对象们，然后把这些数据对象们写入到重试事务产生的新的.log日志记录对象中。&lt;/p>
&lt;p>5.此步可选。为r+1版本的日志记录对象，写一个新的.parquet 快照对象.(最佳实践中，默认每10条日志记录会做一次快照) 然后，在写事务完成后，更新_last_checkpoint文件内容，指向r+1的快照。&lt;/p>
&lt;p>需注意到第5步中，写一个新的.parquet 快照对象，更新_last_checkpoint文件内容，只会影响性能，如果在这一步客户端失败了并不会损害到数据完整性。比如，在生成快照对象时失败了，或者在更新_last_checkpoint文件内容时失败了，其他客户端仍然可以使用老一些的快照去读取数据表的内容。在第4步成功后，事务就算原子性的提交完成了。&lt;/p>
&lt;p>原子性的添加日志记录。在写事务协议中很明显的，步骤4，创建r+1版本的.json日志记录对象需要原子性：只能有一个客户端能成功的创建此日志记录。不幸的事，不是所有的大规模对象存储系统有put-if-absent类似的原子操作，我们针对不同的对象存储做了不同的实现去达到原子性的效果：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Google Cloud Storage 和 Azure Blob Store 都支持原子性的put-if-absent操作，所以直接使用即可&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在类似HDFS的分布式文件系统，我们使用原子的rename操作去rename临时文件到最终位置（如果最终位置文件已存在就fail）.Azure Data Lake Storage [18]也提供了文件系统API中的原子rename操作，所以我们直接使用这些系统的这些方法。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Amazon S3并没有提供原子性的“put if absent” 或者 “rename” 操作。在Databricks的部署服务中，我们使用了一个单独的轻量级协调服务去保证针对一个指定ID的日志记录，只能有一个客户端能够做添加操作。这个服务只有在写事务时才需要（读事务和非数据相关操作不涉及），所以它的load是相对较低的。在开源的Apache Spark的Delta Lake connector上，我们能保证同一个Spark driver程序（SparkContext object）的进程内部能利用in-memory的状态，在事务之间保证拿到不同的日志记录ID，即用户可以在一个单独的spark集群内针对一张Delta table做并发的操作。我们仍然提供了一个API接口，留给用户足够的自由度去实现一个自己日志存储实现类，从而达成事务操作的独立、强一致性。（LogStore）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="heading-11">&lt;/h3>
&lt;p>&lt;strong>3.3  关于隔离级别&lt;/strong>&lt;/p>
&lt;p>在遵循了Delta Lake的并发控制协议后，所有写事务都是线性化隔离级别（serializable）的，也使得事务的日志记录ID的线性增长。这遵循了写事务的提交协议，即每个日志记录ID只有一个写事务能使用。读事务是能达到snapshot isolation或者serializability的。在3.2.1节中描述的读协议只会读取数据表的一个快照，所以客户端使用这个协议就能达成snapshot isolation，但是客户端如果想达到线性化（serializable）的读取，可以发出一个“读after写”的事务，假装mock一次写事务然后再读，来达到线性化。在最佳实践中，Delta Lake 的connector实现了在内存中将每一张已访问过的表的最近“日志记录”ID做cache起来，这样客户端能“读自己所写”，即使客户端使用了snapshot isolation的读能力，也能在多次读操作时读到单调递增的数据表版本。更重要的是，Delta Lake目前只支持单表事务。基于“日志记录”的协议设计，在未来是可以被扩展到管理多张表上去的。&lt;/p>
&lt;h3 id="heading-12">&lt;/h3>
&lt;p>&lt;strong>3.4 事务频率&lt;/strong>&lt;/p>
&lt;p>Delta Lake的写事务频率受限于在写新的日志记录时，需要执行put-if-absent操作的延迟（描述与3.2.2章节）。在任何基于乐观锁的并发控制协议中，高频率的写事务都会导致事务commit失败。实际上，对象存储的写入延迟能达到上百毫秒ms，这严重限制了写事务的tps（transactions per second）。但是我们发现对于Delta Lake应用的并发来说这个并发率也够了，即使是一个相对高并行的streaming流式数据任务（打比方 Spark Streaming jobs），负责把数据导入到云存储，也可以把很多数据对象放在一个写事务当中批量提交。如果在未来，更高频的tps成为需求，我们相信去定制开发一个LogStore服务去负责事务日志管理（类似于Databricks在AWS针对S3存储做的commit服务），是能够提供更快的事务提交能力的（比如把事务日志先存储在低延迟的DBMS上，然后再异步写入对象存储）。当然，snapshot isolation隔离级别的读事务是没有竞争的，他们只需要去读对象存储中的对象即可，所以读事务的并发读是很高的，完全不受限的。&lt;/p>
&lt;h3 id="4delta中的高级功能">4.DELTA中的高级功能&lt;/h3>
&lt;p>Delta Lake的事务设计允许很多更宽范围的高阶数据管理功能，这和很多传统的分析型DBMS提供的便利能力类似。在本章，我们会探讨一些更广泛被使用的特性，以及客户的case或者说客户的痛点。&lt;/p>
&lt;h3 id="heading-13">&lt;/h3>
&lt;p>&lt;strong>4.1 时间穿梭和回滚&lt;/strong>&lt;/p>
&lt;p>数据工程师的pipeline经常会出逻辑错误，比如有时会把脏数据从外部系统导入到大数据系统中。在传统的数据湖设计方案中，很难去通过给单表做增加对象来实现undo更新语义。更多时候，一些工作比如机器学习训练是需要去针对老版本的数据做重新训练的（比如在同一份数据集上去对比 新/老的两种训练算法的效果）。在Delta Lake技术诞生前，这些问题都给Databricks的用户造成过很大的挑战，需要他们去设计很复杂的数据pipeline纠错辅助工具，或者将数据冗余多份。而有了Delta Lake后，基于它底层数据对象和事务日志的不可修改性，使得读取数据过去的历史快照变得很直接和容易，这是一个经典的MVCC实现。客户端只需要一个老的日志记录ID就能读到数据的历史版本。为了更好的帮用户实现Time travel，Delta Lake允许用户去做每张表级别的数据retention inverval配置，而且支持在sql中使用 timestamp 或者commit_id等隐藏字段等语义去帮助读取历史快照版本。客户端也能通过Delta Lake提供的API，在一次读/写操作后，获取到当下使用的commit ID日志记录。比如，我们在开源的MLflow项目中在每次ML训练任务中，使用这个API去自动记录数据表版本号，作为每次训练的元数据。用户会发现在修复数据pipeline的错误时，time travel功能会特别有用。比如，在需要修复一些用户数据时，有效的undo一个更新操作可以通过在数据表的快照上执行一条MERGE语句的SQL达成目标：&lt;/p>
&lt;p>MERGE INTO mytable target USING mytable TIMESTAMP AS OF  source ON source.userId = target.userId WHEN MATCHED THEN UPDATE SET *&lt;/p>
&lt;p>我们还开发了一个CLONE命令，它能够在数据表当下的一份快照上创建一个 copy-onwrite的新版本快照。&lt;/p>
&lt;h3 id="heading-14">&lt;/h3>
&lt;p>&lt;strong>4.2 有效的更新，删除和合并&lt;/strong>&lt;/p>
&lt;p>在企业中很多分析型的数据是需要持续更新的。比如根据GDPR[27]的数据隐私合规要求，企业需要能够有能力按要求删除一个用户相关的所有数据。即使不涉及个人隐私的数据，在某些场景下也有更新需求，比如上游数据pipeline的错误导致数据损坏就需要update去修复数据，再比如延迟到达的数据（late-arriving）也会导致需要对老数据进行更新等等。然后，一些聚合数据集也需要不断的更新聚合结果数据集（比如由数据分析时发出的针对一张表的sum query需要根据时间不断重新计算聚合值）。在传统的数据湖存储格式中，比如在S3上直接将Parquet文件放入目录下，很难在同时有并发读取的请求时去执行更新操作。即使要做，更新任务也要执行的非常小心，因为如果在更新时发生任务fail了，会留下一些“部分更新”的数据碎片。在Delta Lake中，所有这些操作都可以以事务进行运行（同时成功同时失败），在Delta log里记录（增加 or 删除）这些相关的被update的数据对象。Delta Lake支持标准的 SQL UPSERT，DELETE和MERGE语法。&lt;/p>
&lt;h3 id="heading-15">&lt;/h3>
&lt;p>&lt;strong>4.3 流式的数据导入和消费&lt;/strong>&lt;/p>
&lt;p>很多数据团队期望能使用streaming数据pipeline来实时的将数据进行ETL或者聚合操作，但基于传统的云存储是很难做到的。这些数据团队会使用一些独立的流式消息队列，比如Apache Kafka 或者 AWS Kinesis，在处理不好数据流时容易产生数据冗余，同时也给数据团队带来了很多额外的运维管理复杂度。我们在Delta Lake的设计中使用“日志记录”文件去记录事务追加的提交记录，用户可以把此日志记录当作一个message queue来看待，producer写入，consumer消费。有了递增的“日志记录”文件，用户就不需要额外再部署一套单独的消息队列服务了。这个能力由三个场景推演而来：&lt;/p>
&lt;p>写合并. 一个最简化的数据湖就是由一组对象文件组成，这使得写入数据很容易（写新的数据对象即可），但是在 写和读 的性能之间很难找到一个很好的平衡点。如果写入方想要快速的通过写小文件的方式达到快速写入数据的目的，在读取方最终会因为“过多的小文件读”和过多的“元数据”操作而变慢。相反，Delta Lake允许用户去跑一个后台Daemon来以事务的方式达成合并小文件的目标，且不影响读取方。如在3.1.2章描述，在做compact操作时将dataChange flag设置为false，如果已经读了小文件，可以让流streaming的消费者忽略这些compaction操作生成的数据文件。通过写小文件，使下游Streaming应用更快消费到新数据（延迟低，性能略差）。而其他普通的基于老版本数据的查询仍然可以很快。&lt;/p>
&lt;p>Exactly-Once 流式写入 . 写入方在日志记录对象中可以使用txn action type字段，用它来跟踪写入指定表的所有相关数据对象，从而达到“exactly-oncce”写入。总的来说，流式处理系统在写入（更新）外部存储时是需要一些机制去保证写入的幂等性的，需要幂等去避免duplicate数据写入，比如在发生写入错误后job重试时。在复写数据时，如果为每个记录设置一个唯一键（unique key），是能达到幂等效果的。或者说，将所有需要写入的记录放在一个“last version written” 事务记录里，一起成功或者一起失败。Delta Lake使用后一种模式让每个spark应用在每次事务中维护一个（appId，version）元组。比如在Spark Structured Streamingg 的Delta Lake connector中，我们用这个feature支持了所有流式计算语义的exactly-once写入（append, aggregation, upsert ）.&lt;/p>
&lt;p>有效的Log Tailing.  把Delta Lake表当作message queue的终极目标是要能让consumer能有效的找到“增量新写入”. 幸运的是，.json的日志存储格式是一系列按字典序自增ID的日志对象，这就让consumer的增量消费变的容易了：一个消费者可以在对象存储以last log ID为startkey，跑个简单的LIST操作，就能把增量新写入的数据对象找出来了。在日志记录内容中每一条的dataChange flag会允许流式streaming消费者决定是否跳过compact 或者重新整理过的数据，是否直接读新的数据文件（可能是新的小文件）.流式计算应用也能通过自己上一次成功处理完成的last record ID点位，完成stop或者restart操作 。&lt;/p>
&lt;p>将这三个feature合起来，我们发现很多用户真的可以用Delta Lake，一个搭建在对象存储上的数据格式，去实现消息队列的语义，从而完成“秒级延迟”的流式计算pipeline。而不是依赖于一个独立的消息队列服务，比如kafka。&lt;/p>
&lt;h3 id="heading-16">&lt;/h3>
&lt;p>&lt;strong>4.4 数据布局优化&lt;/strong>&lt;/p>
&lt;p>数据layout在分析类系统中对查询性能有很大的影响，特别是在很多分析query都有高度的挑剔复杂度时。因为Delta Lake支持以事务的方式去更新一张数据表，它就必须要能够支持在不影响并发的其他操作的情况下做layout的优化。比如，一个后台Daemon进程会把数据对象做compact，在这些数据对象内部重新排序，甚至去更新这些数据的统计指标、索引信息等等，且要在不影响其他客户端的前提下。我们基于事务这个优势，实现了一些数据layout优化的特性：&lt;/p>
&lt;p>OPTIMIZE Command. 用户可以针对一张表手动触发OPTIMIZE命令，这个命令可以在不影响进行中(on-going)事务的前提下进行小文件的合并，同时重新计算缺少的文件统计信息。默认情况下，这个操作的目标是把文件重新规整为每个文件1GB大小，这个值是我们的经验值，当然用户可以按自定义设置这个参数。&lt;/p>
&lt;p>Z-Ordering by Multiple Attributes.  很多数据集上跑的query都有很高的选择度，query会有很多条件。比如，一个网络安全的数据集，它的数据包含有网络中 (sourceIp, destIp, time) 这样一个三元组，在这三个维度（属性）都会有很多查询使用。如果使用Apache Hive的简单按照path/directory做分区的办法，只能在数据写入时按一部分维度（属性）做分区，但是要使用多个维度（属性）创建分区，分区数会暴涨，这在hive里是极力避免的。&lt;/p>
&lt;p>Delta Lake支持在数据表中按照一些给定的维度（属性）去重新整理记录，使用Z-order[35]技术，在指定的多个维度（属性）上都能达到相对较高的数据本地性。在指定的多维度空间上去计算Z-order曲线还是很容易计算出来的，这个技术的目标是在“经常会涉及到多个维度的查询场景”下，都能达到较好的性能，而不是偏向于某一个维度（在Section 6 的测试中有表现）。用户可以在每张表上设置自己需要的Z-order维度集合，然后跑一个OPTIMIZE命令，就可以达到把数据按Z-order整理好的目的了，用户还可以随时调整Z-order策略。Z-order技术使用了数据统计学技术，能让查询过滤更多的数据，减少读IO开销。在最佳实践中，Z-order技术的目标就是让所有的数据对象，在用户指定的几个维度下，都有一个相对小的值范围，在查询时能保证过滤掉更多的数据对象。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77cnnkjrJxqDSsO6gXVkgw1Uic0TfDwsiclHXqfdMulXZKb11PBeVx2g9BA/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图3:DESCRIBE HISTORY输出了在一张Delta标上每一次的update。&lt;/p>
&lt;p>自动优化. 在Databricks的云服务上，用户可以给一张表设置AUTO OPTIMIZE 属性，从而可以自动的去compact新写入的数据对象。&lt;/p>
&lt;p>总的来说，Delta Lake的设计也允许在数据表做update时，能够维护index和高计算消耗的统计信息。我们在这个点上开发了不少新feature。&lt;/p>
&lt;h3 id="heading-17">&lt;/h3>
&lt;p>&lt;strong>4.5 缓存&lt;/strong>&lt;/p>
&lt;p>很多云用户都会为不同的业务，跑不同的常驻的计算集群，有时候也会根据业务的负载动态伸缩集群规模。在这些集群中，使用本地磁盘将经常访问的数据做caching是一种加速query的很好的机会。比如，AWS i3机型为每个core提供一个237GB的NVME SSD，价格比同等的m5（general-purpose）实例贵个50%。在Databricks，我们在集群针对Delta Lake的数据搞了一层透明的cache处理，这个特性可以帮助访问数据&amp;amp;元数据是都提速。Caching是安全的，这是因为在Delta Lake中，data文件，log文件，checkpoint文件等等一旦写入，都是immutable不可修改的。我们在第6章会看到，使用了cache后，读性能显著的增长了。&lt;/p>
&lt;h3 id="heading-18">&lt;/h3>
&lt;p>&lt;strong>4.6 审计日志&lt;/strong>&lt;/p>
&lt;p>Delta Lake的事务日志也可以被用作审计日志，基于日志中的commitinfo记录。在Databricks，我们开发了一个“锁”机制，去防止用户在spark集群使用UDF去直接访问云对象存储，这保证了只有使用runtime引擎才能向日志记录写入commitinfo记录，从而保证了事务日志的不可变性，也就达到了可审计的目标。用户可以使用 DESCRIBE HISTORY命令去看Delta Lake表的历史版本，如Figure3图所示。在开源版本的Delta Lake中Commit information日志也是可见的。审计日志是企业级数据应用合规要求中，在数据安全要求里越来越重要的强制性要求。&lt;/p>
&lt;h3 id="heading-19">&lt;/h3>
&lt;p>&lt;strong>4.7 Schema 演变和增强&lt;/strong>&lt;/p>
&lt;p>数据源经过长时期的迭代后，通常都会有schema变更的需求，，但是这也带来了挑战，老的数据文件（old Parquet files）可能会有“过期的/错误的”schema。Delta Lake可以以事务的方式完成schema变更，如果需要甚至还可以按照最新的schema去更新底层的数据的对象（比如删除一个用户不再需要的字段）。把每次的schema变更记录保存在事务日志中并维护一个历史，可以不重写老的Paruqet数据文件（当然只能在add column 加列时）。同等重要的是，Delta的客户端要保证新写入的数据是能符合表的schema的。在有Delta Lake这种写入时check schema的机制之前，将Parquet文件写入一个directory经常会有把schema搞错的事情发生，有了这个简单的check机制就能很好的trace到问题，因为在发生schema错误时会抛出错误。&lt;/p>
&lt;h3 id="heading-20">&lt;/h3>
&lt;p>&lt;strong>4.8 Connectors to Query and ETL Engines&lt;/strong>&lt;/p>
&lt;p>Delta Lake在Spark SQL和Structured Streaming通过使用Apache Spark的data source API，提供了全能力的connector。更进一步，它目前和很多系统都提供了“只读”的集成：Apache Hive, Presto, AWS Athena, AWS Redshift, and Snowflake，用户使用这些系统都能去查询Delta table了，跑普通查询甚至用Delta table数据和其他数据源的数据做join也可以。最后，一些ETL和CDC（Change Data Capture）的工具包括Fivetran, Informatica, Qlik and Talend 都是可以写入Delta Lake的 [33, 26]。一些查询引擎的整合使用了特殊的机制，比如Hive里的symbolic links ，会生成叫symlink的manifest文件。一个symlink manifest文件本质上是一个 text file，它包含了“对象存储 or 文件系统”在对应path/directory下可见的文件列表。很多Hive兼容（Hive-compatible）的系统是能够识别这个manifest files的，通常文件叫“_symlink_format_manifest”，当去读一张表对应数据时，可以先去找目录下的这个文件，然后把文件内容里的所有paths作为本张表的数据对象清单。在Delta Lake的上下文中， manifest files的作用就是为读取方提供了一个表的静态快照（包含表的file lists）。要生成一张表的manifest files ，用户需要跑一个简单的SQL指令。然后就可以把数据作为外部表load到Presto, Athena, Redshift or Snowflake等等引擎了。在其他case里，比如Apache Hive，开源社区也有人为Hive设计了一个Delta Lake的connector。&lt;/p>
&lt;h3 id="5delta-lake-use-cases">5.DELTA LAKE USE CASES&lt;/h3>
&lt;p>Delta Lake目前被Databricks中几千个活跃用户所使用，每天使用它处理EB级的数据量，和开源社区里的其他组织一样。这些use cases跨越了很广阔的数据源和应用。Delta Lake的数据源包括：企业级的OLTP系统的Change Data Capture (CDC) logs, 应用logs, 时间序列data, 图数据, 为BI分析用的数据表格的聚合数据, 图片，machine learning（ML）的特征数据等等。在这些数据上跑的应用包括：SQL（最常见的应用类型）， BI（business intelligence） ， streaming（流计算），data science（数据科学），machine learning（机器学习） and graph analytics（图计算）。Delta Lake对大多数使用Parquet、ORC等存储格式的数据应用来说，是一个很好的补充。&lt;/p>
&lt;p>在这些use cases里，我们发现用户会使用Delta Lake来简化他们的企业级数据架构，使用云对象存储，在上面搭建“lakehouse”湖仓一体系统，同时达成数据湖和事务的能力。比如，想象一个从多数据源load数据的典型数据pipeline：从OLTP数据出来的CDC logs和设备产生的sensor data，将两个数据进行一些的ETL然后产生一些服务于数仓和数据科学家的衍生数据表（图1所示）。传统的实现需要集成很多组件，比如使用message queue（Apache Kafka）去承载实时计算的结果；使用一种数据湖存储作为长期存储；再使用一种数据仓库技术（比如Redshift）来为用户提供高速的Analytical分析类查询服务，数仓引擎可能会使用索引技术和告诉的本地磁盘（SSD）。在这些系统中都需要duplicate data，等于同一份数据多了多份拷贝，另外一个挑战是在这些系统中保证数据的一致性。而有了Delta Lake之后，上述的多种存储系统都可以被简单的“单一”云对象存储所取代即可，在上面利用好Delta Lake的ACID事务能力，streaming I/O能力 和 caching能力，这样就能得到同等的性能同时剔除数据架构上的复杂度。虽然Delta Lake不能代替上述系统的所有能力，也不能在所有场景都work的非常好（比如毫秒级的实时系统），但在大多数场景还是能满足需求的。在4.8章我们也介绍了，Delta目前和其他一些查询系统也已经有了connector集成，在后续章节我们会更详细的说一些use case。&lt;/p>
&lt;h3 id="heading-21">&lt;/h3>
&lt;p>&lt;strong>5.1 Data Engineering and ETL&lt;/strong>&lt;/p>
&lt;p>很多组织都在将ETL/ELT和数据仓库搬到云上，来减轻管理维护负担，另一边，更多的组织在将自己的业务数据（OLTP系统的交易数据）和其他数据源（web访问或IOT物联网系统）打通从而给下游的其他数据应用赋能，比如机器学习应用。这些应用都需要一个可靠且容易维护的数据工程化以及/ETL能力去处理这些数据。当这些组织将工作搬到云上后，他们都倾向于使用“云对象存储”作为数据的落地存储，这能带来存储开销的缩减，然后从这些原始数据经过计算加工，将加工后数据再导入到“更优的数仓系统”（比如拥有本地SSD存储）。Delta Lake的ACID事务能力，对UPSERT/MERGE的支持，以及time travel等特性是能够让这些公司直接基于对象存储就能“架设”数据仓库的，比如提供数据仓库常见的rollback，time travel，审计日志等能力。更多的好处是，使用Delta Lake后，避免了使用多种存储，避免了复杂数据链路的维护工作。最后，Delta Lake也同时支持SQL和Spark 编程API去写程序，让创建data pipeline更容易了。我们看到，在跨越不同行业比如（金融服务业、healthcare以及media行业）时，数据处理或者ML机器学习类的工作都在技术上都是差不多的，一旦这些公司的最基本的ETL pipeline和数据完成后，这些组织还可以进一步使用这些数据去充分挖掘价值（比如使用PySpark写一些数据科学分析）。可以在云上再开一个独立的计算集群即可，新集群也可以访问底层同一份数据，底层的基于Delta Lake的存储是共享的。还有一些组织将一部分的pipeline改为流式query（使用Spark Structured Streaming的Streaming SQL）。这些都可以通过新的云虚机（VM）来简单的跑起来，同时访问相同的底层数据。&lt;/p>
&lt;h3 id="heading-22">&lt;/h3>
&lt;p>&lt;strong>5.2 Data Warehousing and BI&lt;/strong>&lt;/p>
&lt;p>传统的数据仓库系统会使用有效的工具将ETL/ELT的功能结9合起来，来满足交互式的查询能力&lt;/p>
&lt;p>比如BI（business intelligence）。支持这些需求的核心技术能力就是使用高效的存储格式（列式存储格式），数据的访问优化比如clustering和indexing，更快的存储介质，和更可靠的查询引擎。Delta Lake能够依托云对象存储直接支撑所有的这些特性，比如列式存储、数据layout优化、min-max统计、SSD caching，所有这些都是依托了它基于事务的ACID设计。之后，为我们还发现很多Delta Lake的用户会基于他们的LakeHouse数据集去跑adhoc query和BI需求，有的直接跑SQL，也有的使用Tableau这样的BI软件。基于这些use case都是常见需求，DataBricks开发了一个新的向量化的专门为BI需求服务的执行引擎，就好像对Spark runtime的优化一样。像其他ETL的case一样，BI直接查询Delta Lake的好处是能给分析师提供更新鲜的新数据，因为数据不再需要被load到另外一个独立的数据仓库系统了。&lt;/p>
&lt;h3 id="heading-23">&lt;/h3>
&lt;p>&lt;strong>5.3 合规 &amp;amp; 重新生成数据&lt;/strong>&lt;/p>
&lt;p>传统数据湖存储格式设计初就是为了不可变数据的，但现在越来越多的国家对数据有了合规要求，比如欧盟的GDPR[27],结合业界的最佳实战来看，对企业而言，需要大家有有效的方法去delete或者correct个人用户的隐私数据。我们看到不少组织将云上的数据集转向使用Delta Lake，就是为了使用它的高效 UPSERT，MERGE 和 DELETE 能力。用户还可以使用审计日志（section4.6）功能去做数据治理。Delta Lake的time travel能力对于需要重新使用老数据的数据科学分析和机器学习场景也非常有用。我们把MLflow和Delta Lake做了整合，MLflow是一个开源的模型管理平台，也是Databricks主导的，它能够自动的记录哪个模型使用了哪个版本的数据集进行了什么训练，这样能够方便开发人员重新跑过去的训练。&lt;/p>
&lt;p>&lt;strong>5.4 Specialized Use Cases&lt;/strong>&lt;/p>
&lt;p>5.4.1 Computer System Event Data&lt;/p>
&lt;p>我们见过的使用Delta Lake的一个最大的单场景需求是“安全信息事件管理平台（SIEM）”，来自一家大型的科技公司。这家公司将一大堆的计算机系统事件记录了下来，包含：TCP和UPD的网络流，认证请求，SSH登陆日志，等等，把这些数据都导入到一张大的Delta Lake表中，数据量有PB级。很多的ETL、SQL、图分析作业 以及 机器学习任务都会使用这个数据源，按照一定的已知行为pattern去搜索一些入侵的证据（比如，怀疑一个用户的登陆事件，或者有人从一些服务器上导出了大量的数据）。这些任务中很多是流式计算任务，都是为了尽可能缩小发现问题的时间。更多的是，超过100个分析师会查询这张表数据，直接用这张Delta Lake table去调查怀疑的告警，或者去设计新的自动化监控任务。这个信息安全的case真的很有意思，因为它很容易就自动的收集了大规模的数据（每天上百TB数据），因为这些数据是需要保留很久的，它将被用来作为法庭上分析新发现的入侵方式（有时在事实发生几个月后才定义出来），因为这些数据需要按很多维度被查询。比如，如果一个分析师发现了某个服务器曾经被攻破过，她可能需要去查查在网络里从这个sourceIP地址出去的数据（看看哪些机器可能从这里被攻击了），以这台机器为destination 的IP地址（看看攻击是从哪些源头来到这台机器），需要按时间，按其他一些维度（比如，攻击者拿到的员工的access token）。为PB级的数据集维护重量级的索引结构会是一件很重的事情，所以这个组织使用了Delta Lake的ZORDER BY特性去重新组织Parquet数据对象，从而提供跨越多个维度的聚类。因为法律要求类的查询伴随的这些维度都会经常组合出现（比如，在百万级别数据中找寻1个IP address），Z-ordering 和Delta Lake本身的min/max统计合在一起，能显著的降低每个query要读取的数据对象个数。Delta Lake的 AUTO OPTIMIZE功能， time travel 和 ACID transactions也在保证数据准确性，在百级别工程师协同访问数据等方面，发挥了重要作用。&lt;/p>
&lt;h3 id="heading-24">&lt;/h3>
&lt;p>5.4.2 Bioinformatics&lt;/p>
&lt;p>生物信息是另一个我们发现Delta Lake被重度使用的领域，它被用来管理机器产生的数据。这里有很多数据源，包括DNA序列，RNA序列，电子医疗记录，还有医学设备的时间序列数据，这些数据让生物医药公司能够收集到关于病人和疾病更细节的信息。这些数据源一般会用来和公共数据集做join，比如和UI Biobank[44],他拥有序列信息和500,000个体的医疗记录。虽然传统的生物信息工具也使用过定制的数据格式，比如SAM，BAM ，VCF[34, 24]，很多组织现在都开始将数据使用数据湖存储格式比如Parquet。大数据基因组学项目[37] 先行使用了这个方法。Delta Lake更进一步的加强了生物信息的工作能力，通过帮助开启全多维分析查询（使用Z-ordering），ACID事务，和高效的UPSERT 和 MERGE。在一些case里，使用这些特性和直接使用Parquet相比快了100x倍。在2019年，Databricks和Regeneron 发布了Glow[28],一个开源的基因组学数据工具集，它使用Delta Lake作为存储。&lt;/p>
&lt;h3 id="heading-25">&lt;/h3>
&lt;p>5.4.3 Media Datasets for Machine Learning&lt;/p>
&lt;p>另一个我们看到很令人惊喜的应用是使用Delta Lake去管理多媒体数据集，比如从website上传的一批图片，用作后续的machine learning。虽然图片和其他媒体文件已经用高效的二进制格式编码好了，管理好这些百万级的对象，在对象存储中也是很有挑战的，因为每个对象只有区区几个kb大小。对象存储的LIST操作会跑上几分钟，很难并发的快速读到足够的对象，然后喂给基于GPU上跑的机器学习任务。我们看到很多组织也将这种媒体文件以二进制的记录存储在Delta table里，然后使用Delta做高速的推理查询，流式处理，和ACID事务。比如，头部的电子商务公司以及旅游公司就使用这种办法去管理用户上传的百万级别的图片。&lt;/p>
&lt;h3 id="6-性能实验">6. 性能实验&lt;/h3>
&lt;p>在这一章，我们通过一些性能实验来表现Delta Lake的特性。我们首先，(1) 分析有很大规模数量对象 or 分区的开源大数据系统的问题，带着问题去看Delta Lake使用中心化的checkpoint去做metadata和统计信息的技术设计。（2）再分析在一张大表中，当查询条件多样化时 Z-ordering的性能问题。&lt;/p>
&lt;p>最后我们还把Delta 的性能和 原生Parquet在TPC-DS数据集上做了对比，在写入场景并没有发现有明显的overhead增加。&lt;/p>
&lt;h3 id="heading-26">&lt;/h3>
&lt;p>&lt;strong>6.1 多对象or分区的影响&lt;/strong>&lt;/p>
&lt;p>Delta Lake的很多设计初心都是为了解决云对象存储的 listing和reading 对象的高延迟。这个延迟会让加载一张几千个数据文件的表 or 创建一个Hive风格的有几千个partition的表 变的很重。小文件常常会给HDFS造成问题，但是在性能这块HDFS还是要好过云对象存储的。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77ch4lzVzSRTDnia3twCwibvlLQSSrPia8bPZdkxdO1JgfLtE5P8W5p4cXzg/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图4: 在查询有大量分区时不同系统的性能表现。未使用Delta的系统查询1million分区时太慢了，结果就没有列出来。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77cv3eTgnE1ytiahkef1o0T2Td1WiaCn83nV516Keyqde50RAoocu1yY0MA/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图5:在拥有100个对象的表中，使用4个字段“全局排序”或“zorder”能过滤掉的Parquet文件比例&lt;/p>
&lt;p>去评估海量数据对象的影响，我们使用Databricks服务，在AWS创建了16-node AWS clusters of i3.2xlarge VMs (where each VM has 8 vCPUs, 61 GB RAM and 1.9 TB SSD storage)  ，还有托管的Apache Hive和Presto。然后我们创建一张33,000,000行的数据表，但是给他分配1000到1,000,000个分区，用这种方式去衡量大规模分区时metadata的overhead，然后在这些记录上跑一个简单的sum query。我们使用Databricks Runtime提供的Spark集群去跑，也用其他的引擎比如Hive 和 Presto ，另一边底层的存储格式同时对比Delta Lake和原生Parquet。&lt;/p>
&lt;p>如图4所示，使用Delta Lake的Databricks Runtime组合在性能上有显著的性能优势，即使在没有SSD做cache的情况下。Hive使用了近一个小时才能找到1张表里的10,000个分区，1万个分区是个挺正常的数量，在给一张hive表按时间（天）分区的同时只要再来一个其他分区键就很容易达到这个规模了。Presto在读100,000分区是用了1个小时还要多。而Databricks Runtime引擎用了差不多450秒完成了100,000分区的listing操作，这很大程度上是因为我们优化了基于对象存储的LIST请求，我们把它并行化了，用spark cluster分布式的执行。&lt;/p>
&lt;p>但是，在1个million分区这个量级，Delta Lake使用了108秒，如果使用了cache on SSD去把日志记录log做cache，可以把时间压缩到只要17秒。百万级别的hive分区看上去好像不现实，但在真实世界里，PB级别的表里真的有甚至上亿级别的数据对象的，在这种数据集上跑LIST操作是很重很重的。&lt;/p>
&lt;h3 id="heading-27">&lt;/h3>
&lt;p>&lt;strong>6.2 Impact of Z-Ordering&lt;/strong>&lt;/p>
&lt;p>要解析Z-Ordering，我们就要评估在访问一张表的数据时能跳过的数据百分比，我们针对这一指标，在使用Z-ordering 和 在数据表只根据一列做partition 或者 sort 来做对比。我们先根据Section5.4.1章节的use case里，基于信息安全数据集的灵感生成一份数据，有4个fields：sourceIP，sourcePort，destIP和destPort ，这几个维度能代表一个网络流量。我们选择32-bit的IP地址和16-bit的端口统一进行随机生成，然后我们把这张数据表存储为100个Parquet对象。然后，我们根据一些query去看能跳过多少对象，这些query的条件中包含一些维度（比如 SELECT SUM(col) WHERE sourceIP = &amp;ldquo;127.0.0.1&amp;rdquo;）。&lt;/p>
&lt;p>图5展示了结果，使用了(1)一个全局排序（特别的，按这个顺序 sourceIP, sourcePort, destIP and destPort ） 和 （2）使用这4个field做Z-ordering。在全局排序下，按照source IP搜索结果是能有效的跳过很多数据的，因为可以使用Parquet对象中source IP列的 min/max统计信息（大多数query只需要读这100个Paruqet对象中的1个），但按其他filed查询是则就没有效率了，因为每个文件都包含了很多记录，这些文件里关于其他列的min/max值范围太大了，甚至逼近整张表的min/max，达不到很好的过滤效果。相反，使用这4列做一个Z-ordering，不论按哪个field去查询，都能至少过滤掉43%的Parquet数据对象，平均能过滤率能达到54%（和简单的排序比平局是25%）。&lt;/p>
&lt;p>如果有些表的数据对象比1000个更多，那Z-order就能带来更大的整体提升了。比如，在一个500TB的网络流量数据集上要按多个维度查询，按上述办法做Z-ordering后，我们在查询时能过滤掉数据表里的93%的数据对象。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/0yBD9iarX0nvHAHdS1bjNgoKNYeovO77cmWs46PmXwRcF2v4OhQWg6CibbnW13Wwea4o86Ep323ibQc8cXVcxChOg/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>图6 ：使用不同查询引擎 +底层存储格式，在TPC-DS压力测试下的表现&lt;/p>
&lt;h3 id="heading-28">&lt;/h3>
&lt;p>&lt;strong>6.3 TPC-DS 性能测试&lt;/strong>&lt;/p>
&lt;p>要给Delta Lake在DBMS关系模型来一轮全面的性能基准测试，我们使用了TPC-DS数据集，在Databricks Runtime上来跑（分别以Delta Lake格式和原生Parquet格式存储），然后再交叉使用原生Spark和Presto来测试。每对组合都使用1个master和8个worker，跑在（ i3.2xlarge AWS VMs, which have 8 vCPUs ）机器上。我们使用TPC-DS在S3存储生成1TB的标准数据集，事实表按代理日key日期列做分区。图6展示了在不同配置下3种组合的平均耗时。我们看到Databricks Runtime和Delta Lake的组合性能是最好的。。（此处有benchmarket 的嫌疑）在这个实验中，Delta Lake在处理大规模分区的优势没有很明显的体现，这是因为整张表太小了，Delta Lake确实也在原生Parquet上做了一些加速，主要是针对benchmark中的大query做加速。Databricks Runtime的执行计划优化会比第三方的Spark服务做的更好一些（大多数基于Apache Spark2.4).&lt;/p>
&lt;h3 id="heading-29">&lt;/h3>
&lt;p>&lt;strong>6.4 写入性能&lt;/strong>&lt;/p>
&lt;p>我们同样使用Delta Lake和原生Parquet这两种格式，通过分别加载一个大的数据集，来对比Delta的统计信息是否显著的增加了写入的开销。图7分别展示了几种组合加载数据的Load时间。（Databricks, Delta Databricks, Parquet 3rd-Party Spark ， 数据集为400 GB TPC-DS store_sales table），硬件资源为（one i3.2xlarge master and eight i3.2xlarge workers ）。可以看到spark在写入Delta Lake的速度和Parquet没有差太多，这表明统计信息的收集并不会在写入数据时增加明显的overhead。&lt;/p>
&lt;p>（评：在较大的数据集上，overhead相对小，但是在小数据集，overhead就相对大了。overhead是根据query的基准SLA的一个相对值）&lt;/p>
&lt;h3 id="7-探讨--局限">7. 探讨 &amp;amp; 局限&lt;/h3>
&lt;p>从我们的经验看，Delta Lake展现出他能依托云对象存储来实现企业级数据处理所要求的ACID事务能力，能支持大规模的流式处理，批处理，和交互式查询工作负载。Delta Lake的设计是很有吸引力的，因为他在使用云存储时并不需要一个很重的中间件层服务，这也让他能够很容易被一些支持Parquet的查询引擎所直接使用。Delta Lake的ACID能力带来了很强大的管理能力和性能提升，但说实话，目前的设计还是有很多局限的，这也是未来工作的方向。&lt;/p>
&lt;p>首先，Delta Lake目前只支持单表的序列化级别的事务，因为每张表都有它自己的事务日志。如果有跨表的事务日志将能打破这个局限，但这可能会显著的增加并发乐观锁的竞争（在给日志记录文件做append时）。在高TPS的事务场景下，一个coordinator是可以承接事务log写入的，这样能解决事务直接读写对象存储。&lt;/p>
&lt;p>然后，在流式工作负载下，Delta Lake受限于云对象存储的latency。比如，使用对象存储的API很难达到ms级的流式延迟要求。另外一边看，我们发现大企业的用户一般都跑并行的任务，在使用Delta Lake去提供秒级的服务延迟在大多数场景下也是能够接受的。&lt;/p>
&lt;p>第三，Delta Lake目前不支持二级索引（只有数据对象级别的min/max统计），我们已经开始着手开发一个基于Bloom filter的index了。Delta 的ACID事务能力，允许我们以事务的方式更新这些索引。&lt;/p>
&lt;h3 id="8-相关工作">8. 相关工作&lt;/h3>
&lt;p>很多学术界的研究和工业界的项目都在思考使用云环境去做数据管理系统。比如，Branter，在基于S3开发OLTP数据库；在最终一致性的KVstore上去实现因果一致性；AWS Aruora 是一个商业的OLTP DBMS系统，它拥有存储计算分离的架构；Google BigQuery ，AWS Redshift Spectrum [39] 和 Snowflake [23] 都是OLAP的DBMS系统，他们都做到了存储计算分离，单独的计算集群都可以访问共享的云对象存储上的数据。其他一些项目，在考虑怎样自动的让DBMS引擎适应弹性、多租户的工作负载。&lt;/p>
&lt;p>Delta Lake参考了这些工作的vision，借助了广泛的云基础设施，但是有一些不同的目标。特别的看，很多云上的DBMS系统需要一个中间件层服务去桥接client端和storage层。（比如Aurora 和 Redshift都有一个frontend server来处理client端连接），这种方式增加了运维的负担（这个frontend节点需要一直保持running），需要考虑扩展性、可用性、在大规模写入数据时可能会造成问题。相反，Delta Lake允许多客户端仅仅依靠云对象存储就能独立的协调工作，不需要再依赖一个单独的服务了（当然在3.2.2章说到，在使用AWS S3时会依赖一个轻量级的日志记录存储服务），这种设计解放了用户的运维压力，同时还保证了弹性扩容读/写的能力。还有，这套架构的HA能力是和底层云对象存储的可用性相同的，在发生灾难事故时，没有什么组件需要重启 or 特殊对待。当然，这个设计能这么从容灵活，也是因为Delta Lake的目标场景天然特性：是OLAP场景，TPS很低频，但事务涉及的数据量很大，因此很适合这种乐观锁的设计。&lt;/p>
&lt;p>最接近Delta Lake设计和初衷的系统是 Apache Hudi[8] 和 Apache Iceberg[10],这两者都定义了数据格式，也都基于云对象存储实现了事务语义。这些系统没有能提供Delta Lake的所有能力，比如，其他两个系统都没有提供数据layout优化的特性（Z-Order），也没有提供把数据湖表当作streaming input 源的能力（Delta Lake的日志记录），也没有基于本地SSD做caching的Databricks runtime服务。&lt;/p>
&lt;p>还有，Hudi同时只能有一个write（类似悲观锁）。&lt;/p>
&lt;p>这些项目都和现在popular的计算引擎有结合，比如Spark 和 Presto，但都缺乏和商业数据仓库组件的connector（Redshift 和 Snowflake）【这个点应该动态发展的去看，有失偏颇】，而在Delta 我们实现了Manifest file以及一些商用的ETL tools。&lt;/p>
&lt;p>Apache Hive ACID[32] 也基于“对象存储”/“分布式文件系统”实现了事务能力，但它要依靠Hive metastore区去track每张表的状态。这会在有几百万分区的时候成为瓶颈（把底层mysql替换成兼容mysql协议的分布式NewSQL即可，比如tidb），也增加了用户的运维负担。Hive ACID 也没法做到 time travel。低延迟的基于HDFS的存储、比如HBase、Kudu，都可以在把数据写入HDFS前将很多small write做合并，但都需要一层独立的分布式文件系统或分布式服务层。在合并高性能的OLTP和OLAP负载之间还是有一条很长的距离的，这个领域也有被称为HTAP系统。这些提供通常会有一个单独的为OLTP优化的写入存储，然后有一个为OLAP优化的长期存储。在我们的实际工作中，我们非常想基于对象存储开发一个支持高TPS的并发协议，但不使用一个独立的外部存储系统。&lt;/p>
&lt;h3 id="9结论">9.结论&lt;/h3>
&lt;p>我们已经介绍了Delta Lake，一个在云对象存储上搭建的ACID的数据表存储层服务，它给数据仓库带来了很多DBMS-like系统的性能和管理数据的feature，但并没有带来太多overhead。&lt;/p>
&lt;p>它只是一种存储格式+一些客户端访问协议，这简化了维护成本，天生的highly available，也让客户端可以以 直接、高带宽的方式访问云对象存储。目前Delta Lake已经被几千家公司所使用，每天处理EB级（成千上万PB）的数据，被用来取代更复杂的基于很多数据系统柔和而成的复杂架构数仓。&lt;/p>
&lt;p>最后，Delta Lake是开源的，基于Apache 2 license at  https://delta.io.&lt;/p>
&lt;p>本文翻译自：https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf&lt;/p></description></item><item><title/><link>https://justice.bj.cn/post/30.architech/graphdb/janusgraph/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/graphdb/janusgraph/</guid><description>&lt;h2 id="janusgraph">JanusGraph&lt;/h2>
&lt;h3 id="简介">简介&lt;/h3>
&lt;p>JanusGraph是一个可扩展的图数据库，可以把包含数千亿个顶点和边的图存储在多机集群上。它支持事务，支持数千用户实时、并发访问存储在其中的图。（JanusGraph is a scalable &lt;a href="http://en.wikipedia.org/wiki/Graph_database">graph database&lt;/a> optimized for storing and querying graphs containing hundreds of billions of vertices and edges distributed across a multi-machine cluster. JanusGraph is a transactional database that can support thousands of concurrent users executing complex graph traversals in real time.)&lt;/p></description></item><item><title>AWS常用概念</title><link>https://justice.bj.cn/post/30.architech/aws-s3/aws-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/aws-s3/aws-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</guid><description>&lt;h1 id="aws常用概念">AWS常用概念&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>每个&lt;strong>Region&lt;/strong>都是完全独立的。每个&lt;strong>Availability Zone&lt;/strong>都是隔离的，但是Region中的可用区通过低延迟链接连接。&lt;strong>Local Zone&lt;/strong>是一种AWS基础架构部署，可将所选服务放置在更接近最终用户的位置。&lt;strong>Local Zone&lt;/strong>是与您所在区域不同位置的区域的扩展。它为AWS基础架构提供了高带宽主干，非常适合对延迟敏感的应用程序，例如机器学习。下图说明了区域，可用区域和本地区域之间的关系。&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/21-08-59-25-2020-03-03-12-48-56-image.png" alt="">&lt;/p>
&lt;p>Amazon EC2资源是以下资源之一：全局，与Region，&lt;strong>Availability Zone&lt;/strong>或&lt;strong>Local Zone&lt;/strong>绑定&lt;/p>
&lt;h2 id="region">Region&lt;/h2>
&lt;p>每个Amazon EC2 &lt;strong>Region&lt;/strong>都旨在与其他Amazon EC2 &lt;strong>Region&lt;/strong>隔离。这样可以实现最大的容错能力和稳定性。&lt;/p>
&lt;p>当您查看您的资源时，您只会看到与您指定的区域相关联的资源。这是因为区域彼此隔离，并且我们不会自动在区域之间复制资源。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>BlobFS</title><link>https://justice.bj.cn/post/40.storage/spdk/blobfs%E6%BA%90%E7%A0%81/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/spdk/blobfs%E6%BA%90%E7%A0%81/</guid><description>&lt;h1 id="blobfs">BlobFS&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>blobfs 是 spdk 中基于 blobstore 块设备实现的一个简易的文件系统。&lt;/p>
&lt;h2 id="编译">编译&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ yum install -y libaio libaio-devel ncurses ncureses-devel CUnit fuse3 fuse3-devel jq
$ pip3 install meson
$ wget https://sourceforge.net/projects/cunit/files/latest/download
$ git clone https://github.com/spdk/spdk
$ &lt;span class="nb">cd&lt;/span> spdk
$ git submodule update --init
$ sh scripts/pkgdep.sh
$ wget -O /tmp/ninja-linux.zip https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-linux.zip &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> unzip /tmp/ninja-linux.zip &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> mv ninja /usr/local/bin/
$ ./configure --with-fuse
$ wget http://mirror.centos.org/centos/8/BaseOS/x86_64/os/Packages/libaio-devel-0.3.112-1.el8.x86_64.rpm -o /tmp/
$ sudo rpm -ivh /tmp/
$ make -j8
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="挂载-blobfs">挂载 blobfs&lt;/h2>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1">#iommu 设置&lt;/span>
&lt;span class="c1">#/etc/default/grub中，增加&lt;/span>
&lt;span class="nv">GRUB_CMDLINE_LINUX&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;xxx default_hugepagesz=1G hugepagesz=1G hugepages=16 hugepagesz=2M hugepages=2048 intel_iommu=on&amp;#34;&lt;/span>
$ grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg &lt;span class="c1">#uefi&lt;/span>
$ grub2-mkconfig -o /boot/grub2/grub.cfg &lt;span class="c1">#no uefi&lt;/span>
$ mkdir -p /mnt/huge1G
$ &lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;nodev /mnt/huge1G hugetlbfs pagesize=1GB 0 0&amp;#34;&lt;/span> &amp;gt;&amp;gt; /etc/fstab
&lt;span class="c1"># 设置hugepage&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">16&lt;/span> &amp;gt; /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
$ mount -t hugetlbfs none /mnt/huge1G -o &lt;span class="nv">pagesize&lt;/span>&lt;span class="o">=&lt;/span>1G
&lt;span class="c1"># 加载vfio-pci 驱动模块&lt;/span>
$ modprobe vfio-pci
$ modprobe
&lt;span class="c1"># 查看nvme 设备&lt;/span>
$ lspci -nn &lt;span class="p">|&lt;/span> grep &lt;span class="s2">&amp;#34;Non-Volatile&amp;#34;&lt;/span>
&lt;span class="c1">#&amp;gt; 84:00.0 Non-Volatile memory controller [0108]: Intel Corporation NVMe Datacenter SSD [3DNAND, Beta Rock Controller] [8086:0a54]&lt;/span>
&lt;span class="c1"># [[[[&amp;lt;domain&amp;gt;]:]&amp;lt;bus&amp;gt;]:][&amp;lt;device&amp;gt;][.[&amp;lt;func&amp;gt;]]：0000:84:00.0&lt;/span>
&lt;span class="c1"># [&amp;lt;vendor&amp;gt;]:[&amp;lt;device&amp;gt;]： 8086:0a54&lt;/span>
&lt;span class="c1"># 解绑nvme驱动&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="s2">&amp;#34;0000:84:00.0&amp;#34;&lt;/span> &amp;gt;/sys/bus/pci/devices/0000:84:00.0/driver/unbind
&lt;span class="c1"># 绑定到vfio驱动&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">8086&lt;/span> 0a54 &lt;span class="p">|&lt;/span> tee /sys/bus/pci/drivers/vfio-pci/new_id
&lt;span class="c1">#&amp;gt; 绑定成功后，检查&lt;/span>
&lt;span class="c1">#&amp;gt; - lsblk 看不到nvme盘&lt;/span>
&lt;span class="c1">#&amp;gt; - vfio下面多了一个设备文件：ls /dev/vfio/4&lt;/span>
&lt;span class="c1"># 或者绑定到uio&lt;/span>
$ &lt;span class="nb">echo&lt;/span> &lt;span class="m">8086&lt;/span> 0a54 &amp;gt; /sys/bus/pci/drivers/uio_pci_generic/new_id
&lt;span class="c1"># 驱动准备完成&lt;/span>
$ &lt;span class="nb">cd&lt;/span> &amp;lt;SPDK_HOME&amp;gt;
&lt;span class="c1"># 生成nvme配置文件&lt;/span>
$ scripts/gen_nvme.sh --json-with-subsystems &amp;gt; nvme.json
&lt;span class="c1"># 查看状态&lt;/span>
$ sudo scripts/setup.sh status
&lt;span class="c1"># 设置hugepage&lt;/span>
$ &lt;span class="nv">HUGEMEM&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">5120&lt;/span> scripts/setup.sh
&lt;span class="c1"># 生成配置文件&lt;/span>
$ scripts/gen_nvme.sh --json-with-subsystems &amp;gt; nvme.json
&lt;span class="c1"># 初始化blobfs&lt;/span>
$ ./test/blobfs/mkfs/mkfs ./nvme.json Nvme0n1
&lt;span class="c1"># 挂载至/mnt/fuse&lt;/span>
$ ./test/blobfs/fuse/fuse ./nvme.json Nvme0n1 /mnt/fuse
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="源码解读">源码解读&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>lib&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>module/blobfs&lt;/code>目录&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>module&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// 检测bdev设备是否存在blobfs，若存在，则加载blobfs
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_detect&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1">//
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="k">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_create&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1">// 挂载
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="k">static&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="n">spdk_blobfs_bdev_mount&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="kt">char&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">bdev_name&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="kt">char&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">mountpoint&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">spdk_blobfs_bdev_op_complete&lt;/span> &lt;span class="n">cb_fn&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">cb_arg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="c1">// include 目录
&lt;/span>&lt;span class="c1">// spdk/include/blobfs.h
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// lib 目录
&lt;/span>&lt;span class="c1">// spdk/lib/blobfs.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// module目录
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_bdev_rpc.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_fuse.c
&lt;/span>&lt;span class="c1">// spdk/module/blobfs/bdev/blobfs_fuse.h
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// test目录
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/blobfs.sh //blobfs 功能测试启动脚本
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/fuse/fuse.c
&lt;/span>&lt;span class="c1">// spdk/test/blobfs/mkfs/mkfs.c
&lt;/span>&lt;span class="c1">&lt;/span>
&lt;span class="c1">// spdk/test/blobfs/rocksdb/
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="blobfs-patch">blobfs patch&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://review.spdk.io/gerrit/c/spdk/spdk/+/5420">https://review.spdk.io/gerrit/c/spdk/spdk/+/5420&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="patch">Patch&lt;/h2>
&lt;h3 id="5420-random-write">5420: random write&lt;/h3>
&lt;p>新 API&lt;code>spdk_file_randomwrite()&lt;/code>用于支持文件随机写。原有代码仅支持追加写，在追加写和随机读之间有一个内存缓存。&lt;/p>
&lt;p>为了支持随机写特性，需要进行如下工作：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>将缓存中的数据刷入到后端 blobstore 中；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除所有的缓存&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>BlobStore</title><link>https://justice.bj.cn/post/40.storage/spdk/blobstore/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/spdk/blobstore/</guid><description>&lt;h1 id="blobstore">BlobStore&lt;/h1>
&lt;hr>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>SPDK bdev层类似于内核中的通用块设备层，是对底层不同类型设备（如NVMe bdev、Malloc bdev、AIO bdev等）的统一抽象管理。&lt;/p>
&lt;p>BlobStore是位于SPDK bdev之上，通过不同层级的抽象，实现对磁盘块(LBA)的管理，实现了对Blob的管理，包括Blob的分配、删除、读取、写入、元数据的管理等；&lt;/p>
&lt;p>BlobFS是在Blobstore的基础上进行封装的一个轻量级文件系统，用于提供部分对于文件操作的接口，并将对文件的操作转换为对Blob的操作，&lt;/p>
&lt;p>用于与用户态文件系统Blobstore Filesystem （BlobFS）集成，从而代替传统的文件系统，支持更上层的服务，如数据库MySQL、K-V存储引擎Rocksdb以及分布式存储系统Ceph、Cassandra等。&lt;/p>
&lt;hr>
&lt;h2 id="blobstore-1">BlobStore&lt;/h2>
&lt;h3 id="数据块管理">数据块管理&lt;/h3>
&lt;p>在blobstore中，将SSD中的块划分为多个抽象层：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-54-17-2020-11-09-10-54-11-image.png" alt="">&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Logical Block&lt;/strong>：与块设备中所提供的逻辑块相对应，通常为512B或4KiB。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Page&lt;/strong>：由多个连续的Logical Block构成，通常一个page的大小为4KiB，因此一个Page由八个或一个Logical Block构成，取决于Logical Block的大小。&lt;/p>
&lt;ul>
&lt;li>在Blobstore中，Page是连续的，即从SSD的LBA 0开始，多个或一个块构成Page 0,接下来是Page 1，依次类推。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cluster&lt;/strong>：由多个连续的Page构成，通常一个Cluster的大小默认为1MiB，因此一个Cluster由256个Page构成。&lt;/p>
&lt;ul>
&lt;li>Cluster与Page一样，是连续的，即从SSD的LBA 0开始的位置依次为Cluster 0到Cluster N。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Blob&lt;/strong>：Blobstore中主要的操作对象为Blob，与BlobFS中的文件相对应，提供read、write、create、delete等操作。&lt;/p>
&lt;ul>
&lt;li>一个Blob由多个Cluster构成，但构成Blob中的Cluster并不一定是连续的。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="数据块管理-1">数据块管理&lt;/h2>
&lt;p>在Blobstore中，会将cluster 0作为一个特殊的cluster。&lt;/p>
&lt;p>该cluster用于存放Blobtore的所有信息以及元数据，对每个blob数据块的查找、分配都是依赖cluster 0中所记录的元数据所进行的。&lt;/p>
&lt;p>Cluster 0的结构如下：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-54-53-2020-11-09-10-54-48-image.png" alt="">&lt;/p>
&lt;p>Cluster 0中的第一个page作为super block，Blobstore初始化后的一些基本信息都存放在super block中，例如cluster的大小、已使用page的起始位置、已使用page的个数、已使用cluster的起始位置、已使用cluster的个数、Blobstore的大小等信息。&lt;/p>
&lt;hr>
&lt;h3 id="元数据">元数据&lt;/h3>
&lt;p>Cluster 0中的其它page将组成元数据域（metadata region）。元数据域主要由以下几部分组成：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-14-10-48-2020-11-09-14-10-36-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Metadata Page Allocation：用于记录所有元数据page的分配情况。在分配或释放元数据页后，将会对metadata page allocation中的数据做相应的修改。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Cluster Allocation：用于记录所有cluster的分配情况。在分配新的cluster或释放cluster后会对cluster allocation中的数据做相应的修改。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Blob Id Allocation：用于记录blob id的分配情况。对于blobstore中的所有blob，都是通过唯一的标识符blob id将其对应起来。在元数据域中，将会在blob allocation中记录所有的blob id分配情况。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Metadata Pages Region：元数据页区域中存放着每个blob的元数据页。每个blob中所分配的cluster都会记录在该blob的元数据页中，在读写blob时，首先会通过blob id定位到该blob的元数据页，其次根据元数据页中所记录的信息，检索到对应的cluster。对于每个blob的元数据页，并不是连续的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>对于一个blob来说，metadata page记录了该blob的所有信息，数据存放于分配给该blob的cluster中。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在创建blob时，首先会为其分配blob id以及metadata page，其次更新metadata region。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当对blob进行写入时，首先会为其分配cluster，其次更新该blob的metadata page，最后将数据写入，并持久化到磁盘中。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>为了实现对磁盘空间的动态分配管理，Blobstore中为每个blob分配的cluster并不是连续的。&lt;/p>
&lt;p>对于每个blob，通过相应的结构维护当前使用的cluster以及metadata page的信息：clusters与pages。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Cluster: 记录了当前该blob所有cluster的LBA起始地址，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pages: 记录了当前该blob所有metadata page的LBA起始地址。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Blobstore实现了对磁盘空间分配的动态管理，并保证断电不丢失数据，具有persistent特性。&lt;/p>
&lt;p>Blobstore中的配置信息与数据信息均在super block与metadata region中管理，在重启后，若要保持persistent，可以通过Blobstore中所提供的load操作。&lt;/p>
&lt;p>&lt;strong>注意&lt;/strong>：&lt;/p>
&lt;blockquote>
&lt;p>Blob的persistent主要是针对NVMe这类bdev。对于Malloc bdev，由于其本身的性质，是无法保证Blob的persistent，需要重启后进行重新配置。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="blobfs">BlobFS&lt;/h2>
&lt;h3 id="blobfs-文件接口">BlobFS 文件接口&lt;/h3>
&lt;p>blobfs文件系统接口实现了基本的文件操作，&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>操作&lt;/th>
&lt;th>同步API&lt;/th>
&lt;th>异步API&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>打开文件&lt;/td>
&lt;td>spdk_fs_open_file&lt;/td>
&lt;td>spdk_fs_open_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>创建文件&lt;/td>
&lt;td>spdk_fs_create_file&lt;/td>
&lt;td>spdk_fs_create_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>删除文件&lt;/td>
&lt;td>spdk_fs_delete_file&lt;/td>
&lt;td>spdk_fs_delete_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>重命名文件&lt;/td>
&lt;td>spdk_fs_rename_file&lt;/td>
&lt;td>spdk_fs_rename_file_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>文件状态&lt;/td>
&lt;td>spdk_fs_file_stat&lt;/td>
&lt;td>spdk_fs_file_stat_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>写&lt;/td>
&lt;td>spdk_file_write&lt;/td>
&lt;td>spdk_file_write_async/sspdkfile_writev_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>读&lt;/td>
&lt;td>spdk_file_read&lt;/td>
&lt;td>spdk_file_read_async/sspdkfile_readv_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>truncate&lt;/td>
&lt;td>spdk_file_truncate&lt;/td>
&lt;td>spdk_file_truncate_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sync&lt;/td>
&lt;td>spdk_file_sync&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>关闭&lt;/td>
&lt;td>spdk_file_close&lt;/td>
&lt;td>spdk_file_close_async&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h3 id="缓存">缓存&lt;/h3>
&lt;p>为了提高文件的读取效率，BlobFS在内存中提供了cache buffer，由多层树结构组成，其结构如下所示：&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-58-02-2020-11-09-10-56-44-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>最底层Level 0叶子节点为buffer node，是用于存放数据的buffer。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Level 0以上的其它层中，均为tree node，用于构建树的索引结构。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在文件读写的时候，根据文件结构中的根节点以及读取位置的offset信息，在树结构中通过索引查找buffer node的位置，即从Level N，逐步定位到对应的Level 0的叶子节点。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="写">写&lt;/h3>
&lt;p>BlobFS目前用于支持上层的Rocksdb，在Rocksdb的抽象环境层中提供文件的接口，目前仅支持append类型的写操作。&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-57-32-2020-11-09-10-57-04-image.png" alt="">&lt;/p>
&lt;hr>
&lt;p>在进行文件写入：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先会根据文件当前的写入位置检查是否符合cache buffer写入需求，若满足，则直接将数据写入到cache buffer中，同时触发异步的flush操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在flush的过程中，BlobFS触发Blob的写操作，将cache buffer中的数据，写入到文件对应blob的相应位置。若不满足cache buffer的写入需求，BlobFS则直接触发文件对应的blob的写操作。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Blobstore首先为该blob分配cluster，根据计算得到的写入LBA信息，向SPDK bdev层发送异步的写请求，将数据写入，并更新相应的元数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于元数据的更新，出于性能考虑，当前对元数据的更新都在内存中操作，当用户使用强制同步或卸载Blobstore时，更新后的元数据信息才会同步到磁盘中。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此外，blob结构中维护了两份可变信息（指cluster与metadata page）的元数据，分别为clean与active。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Clean中记录的是当前磁盘的元数据信息，&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而active中记录的是当前在内存中更新后的元数据信息。同步操作会将clean中记录的信息与active记录的信息相匹配。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="读流程">读流程&lt;/h3>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/09-10-47-19-2020-11-04-09-45-33-image.png" alt="">&lt;/p>
&lt;hr>
&lt;p>在文件读写时：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先会进行read ahead操作，将一部分数据从磁盘预先读取到内存的buffer中；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>其后，根据cache buffer的大小，对文件的I/O进行切分，使每个I/O的最大长度不超过一个cache buffer的大小；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对于拆分后的文件I/O，会根据其offset在cache buffer tree中查找相应的buffer；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>若存在，则直接从cache buffer中读取数据，进行memcpy；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>而对于没有缓存到cache buffer中的数据，将会对该文件的读取，转换到该文件对应的Blob进行读取。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对Blob读取时候，根据已打开的blob结构中记录的信息，可以获取该blob所有cluster的LBA起始位置，并根据读取位置的offset信息，计算相应的LBA地址。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后向SPDK bdev层发送异步的读请求，并等待I/O完成。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BlobFS所提供的读操作为同步读，I/O完成后会在callback函数中，通过信号量通知BlobFS完成信号，至此文件读取结束。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="blobfs-fuse">BlobFS FUSE&lt;/h3>
&lt;p>BlobFS提供了一个FUSE插件，用于将SPDK BlobFS作为内核文件系统安装，以便进行检查或调试。FUSE插件需要fuse3，并在系统上检测到fuse3时自动构建。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">test/blobfs/fuse/fuse /usr/local/etc/spdk/rocksdb.conf Nvme0n1 /mnt/fuse
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="k">static&lt;/span> &lt;span class="k">struct&lt;/span> &lt;span class="n">fuse_operations&lt;/span> &lt;span class="n">spdk_fuse_oper&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">getattr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_getattr&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">readdir&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_readdir&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">mknod&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_mknod&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">unlink&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_unlink&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">truncate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_truncate&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">utimens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_utimens&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">open&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_open&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">release&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_release&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">read&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_read&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">write&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_write&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">flush&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_flush&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">fsync&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_fsync&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">.&lt;/span>&lt;span class="n">rename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">fuse_rename&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">};&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="blobfs当前限制">BlobFS当前限制&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>现有BlobFS只在RocksDB上进行了测试，其他不同于RocksDB的文件系统使用场合可能会有问题，以后会进行更严格的测试；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>现在只支持同步操作API。异步API开发中，未经过严格测试，将于以后版本中完成；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>文件&lt;code>rename&lt;/code>API不是原子操作。将于未来版本修复；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当前不支持目录，只支持扁平的文件命名空间。文件名作为xattrs存储于blob中，文件名&lt;code>lookup&lt;/code>为&lt;code>O(n)&lt;/code>级。&lt;code>btree&lt;/code>版本目录支持实现将于未来版本支持；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当前&lt;code>write&lt;/code>操作仅支持append到文件末尾。任意位置写操作将在未来版本实现；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Blobstore实现对Blob管理，Blob类似与文件的概念，但又不完全等同于文件，Blob没有完全遵循文件的POSIX接口，因此避免与文件混淆，在SPDK中称之为Blob而不是File。&lt;/p>
&lt;p>Blobstore Filesystem （BlobFS）是基于Blobstore实现的轻量级文件系统，对Blobstore进行封装，提供一些文件的常用接口，如read、write、open、sync等，其目的在于作为文件系统支持更上层的应用，例如Rocksdb。但其本质仍然是Blobstore，因此命名为BlobFS。&lt;/p>
&lt;p>目前SPDK基于维护了Rocksdb的一个分支，该分支下的Rocksdb在环境抽象层主要通过BlobFS进行对接，I/O可以经由BlobFS绕过内核I/O栈。&lt;/p>
&lt;hr>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://spdk.io/doc/blobfs.html">SPDK: BlobFS (Blobstore Filesystem)&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.sdnlab.com/22880.html">https://www.sdnlab.com/22880.html&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.atzlinux.com/atzlinux/doc/os2atc2019/SPDK-bytedance-miaoyu.pdf">SPDK在字节跳动存储业务中的应⽤&lt;/a>&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>BlueStore</title><link>https://justice.bj.cn/post/40.storage/ceph/ceph-bluestore/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/ceph/ceph-bluestore/</guid><description>&lt;h1 id="bluestore">BlueStore&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Ceph早期的单机对象存储引擎是&lt;code>FileStore&lt;/code>，为了维护数据的一致性，写入之前数据会先写&lt;code>Journal&lt;/code>，然后再写到文件系统，会有一倍的写放大，而同时现在的文件系统一般都是日志型文件系统(ext系列、xfs)，文件系统本身为了数据的一致性，也会写&lt;code>Journal&lt;/code>，此时便相当于维护了两份&lt;code>Journal&lt;/code>；另外&lt;code>FileStore&lt;/code>是针对&lt;code>HDD&lt;/code>的，并没有对&lt;code>SSD&lt;/code>作优化，随着&lt;code>SSD&lt;/code>的普及，针对&lt;code>SSD&lt;/code>优化的单机对象存储也被提上了日程，&lt;code>BlueStore&lt;/code>便由此应运而出。&lt;/p>
&lt;p>&lt;code>BlueStore&lt;/code>最早在&lt;code>Jewel&lt;/code>版本中引入，用于在&lt;code>SSD&lt;/code>上替代传统的&lt;code>FileStore&lt;/code>。作为新一代的高性能对象存储后端，&lt;code>BlueStore&lt;/code>在设计中便充分考虑了对&lt;code>SSD&lt;/code>以及&lt;code>NVME&lt;/code>的适配。针对&lt;code>FileStore&lt;/code>的缺陷，&lt;code>BlueStore&lt;/code>选择绕过文件系统，直接接管裸设备，直接进行对象数据IO操作，同时元数据存放在&lt;code>RocksDB&lt;/code>，大大缩短了整个对象存储的IO路径。&lt;code>BlueStore&lt;/code>可以理解为一个支持&lt;code>ACID&lt;/code>事物型的本地日志文件系统。&lt;/p>
&lt;h2 id="为什么需要bluestore">为什么需要BlueStore&lt;/h2>
&lt;p>ceph是目前业内比较普遍使用的开源分布式存储系统，实现有多种类型的本地存储系统；在较早的版本当中，ceph默认使用FileStore作为后端存储，但是由于FileStore存在一些缺陷，重新设计开发了BlueStore，并在L版本之后作为默认的后端存储。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-04d2616b034660107a7fadccd7ea4fa0_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>IO放大&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>FileStore底层使用POSIX规范的文件系统接口，例如xfs、ext4、btrfs，然而这类文件系统本身不支持数据或元数据的事务操作接口（btrfs提供事务钩子的接口，但是测试过程中发现会导致系统宕机），而ceph对于数据写入要求十分严格，需要满足事务的特性（ACID）；为此FileStore实现了FileJournal功能，所有的事务都需要先写到FileJournal中，之后才会写入对应的文件中，以此来保证事务的原子性，但是这导致了数据“双写”的问题，造成至少一半磁盘带宽的浪费。&lt;/p>
&lt;p>此外xfs、ext4、btrfs这类文件系统本身存在一定的IO放大（即一次读写请求实际在低层磁盘发生的IO次数），再加上FileStore的日志双写，放大倍数成倍增加。&lt;/p>
&lt;p>下图中的数据表示了以block大小为单位对不同文件系统进行读写，在不同场景下的读写放大及空间放大情况。我们以ext4文件系统说明下个参数的含义。在对文件进行Overwrite时，即将数据覆盖写入到文件中，除了写入数据外，还涉及到日志的写入（其中日志写入两次，一次记录更改的inode，一次为commit记录，具体可参考[5])、文件inode的更改，每次写的最小单位是block，因此最终相当于写入次数以及空间放大了四倍；而在进行Append写入时，由于需要新分配空间，因此相对于Overwrite增加了bitmap的更改以及superblock的更改（superblock记录总的空间分配情况），写放大和空间放大均为六倍。读文件时，在没有命中任何缓存的情况下（cold cache），需要读大量元数据，例如：目录、文件inode、superblock等，最终读放大为六倍；而如果是在顺序读的情况下（warm cache），像superblock、bitmap、目录等这些元数据都缓存在内存中，只需读取文件inode和文件数据。&lt;/p>
&lt;p>同理，其他文件系统由于不同的结构和设计原理，其IO放大和空间放大系数也各不相同。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-39087a1897ba0faa3597ca786d579471_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>对象遍历&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>ceph的数据被划分为object存放，object以32位的hash值进行标识，ceph在进行scrubbing、backfill或者recovery时都需要根据hash值遍历这些object；POSIX文件系统不提供有序的文件遍历接口，为此FileStore根据文件的数量和hash的前缀将object划分到不同的子目录，其原则如下：&lt;/p>
&lt;ul>
&lt;li>当目录下的文件个数&amp;gt;100个时，拆分子目录；目录名以文件名的hash前缀为依据（拆分一级目录时，以hash第一位为拆分依据，二级目录以第二位hash为拆分依据，依次类推）&lt;/li>
&lt;li>当所有子目录下的文件个数&amp;lt;50个时，将合并到上级目录&lt;/li>
&lt;/ul>
&lt;p>因此FileStore在使用过程中需要不断合并拆分目录结构；这种方式将文件按照前缀放到不同目录，但对于同一目录中的文件依然无法很好排序，因此需要将目录中的所有文件读到内存进行排序，这样在一定程度上增加了CPU开销。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-3c8f6b97fea136794ccb7cb2a1f3d006_1440w.jpg" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>其他&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>FileStore由于设计的较早，无法支持当前较新的存储技术，例如使用spdk技术读写NVMe盘。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据和元数据分离不彻底。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>流控机制不完整导致IOPS和带宽抖动（FileStore自身无法控制本地文件系统的刷盘行为）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>频繁syncfs系统调用导致CPU利用率居高不下。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bluestore介绍">&lt;strong>BlueStore介绍&lt;/strong>&lt;/h2>
&lt;h2 id="需求">&lt;strong>需求&lt;/strong>&lt;/h2>
&lt;p>首先看下BlueStore设计之初的一些需求：&lt;/p>
&lt;ul>
&lt;li>对全SSD及全NVMe SSD闪存适配&lt;/li>
&lt;li>绕过本地文件系统层，直接管理裸设备，缩短IO路径&lt;/li>
&lt;li>严格分离元数据和数据，提高索引效率&lt;/li>
&lt;li>使用KV索引，解决文件系统目录结构遍历效率低的问题&lt;/li>
&lt;li>支持多种设备类型&lt;/li>
&lt;li>解决日志“双写”问题&lt;/li>
&lt;li>期望带来至少2倍的写性能提升和同等读性能&lt;/li>
&lt;li>增加数据校验及数据压缩等功能&lt;/li>
&lt;/ul>
&lt;h2 id="逻辑架构">&lt;strong>逻辑架构&lt;/strong>&lt;/h2>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-c3c40bb427a8a562eed3d2040b5f8a5e_1440w.jpg" alt="">&lt;/p>
&lt;p>BlueStore的逻辑架构如上图所示，模块的划分都还比较清晰，我们来看下各模块的作用：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>RocksDB&lt;/strong>：rocksdb是facebook基于leveldb开发的一款kv数据库，BlueStore将元数据全部存放至RocksDB中，这些元数据包括存储预写式日志、数据对象元数据、Ceph的omap数据信息、以及分配器的元数据 。&lt;/li>
&lt;li>&lt;strong>BlueRocksEnv&lt;/strong>：这是RocksDB与BlueFS交互的接口；RocksDB提供了文件操作的接口EnvWrapper，用户可以通过继承实现该接口来自定义底层的读写操作，BlueRocksEnv就是继承自EnvWrapper实现对BlueFS的读写。&lt;/li>
&lt;li>&lt;strong>BlueFS&lt;/strong>：BlueFS是BlueStore针对RocksDB开发的轻量级文件系统，用于存放RocksDB产生的.sst和.log等文件。&lt;/li>
&lt;li>&lt;strong>BlockDecive&lt;/strong>：BlueStore抛弃了传统的ext4、xfs文件系统，使用直接管理裸盘的方式；BlueStore支持同时使用多种不同类型的设备，在逻辑上BlueStore将存储空间划分为三层：慢速（Slow）空间、高速（DB）空间、超高速（WAL）空间，不同的空间可以指定使用不同的设备类型，当然也可使用同一块设备，具体我们会在后面的文章进行说明。&lt;/li>
&lt;li>&lt;strong>Allocator&lt;/strong>：负责裸设备的空间管理，只在内存做标记，目前支持StupidAllocator和BitmapAllocator两种分配器,Stupid基于extent的方式实现 。&lt;/li>
&lt;/ul>
&lt;h2 id="设计思想">&lt;strong>设计思想&lt;/strong>&lt;/h2>
&lt;p>在设计分布式文件系统的本地存储时，我们必须考虑数据的一致性和可靠性。在数据写入的过程中，由于可能存在异常掉电、进程崩溃等突发情况，导致数据还未全部写入成功便结束。虽然硬盘本身可以保证在扇区级别写入的原子性，但是一般文件系统的一个写请求通常包含多个扇区的数据和元数据更新，无法做到原子写。&lt;/p>
&lt;p>常用的解决办法是引入日志系统，数据写入磁盘之前先写到日志系统，然后再将数据落盘；日志写入成功后，即便写数据时出现异常，也可以通过日志回放重新写入这部分数据；如果写日志的过程中出现异常，则直接放弃这部分日志，视为写入失败即可，以此保证原子写入。但是这种方式导致每份数据都需要在磁盘上写入两次，严重降低了数据的写入效率。&lt;/p>
&lt;p>另一种方式则是采用ROW（Redirect on write）的方式，即数据需要覆盖写入时，将数据写到新的位置，然后更新元数据索引，这种方式由于不存在覆盖写，只需保证元数据更新的原子性即可。对于对齐的覆盖写入时，这种方式没有问题，但是如果是非对齐的覆盖写呢？&lt;/p>
&lt;p>我们举个例子：某文件的逻辑空间 [0,4096) 区间的数据在磁盘上的物理映射地址为[0, 4096)，磁盘的块（即磁盘读写的最小单元）大小为4096；如果要覆盖写文件[0,4096)区间的数据，那使用ROW的方式没有问题，重新再磁盘上分配一个新的块写入，然后更新元数据中的映射关系即可；但是如果写文件[512,4096)区域，也就是非对齐的覆盖写时，新分配的块中只有部分数据，旧的物理空间中仍有部分数据有效，这样元数据中需要维护两份索引，而且在读取文件的该块数据时，需要从多块磁盘块中读取数据，如果多次进行非对齐覆盖写，这种问题将更严重。&lt;/p>
&lt;p>解决这种问题办法是使用RMW（Read Modify Write）的方法，即在发生非对齐覆盖写时，先读取旧的数据，更新的数据合并后，对齐写入到磁盘中，从而减少元数据、提高读性能，但这种方式也存在一种缺点，写数据时需要先读数据，存在一定的性能损耗。&lt;/p>
&lt;p>分析完ROW的方式后，读者是否会有疑问，每次写入都放到新的位置，那么文件在磁盘中的物理连续性岂不是无法保证？的确，在传统的文件系统设计时，都是面向HDD盘，这种类型的盘在读写时会有磁头寻道的时间，对于非连续的物理空间读写，性能极差，在设计时会尽可能考虑数据存放的连续性，因此很少会采用ROW的方式。但是随着SSD盘的逐渐普及，随机读写的性能不再成为主要的性能关注点，越来越多的存储系统开始采用全闪存的磁盘阵列，相信ROW的方式会成为更加主流的方式。&lt;/p>
&lt;p>我们再来看下BlueStore是怎么实现的，BlueStore在设计时便考虑了全闪存的磁盘阵列，但是仍要考虑使用HDD盘的场景，因此并未完全采用ROW的方式。&lt;/p>
&lt;p>我们以下图为例进行说明，BlueStore提供了一个最小分配单元min_alloc_size的配置项，一般为磁盘块大小的整数倍，在此例中min_alloc_size为block大小的4倍。&lt;/p>
&lt;p>写入的数据如果与min_alloc_size大小对齐，则使用ROW的方式，将数据写到新的地址空间，然后更改元数据索引，并回收原先占用的空间，元数据更新的原子性由RocksDB的事务特性进行保障。&lt;/p>
&lt;p>而对于非min_alloc_size对齐的区域，则使用RMW的方式进行原地覆盖写（只读取非块大小对齐区域所在块，一般就是写入数据的第一个或最后一个块），写入的这部分数据可能跨多个块（因为min_alloc_size是块大小的整数倍），而磁盘只保证单个块大小的原子写入，对于多个块的原子写需要引入类似日志的功能，BlueStore用RocksDB来实现日志功能，将覆盖的这部分数据记到RocksDB中，完成以后再将数据覆盖写入到实际的数据区域，落盘成功以后再删除日志中的记录。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-206c24ef8e2e39e070256e2478a8f676_1440w.jpg" alt="">&lt;/p>
&lt;p> bluestore不使用本地文件系统，直接接管裸设备，并且只使用一个原始分区，HDD/SSD所在的物理块设备实现在用户态下使用linux aio直接对裸设备进行I/O操作。由于操作系统支持的aio操作只支持directIO，所以对BlockDevice的写操作直接写入磁盘，并且需要按照page对齐。其内部有一个aio_thread 线程，用来检查aio是否完成。其完成后，通过回调函数aio_callback 通知调用方。&lt;/p>
&lt;h2 id="存储模型">存储模型&lt;/h2>
&lt;p>&lt;img src="https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/03-14-38-09-2020-11-03-14-37-46-image.png?token=AARMIEXD2SMH33GE2RKURI27UD5RA" alt="">&lt;/p>
&lt;h3 id="缓存模块">缓存模块&lt;/h3>
&lt;p>BlueStore抛弃了文件系统，直接管理裸设备，用不了文件系统的Cache机制，自己实现元数据和数据的Cache。&lt;/p>
&lt;p>&lt;code>BlueStore&lt;/code>有两种Cache算法：&lt;code>LRU&lt;/code>和&lt;code>2Q&lt;/code>。元数据使用&lt;code>LRU&lt;/code>Cache策略，数据使用&lt;code>2Q&lt;/code>Cache策略。&lt;/p>
&lt;p>Bluestore实现了自己的缓存机制，定义了structure ：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>OnodeSpace，用来map 到内存中的ONODE；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BufferSpace，用来map 块信息blob，每个blob都在bufferSpace中缓存了状态数据。&lt;/p>
&lt;p>二者在缓存中依照LRU的方式决定生命周期。 &lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="freelistmanager模块">FreelistManager模块&lt;/h3>
&lt;p>FreelistManager用来映射磁盘的使用信息，最初实现是采用k-v的方式来存储对应的磁盘块的使用情况，但是由于更新数据时需要修改映射，需要线程锁来控制修改，而且这种方式对内存消耗很大；后续修改为bitmap的映射方式，设定一个offset来以bitmap的方式map多个block使用信息，使用XOR计算来更新块的使用情况，这种方式不会出现in-memory 状态。 &lt;/p>
&lt;h3 id="allocator模块">Allocator模块&lt;/h3>
&lt;p>用来委派具体哪个实际存储块用来存储当前的object数据；同样采用bitmap的方式来实现allocator，同时采用层级索引来存储多种状态，这种方式对内存的消耗相对较小，平均1TB磁盘需要大概35M左右的ram空间&lt;/p>
&lt;h2 id="bluestore的元数据管理">&lt;strong>BlueStore的元数据管理&lt;/strong>&lt;/h2>
&lt;p>bluestore自己管理裸盘，因此需要有元数据来管理对象，对应的就是Onode，Onode是常驻内存的数据结构，持久化的时候会以kv的形式存到rocksdb里。&lt;/p>
&lt;p>在onode里又分为lextent，表示逻辑的数据块，用一个map来记录，一个onode里会存在多个lextent，lextent通过blob的id对应到blob（bluestore_blob_t ），blob里通过pextent对应到实际物理盘上的区域（pextent里就是offset和length来定位物理盘的位置区域）。一个onode里的多个lextent可能在同一个blob里，而一个blob也可能对应到多个pextent。&lt;/p>
&lt;p>另外还有Bnode这个元数据，它是用来表示多个object可能共享extent，目前在做了快照后写I/O触发的cow进行clone的时候会用到。&lt;/p>
&lt;p>Onode代表对象，名字大概是从Linux VFS的Inode沿袭过来的。Onode常驻内存，在RocksDB中以KeyValue形式持久化；关于内存Cache的结构，在CDM的Slides中有讲。Onode包含多个lextent，即逻辑extent。Blob通过映射pextent、即物理extent，映射到磁盘上的物理区域。Blob通常包括来自同一个对象的多段数据，但是也可能被其它对象引用。Bnode是对象快照后，被用于多个对象共享数据的。&lt;/p>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/code/github/gitnote/img/2020-11-03-13-53-30-image.png" alt="">&lt;/p>
&lt;p>上面仅是关于对象映射的。更进一步，RocksDB中存储有许多类型的元数据，包括块分配、对象集合、快照、延迟写（Deferred Writes）、对象属性（Omap，即一个对象上可以附加一些KeyValue对作为属性，例如给图片加上地点、日期等），等等。在CDM的Slides中有详述。&lt;/p>
&lt;h2 id="bluestore的写路径">&lt;strong>BlueStore的写路径&lt;/strong>&lt;/h2>
&lt;p>写路径包含了对事务的处理，也回答了BlueStore如何解决日志双写问题。&lt;/p>
&lt;p>首先，Ceph的事务只工作于单个OSD内，能够保证多个对象操作被ACID地执行，主要是用于实现自身的高级功能。每个PG（Placement Group，类似Dynamo的vnode，将hash映射到同一个组内的对象组到一起）内有一个OpSequencer，通过它保证PG内的操作按序执行。事务需要处理的写分三种：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>写到新分配的区域。考虑ACID，因为此写不覆盖已有数据，即使中途断电，因为RocksDB中的元数据没有更新，不用担心ACID语义被破坏。后文可见RocksDB的元数据更新是在数据写之后做的。因而，日志是不需要的。在数据写完之后，元数据更新写入RocksDB；RocksDB本身支持事务，元数据更新作为RocksDB的事务提交即可。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>写到Blob中的新位置。同理，日志是不需要的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Deferred Writes（延迟写），只用于覆写（Overwrite）情况。从上面也可以看到，只有覆写需要考虑日志问题。如果新写比块大小（min_alloc_size）更小，那么会将其数据与元数据合并写入到RocksDB中，之后异步地把数据搬到实际落盘位置；这就是日志了。如果新写比块大小更大，那么分割它，整块的部分写入新分配块中，即按（1）处理，；不足的部分按（3）中上种情况处理。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>上述基本概述了BlueStore的写处理。可以看到其是如何解决FileStore的日志双写问题的。首先，没有Linux文件系统了，也就没有了多余的Journaling of Journal问题。然后，大部分写是写到新位置的，而不是覆写，因此不需要对它们使用日志；写仍然发生了两次，第一次是数据落盘，然后是RocksDB事务提交，但不再需要在日志中包含数据了。最后，小的覆写合并到日志中提交，一次写完即可返回用户，之后异步地把数据搬到实际位置（小数据合并到日志是个常用技巧）；大的覆写被分割，整块部分用Append-only方式处理，也绕开了日志的需要。至此，成为一个自然而正常的处理方式。（P.S.总之，个人感觉日志双写不是一个该存在的问题，不知为何成了一个问题，好在今天终于不是问题了。）&lt;/p>
&lt;p>更深入地，Ceph的开发文档中列出了所有的写策略处理方式。可以看到Inline Compression也是BlueStore的功能点之一；其中也有对Partial-write问题的处理。&lt;/p>
&lt;p>CDM的Slides中有BlueStore写的状态机图。状态机是存储中常用的处理方式，处理写路径，Ceph的PG Peering过程也有相应的状态机。数据落盘，对应的是PREPARE-&amp;gt;AIO_WAIT间的“Initiate some AIO”一步。之后经过多个队列，向RocksDB提交事务，以及完成Deferred Write和Cleanup。直到最终完成。&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/03-13-56-33-2020-11-03-13-56-11-image.png?token=AARMIEVN2WIMILBRLCACEIC7UDYU6" alt="">&lt;/p>
&lt;p>另外，BlueStore使用Direct IO提交数据，这样数据会立即落盘，而不是在内核中缓存；从而，存储系统可以完全自主地控制写的持久化。这是一个如今常见的做法。但代价是，不能利用内核缓存，需要自己处理缓存问题；也必须处理好数据对齐，以及写小于一扇区时的Partial-write问题。&lt;/p>
&lt;h2 id="bluefs的架构">BlueFS的架构&lt;/h2>
&lt;p>BlueFS以尽量简单为目的设计，专门用于支持RocksDB；RocksDB总之还是需要一个文件系统来工作的。BlueFS不支持POSIX接口。总的来说，它有这些特点：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>目录结构方面，BlueFS只有扁平的目录结构，没有树形层次关系；用于放置RocksDB的db.wal/，db/，db.slow/文件。这些文件可以被挂载到不同的硬盘上，例如db.wal/放在NVMRAM上；db/包含热SST数据，放在SSD上；db.slow/放在磁盘上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据写入方面，BlueFS不支持覆写，只支持追加（Append-only）。块分配粒度较粗，越1MB。有垃圾回收机制定期处理被浪费掉的空间。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对元数据的操作记录到日志，每次挂载时重放日志，来获得当前的元数据。元数据生存在内存中，并没有持久化在磁盘上，不需要存储诸如空闲块链表之类的。当日志过大时，会进行重写Compact。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Superblock用于存储整个文件系统级别的元数据，日志和数据本着尽量简单的设计，按照追加的方式不断写入。关于写放大的问题，这是Append-only式通有的，在Write Behaviors论文中有详述。&lt;/p>
&lt;h2 id="总结">&lt;strong>总结&lt;/strong>&lt;/h2>
&lt;p>BlueStore的设计考虑了FileStore中存在的一些硬伤，抛弃了传统的文件系统直接管理裸设备，缩短了IO路径，同时采用ROW的方式，避免了日志双写的问题，在写入性能上有了极大的提高。&lt;/p>
&lt;p>通过分析BlueStore的基本结构、考虑的问题以及设计思想，我们对于BlueStore有了大概的了解；BlueStore在设计时有考虑到未来存储的应用环境，是一种比较先进的本地文件系统，但也不可避免存在一些缺陷，例如较为复杂的元数据结构和IO逻辑，在大量小IO下可能存在的double write问题，较大的元数据内存占用等（当然有些问题在ceph的使用场景下可能不存在，但是我们如果希望借鉴BlueStore来设计本地文件系统就不得不考虑这些问题）。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/news/45599">Ceph BlueStore 和双写问题 - 云+社区 - 腾讯云&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cloud-atlas.readthedocs.io/zh_CN/latest/ceph/bluestore.html">https://cloud-atlas.readthedocs.io/zh_CN/latest/ceph/bluestore.html&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.cnblogs.com/fengjian2016/p/9747689.html">Ceph的BlueStore总体介绍 - fengjian1585 - 博客园&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/46362124">https://zhuanlan.zhihu.com/p/46362124&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/91015613">https://zhuanlan.zhihu.com/p/91015613&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.51cto.com/wendashuai/2500499">BlueStore源码分析之对象IO-Darren_Wen-51CTO博客&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.codeleading.com/article/8882498924/">ceph bluestore 磁盘空间管理源码解析 - 代码先锋网&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/68067068">https://zhuanlan.zhihu.com/p/68067068&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/91018497">BlueStore源码分析之BitMap分配器&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://docs.google.com/presentation/d/1_1Otkgv76fbCU2Zogjz748sEAG-1Nfiidbb6mgTON-A/edit#slide=id.p">https://docs.google.com/presentation/d/1_1Otkgv76fbCU2Zogjz748sEAG-1Nfiidbb6mgTON-A/edit#slide=id.p&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.e-learn.cn/topic/3434557">BlueStore源码分析之FreelistManager | 易学教程&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>BookKeeper</title><link>https://justice.bj.cn/post/30.architech/bookkeeper/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/bookkeeper/</guid><description>&lt;h1 id="bookkeeper">BookKeeper&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>BookKeeper是一个可靠的日志流记录系统，用于将系统产生的日志(也可以是其他数据)记录在BookKeeper集群上，由BookKeeper这个第三方Storage保证数据存储的可靠和一致性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>典型场景是系统写write-ahead log，即先把log写到BookKeeper上，BookKeeper诞生于Hadoop2.0的namenode HA，由yahoo于2009年创建，并在2011年开源。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/13-09-49-25-2019-12-10-19-03-10-image.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://gitee.com/justice/gitnote-img-bed/raw/master/2020/11/13-09-49-44-2019-12-10-18-03-31-image.png" alt="Apache BookKeeper 架构图">&lt;/p>
&lt;h2 id="复制">复制&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>BookKeeper对所有数据都会复制和存储相同的多份拷贝——一般是三份或是五份——到不同的机器上，可以是同一数据中心，也可以是跨数据中心。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不像其他使用主/从或是管道复制算法在副本之间复制数据的分布式系统（例如Apache HDFS、Ceph、Kafka），Apache BookKeeper使用一种多数投票并行复制算法在确保可预测的低延时的基础上复制数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>BookKeeper复制基于以下核心思想：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>日志流的原子结构是记录而不是字节。也就是说，数据总是以不可分割的记录形式（包括了元数据）存放的，而不是一个个字节组成的数组。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志流中记录的顺序与实际记录的实际存储是解耦的。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>一个 &lt;strong>BookKeeper 集群&lt;/strong>包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Bookies：一组独立的存储服务器&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>元数据存储&lt;/strong>系统：用于服务发现和元数据管理&lt;/p>
&lt;p>BookKeeper 客户端可以使用较高级别的 DistributedLog API（也称为&lt;strong>日志流 API&lt;/strong>）或较低级别的** ledger API**。Ledger API 允许用户直接与 bookies 交互。下图即为 BookKeeper 安装的典型示例。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkHv9TACJCDicQzzDs25hTzvIgQ4axO43jbDOj4pFdicP1dgEhPwmUWnSg/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>典型的 BookKeeper 安装（通过多个 API 连接的应用程序）&lt;/p>
&lt;hr>
&lt;p>&lt;strong>流存储要求&lt;/strong>&lt;/p>
&lt;p>在上篇文章中已经提到，实时存储平台应该&lt;strong>同时&lt;/strong>满足以下要求：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>即使在强持久性条件下&lt;/strong>，客户端也能够以极低的延迟（小于 5 毫秒）读写 entry 流&lt;/p>
&lt;/li>
&lt;li>
&lt;p>能够持久、一致、容错地存储数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在写入时，客户端能够进行流式传输或追尾传输&lt;/p>
&lt;/li>
&lt;li>
&lt;p>有效存储数据，支持访问历史数据与实时数据&lt;/p>
&lt;p>BookKeeper 通过提供以下保证来&lt;strong>同时&lt;/strong>满足上述各项要求：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkPUekjeqBcL7WaPz8S1zgIcqW2pTDdaG0aOzQfp26Uyo2fJiaZrE2b8A/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>&lt;strong>多副本&lt;/strong>&lt;/p>
&lt;p>BookKeeper 在一个数据中心内的多个机器上，或是多个数据中心之间，复制每条数据记录并存储多个副本（通常是 3 个或 5 个副本）。&lt;/p>
&lt;p>一些分布式系统使用主/从或管道复制算法在副本之间复制数据（例如，Apache HDFS、Ceph、Kafka 等），BookKeeper 的不同之处在于使用 &lt;strong>quorum-vote 并行复制算法&lt;/strong>来复制数据，以确保可预测的低延迟。下图即为 BookKeeper 集成中的多副本。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkZNRjkWuaYuybA5bYIgbia5IGCScA5TnMYiaRsaWZdIeKwZLia4TOQpGag/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">BookKeeper 多副本中的 ensemble、写入、ack quorum&lt;/p>
&lt;p>在上图中：&lt;/p>
&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>
&lt;p>从 BookKeeper 集群中（自动）选择一组 bookies（图例中为 bookies 1-5）。这一组 bookies 即为给定 **ledger **上用于存储数据记录的 &lt;strong>ensemble&lt;/strong>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ledger 中的数据分布在 bookies 的 ensemble 中。也就是说，每条记录都存有多个副本。用户可以在客户端级别配置副本数，即&lt;strong>写入 quorum 大小&lt;/strong>。在上图中，写入 quorum 大小为 3，即记录写入到 bookie 2、bookie 3 与 bookie 4。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>客户端向 ensemble 中写入数据记录时，需要等待直至有指定数量的副本发送确认（ack）。副本数即为 &lt;strong>ack quorum 大小&lt;/strong>。接收到指定数量的 ack 后，客户端默认写入成功。在上图中，ack quorum 大小为 2，也就是说，比如 bookie 3 和 bookie 4 存储数据记录，则向客户端发送一条确认。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当 bookie 发生故障时，ensemble 的组成会发生变化。正常的 bookies 会取代终止的 bookies，这种取代可能只是暂时的。例如：如果 &lt;strong>Bookie 5&lt;/strong> 终止，&lt;strong>Bookie x&lt;/strong> 可能会取代它。&lt;/p>
&lt;p>&lt;strong>多副本：核心理念&lt;/strong>&lt;/p>
&lt;p>BookKeeper 多副本基于以下核心理念：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志流面向记录而不是面向字节。这意味着，数据总是存储为不可分割的记录（包括元数据），而不是存储为单个字节数组。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>日志（流）中记录的顺序与记录副本的实际存储顺序分离。&lt;/p>
&lt;p>这两个核心理念确保 BookKeeper 多副本能够实现以下几项功能：&lt;/p>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>为向 bookies 写入记录提供多种选择，从而确保即使集群中多个 bookies 终止或运行缓慢，写入操作仍然可以完成（只要有足够的容量来处理负载）。可以通过改变 ensemble 来实现。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过增加 ensemble 大小来最大化单个日志（流）的带宽，以使单个日志不受一台或一小组机器的限制。可以通过将 ensemble 大小配置为大于写入 quorum 大小来实现。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过调整 ack quorum 大小来改善追加时的延迟。这对于确保 BookKeeper 的低延迟十分重要，同时还可以提供一致性与持久性保证。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过多对多副本恢复提供快速&lt;strong>再复制&lt;/strong>（再复制为复制不足的记录创建更多副本，例如：副本数小于写入 quorum 大小）。所有的 bookies 都可以作为记录副本的提供者&lt;em>与&lt;/em>接受者。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="持久性">持久性&lt;/h2>
&lt;p>保证复制每条写入 BookKeeper 的数据记录，并持久化到指定数量的 bookies 中。可以通过使用磁盘 fsync 和写入确认来实现。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>在单个 bookie 上，将确认发送给客户端之前，数据记录已明确写入（启用 fsync）磁盘，以便在发生故障时能够持久保存数据。这样可以保证数据写入到持久化存储中不依赖电源，可以被重新读取使用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在单个集群内，复制数据记录到多个 bookies，以实现容错。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>仅当客户端收到指定数量（通过 ack quorum 大小指定）的 bookies 响应时，才 ack 数据记录。&lt;/p>
&lt;p>最新的 NoSQL 类型数据库、分布式文件系统和消息系统（例如：Apache Kafka）都假定：保证最佳持久化的有效方式是将数据复制到多个节点的内存中。但问题是，这些系统允许潜在的数据丢失。&lt;/p>
&lt;p>&lt;strong>BookKeeper 旨在提供更强的持久性保证，完全防止数据丢失，从而满足企业的严格要求。&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="一致性">一致性&lt;/h3>
&lt;p>保证一致性是分布式系统中的常见问题，尤其是在引入多副本以确保持久性和高可用时。BookKeeper 为存储在日志中的数据提供了简单而强大的一致性保证（可重复读取的一致性）：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>如果记录已被引用程序 ack，则必须&lt;strong>立即&lt;/strong>可读。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果记录被读取一次，则必须&lt;strong>始终&lt;/strong>可读。如果记录 &lt;strong>R&lt;/strong> 成功写入，则在 &lt;strong>R&lt;/strong> 之前的所有记录都已成功提交/保存，并且将始终可读。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在不同读者之间，存储记录的顺序必须完全相同且可重复。&lt;/p>
&lt;p>这种可重复读取的一致性由 BookKeeper 中的 LastAddConfirmed（LAC）协议实现。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="高可用">高可用&lt;/h3>
&lt;p>在 CAP（Consistency：一致性、Availability：高可用、Partition tolerance：分区容错）条件下，BookKeeper 是一个 CP 系统。&lt;/p>
&lt;p>但实际上，即使存在硬件、网络或其他故障，Apache BookKeeper 仍然可以提供高可用性。为保证写入与读取的高可用性能，BookKeeper 采用了以下机制：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/4ibRRsibIGr0agzRnWA5xz0jmRCKR0arKkkeiauEpNQvI7UticfwQkH4NGerkEJkgLnEIO8Wpd0wEicOLX36SrhVN4Q/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="低延迟">低延迟&lt;/h3>
&lt;p>强持久性和一致性是分布式系统的复杂问题，特别是当分布式系统还需要满足企业级低延迟时。BookKeeper 通过以下方式满足这些要求：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在单个 bookie 上，bookie 服务器旨在用于不同工作负载（写入、追尾读、追赶读/随机读）之间的I/O 隔离。在 journal 上部署 &lt;strong>group-committing 机制&lt;/strong>以平衡延迟与吞吐量。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用 &lt;strong>quorum-vote 并行复制 schema&lt;/strong> 缓解由于网络故障、JVM 垃圾回收暂停和磁盘运行缓慢引起的延迟损失。这样不仅可以改善追尾延迟，还能保证可预测的 p99 低延迟。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用长轮询机制在 ack 并确认新记录后，立刻向追尾的写入者发出通知并发送记录。&lt;/p>
&lt;p>最后，值得一提的是，明确 fsync 和写入确认的持久性与可重复的读取一致性对于状态处理（尤其是流应用程序的 effectively-once 处理）非常重要。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="http://matt33.com/2019/01/28/bk-store-realize/">http://matt33.com/2019/01/28/bk-store-realize/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://cloud.tencent.com/developer/news/339495">为何选择Apache BookKeeper？第一部分：一致性、持久性以及可用性 - 云+社区 - 腾讯云&lt;/a>&lt;/p>
&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>Ceph基础</title><link>https://justice.bj.cn/post/40.storage/ceph/ceph/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/40.storage/ceph/ceph/</guid><description>&lt;h1 id="ceph基础">Ceph基础&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Ceph 最初是一项关于存储系统的 PhD 研究项目，由 Sage Weil 在 University of California, Santa Cruz（UCSC）实施。&lt;/p>
&lt;h2 id="特性">特性&lt;/h2>
&lt;h3 id="优点">优点&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Ceph支持对象存储、块存储和文件存储服务，故称为统一存储。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>采用CRUSH算法，数据分布均衡，并行度高，不需要维护固定的元数据结构；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据具有强一致，确保所有副本写入完成才返回确认，适合读多写少场景；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>去中心化，MDS之间地位相同，无固定的中心节点&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="缺点">缺点&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>去中心化的分布式解决方案，需要提前做好规划设计，对技术团队的要求能力比较高。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ceph扩容时，由于其数据分布均衡的特性，会导致整个存储系统性能的下降。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2019-12-19-16-33-55-image.png" alt="">&lt;/p>
&lt;h2 id="基本概念">基本概念&lt;/h2>
&lt;h3 id="crush算法">CRUSH算法&lt;/h3>
&lt;p>CEPH的数据分布算法，它是一个分层的，区分故障域的分布式算法。在CRUSH算法中，对于不同的物理设备统一抽象成了bucket，每个结点都是一个bucket，其对应的物理结构各不相同。例如下图中的root，row（机架）， cabinet（机柜）， disk都是bucket的一种&lt;/p>
&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2019-12-19-16-42-38-image.png" title="" alt="" data-align="center">
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://www.infoq.cn/article/BRjtISYrUdHgec4ODExH">分布式存储 Ceph 介绍及原理架构分享（上）-InfoQ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://juejin.im/entry/5b208433518825137b50cd10">https://juejin.im/entry/5b208433518825137b50cd10&lt;/a>&lt;/li>
&lt;li>&lt;/li>
&lt;/ol></description></item><item><title>Docker</title><link>https://justice.bj.cn/post/32.cloudnaive/docker%E7%AE%80%E4%BB%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/32.cloudnaive/docker%E7%AE%80%E4%BB%8B/</guid><description>&lt;h1 id="docker">Docker&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>docker是一个开源的应用容器引擎，基于go语言开发并遵循了apache2.0协议开源。&lt;/p>
&lt;p>docker可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的linux服务器，也可以实现虚拟化。&lt;/p>
&lt;p>容器是完全使用沙箱机制，相互之间不会有任何接口（类iphone的app），并且容器开销极其低。&lt;/p>
&lt;h3 id="容器和虚拟机">容器和虚拟机&lt;/h3>
&lt;p>容器时在linux上本机运行，并与其他容器共享主机的内核，它运行的一个独立的进程，不占用其他任何可执行文件的内存，非常轻量。&lt;/p>
&lt;p>虚拟机运行的是一个完成的操作系统，通过虚拟机管理程序对主机资源进行虚拟访问，相比之下需要的资源更多。&lt;/p>
&lt;p> &lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQh5SJzVziaQ5Zhib5Z2hlSNS1xwjicXK5fra81kibukKyz2K8ZYRmBrtGdQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="8容器在内核中支持2种重要技术">&lt;strong>8、容器在内核中支持2种重要技术&lt;/strong>&lt;/h3>
&lt;p>docker本质就是宿主机的一个进程，docker是通过namespace实现资源隔离，通过cgroup实现资源限制，通过写时复制技术（copy-on-write）实现了高效的文件操作（类似虚拟机的磁盘比如分配500g并不是实际占用物理磁盘500g）&lt;/p>
&lt;p>1）namespaces 名称空间&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQiaXAsDznWiaTVMEgH9l7wg603nZJ7ia0yib2AtrvNwfLMIDWg1raSTBSBQ/640?wx_fmt=png&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p> 2）control Group 控制组&lt;/p>
&lt;p>cgroup的特点是：　&lt;/p>
&lt;ul>
&lt;li>
&lt;p>cgroup的api以一个伪文件系统的实现方式，用户的程序可以通过文件系统实现cgroup的组件管理&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cgroup的组件管理操作单元可以细粒度到线程级别，另外用户可以创建和销毁cgroup，从而实现资源载分配和再利用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>所有资源管理的功能都以子系统的方式实现，接口统一子任务创建之初与其父任务处于同一个cgroup的控制组&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>四大功能：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>资源限制：可以对任务使用的资源总额进行限制&lt;/p>
&lt;/li>
&lt;li>
&lt;p>优先级分配：通过分配的cpu时间片数量以及磁盘IO带宽大小，实际上相当于控制了任务运行优先级&lt;/p>
&lt;/li>
&lt;li>
&lt;p>资源统计：可以统计系统的资源使用量，如cpu时长，内存用量等&lt;/p>
&lt;/li>
&lt;li>
&lt;p>任务控制：cgroup可以对任务执行挂起、恢复等操作&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="heading">&lt;/h3>
&lt;h2 id="重要概念">重要概念&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>image镜像：docker镜像就是一个只读模板，比如，一个镜像可以包含一个完整的centos，里面仅安装apache或用户的其他应用，镜像可以用来创建docker容器，另外docker提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下周一个已经做好的镜像来直接使用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>container容器：docker利用容器来运行应用，容器是从镜像创建的运行实例，它可以被启动，开始、停止、删除、每个容器都是互相隔离的，保证安全的平台，可以吧容器看做是要给简易版的linux环境（包括root用户权限、镜像空间、用户空间和网络空间等）和运行再其中的应用程序&lt;/p>
&lt;/li>
&lt;li>
&lt;p>repostory仓库：仓库是集中存储镜像文件的沧桑，registry是仓库主从服务器，实际上参考注册服务器上存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>仓库分为两种，公有参考，和私有仓库，最大的公开仓库是docker Hub，存放了数量庞大的镜像供用户下周，国内的docker pool，这里仓库的概念与Git类似，registry可以理解为github这样的托管服务&lt;/p>
&lt;h2 id="架构">架构&lt;/h2>
&lt;h3 id="1总体架构">&lt;strong>1、总体架构&lt;/strong>&lt;/h3>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQP05SZIegcKiaLYoSNMNVAKLiaGdaTWNg6u4bUdvcuNWcvusCkZ7wvibuA/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>distribution 负责与docker registry交互，上传洗澡镜像以及v2 registry 有关的源数据&lt;/p>
&lt;/li>
&lt;li>
&lt;p>registry负责docker registry有关的身份认证、镜像查找、镜像验证以及管理registry mirror等交互操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>image 负责与镜像源数据有关的存储、查找，镜像层的索引、查找以及镜像tar包有关的导入、导出操作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>reference负责存储本地所有镜像的repository和tag名，并维护与镜像id之间的映射关系&lt;/p>
&lt;/li>
&lt;li>
&lt;p>layer模块负责与镜像层和容器层源数据有关的增删改查，并负责将镜像层的增删改查映射到实际存储镜像层文件的graphdriver模块&lt;/p>
&lt;/li>
&lt;li>
&lt;p>graghdriver是所有与容器镜像相关操作的执行者&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="2docker架构2">&lt;strong>2、docker架构2&lt;/strong>&lt;/h3>
&lt;p>如果觉得上面架构图比较乱可以看这个架构：&lt;/p>
&lt;h3 id="httpsmmbizqpiccnmmbiz_jpgicnyeyk3vqgm9yicn3viaynt53xk9vanziqbqbbbiugyerdnepf9jkodiafzjgqg9zmxi0qy0empg0iaonaicmdy5vqq640wx_fmtjpegtpwebpwxfrom5wx_lazy1wx_co1">&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQbQbBBIUgYerDNepf9jkoDiafzJGQg9zMXI0qY0eMPg0iaoNaicMDY5VQQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/h3>
&lt;p>从上图不难看出，用户是使用Docker Client与Docker Daemon建立通信，并发送请求给后者。&lt;/p>
&lt;p>而Docker Daemon作为Docker架构中的主体部分，首先提供Server的功能使其可以接受Docker Client的请求；而后Engine执行Docker内部的一系列工作，每一项工作都是以一个Job的形式的存在。&lt;/p>
&lt;p>Job的运行过程中，当需要容器镜像时，则从Docker Registry中下载镜像，并通过镜像管理驱动graphdriver将下载镜像以Graph的形式存储；当需要为Docker创建网络环境时，通过网络管理驱动networkdriver创建并配置Docker容器网络环境；当需要限制Docker容器运行资源或执行用户指令等操作时，则通过execdriver来完成。&lt;/p>
&lt;p>而libcontainer是一项独立的容器管理包，networkdriver以及execdriver都是通过libcontainer来实现具体对容器进行的操作。当执行完运行容器的命令后，一个实际的Docker容器就处于运行状态，该容器拥有独立的文件系统，独立并且安全的运行环境等。&lt;/p>
&lt;h3 id="3docker架构3">&lt;strong>3、docker架构3&lt;/strong>&lt;/h3>
&lt;p>再来看看另外一个架构，这个个架构就简单清晰指明了server/client交互，容器和镜像、数据之间的一些联系。&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQPo5rennpNHVZaT2ceMmGrP5icFBVD94LiaP15ib8L2zHVIj0kkpyvy87A/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>这个架构图更加清晰了架构&lt;/p>
&lt;p>docker daemon就是docker的守护进程即server端，可以是远程的，也可以是本地的，这个不是C/S架构吗，客户端Docker client 是通过rest api进行通信。&lt;/p>
&lt;p>docker cli 用来管理容器和镜像，客户端提供一个只读镜像，然后通过镜像可以创建多个容器，这些容器可以只是一个RFS（Root file system根文件系统），也可以ishi一个包含了用户应用的RFS，容器再docker client中只是要给进程，两个进程之间互不可见。&lt;/p>
&lt;p>用户不能与server直接交互，但可以通过与容器这个桥梁来交互，由于是操作系统级别的虚拟技术，中间的损耗几乎可以不计。&lt;/p>
&lt;h2 id="三docker架构2各个模块的功能带完善">&lt;strong>三、docker架构2各个模块的功能（带完善）&lt;/strong>&lt;/h2>
&lt;p>主要的模块有：Docker Client、Docker Daemon、Docker Registry、Graph、Driver、libcontainer以及Docker container。　　　&lt;/p>
&lt;h3 id="heading-1">&lt;/h3>
&lt;h3 id="1docker-client">&lt;strong>1、docker client&lt;/strong>&lt;/h3>
&lt;p>docker client 是docker架构中用户用来和docker daemon建立通信的客户端，用户使用的可执行文件为docker，通过docker命令行工具可以发起众多管理container的请求。&lt;/p>
&lt;p>docker client可以通过一下三宗方式和docker daemon建立通信：tcp://host:port;unix:path_to_socket;fd://socketfd。，docker client可以通过设置命令行flag参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性&lt;/p>
&lt;p>docker client发送容器管理请求后，由docker daemon接受并处理请求，当docker client 接收到返回的请求相应并简单处理后，docker client 一次完整的生命周期就结束了，当需要继续发送容器管理请求时，用户必须再次通过docker可以执行文件创建docker client。&lt;/p>
&lt;h3 id="heading-2">&lt;/h3>
&lt;h3 id="2docker-daemon">&lt;strong>2、docker daemon&lt;/strong>&lt;/h3>
&lt;p>docker daemon 是docker架构中一个常驻在后台的系统进程，功能是：接收处理docker client发送的请求。该守护进程在后台启动一个server，server负载接受docker client发送的请求；接受请求后，server通过路由与分发调度，找到相应的handler来执行请求。&lt;/p>
&lt;p>docker daemon启动所使用的可执行文件也为docker，与docker client启动所使用的可执行文件docker相同，在docker命令执行时，通过传入的参数来判别docker daemon与docker client。&lt;/p>
&lt;p>docker daemon的架构可以分为：docker server、engine、job。daemon&lt;/p>
&lt;h3 id="heading-3">&lt;/h3>
&lt;h3 id="3docker-server">&lt;strong>3、docker server&lt;/strong>&lt;/h3>
&lt;p>docker server在docker架构中时专门服务于docker client的server，该server的功能时：接受并调度分发docker client发送的请求，架构图如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>在Docker的启动过程中，通过包gorilla/mux（golang的类库解析），创建了一个mux.Router，提供请求的路由功能。在Golang中，gorilla/mux是一个强大的URL路由器以及调度分发器。该mux.Router中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler三部分组成。&lt;/p>
&lt;p>若Docker Client通过HTTP的形式访问Docker Daemon，创建完mux.Router之后，Docker将Server的监听地址以及mux.Router作为参数，创建一个httpSrv=http.Server{}，最终执行httpSrv.Serve()为请求服务。&lt;/p>
&lt;p>在Server的服务过程中，Server在listener上接受Docker Client的访问请求，并创建一个全新的goroutine来服务该请求。在goroutine中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的Handler来处理该请求，最后Handler处理完请求之后回复该请求。&lt;/p>
&lt;p>需要注意的是：Docker Server的运行在Docker的启动过程中，是靠一个名为”serveapi”的job的运行来完成的。原则上，Docker Server的运行是众多job中的一个，但是为了强调Docker Server的重要性以及为后续job服务的重要特性，将该”serveapi”的job单独抽离出来分析，理解为Docker Server。&lt;/p>
&lt;h3 id="heading-4">&lt;/h3>
&lt;h3 id="4engine">&lt;strong>4、engine&lt;/strong>&lt;/h3>
&lt;p>Engine是Docker架构中的运行引擎，同时也Docker运行的核心模块。它扮演Docker container存储仓库的角色，并且通过执行job的方式来操纵管理这些容器。&lt;/p>
&lt;p>在Engine数据结构的设计与实现过程中，有一个handler对象。该handler对象存储的都是关于众多特定job的handler处理访问。举例说明，Engine的handler对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为”create”的job在运行时，执行的是daemon.ContainerCreate的handler。&lt;/p>
&lt;h3 id="heading-5">&lt;/h3>
&lt;h3 id="5job">&lt;strong>5、job&lt;/strong>&lt;/h3>
&lt;p>一个Job可以认为是Docker架构中Engine内部最基本的工作执行单元。Docker可以做的每一项工作，都可以抽象为一个job。例如：在容器内部运行一个进程，这是一个job；创建一个新的容器，这是一个job，从Internet上下载一个文档，这是一个job；包括之前在Docker Server部分说过的，创建Server服务于HTTP的API，这也是一个job，等等。&lt;/p>
&lt;p>Job的设计者，把Job设计得与Unix进程相仿。比如说：Job有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。&lt;/p>
&lt;h3 id="heading-6">&lt;/h3>
&lt;h3 id="6docker-registry">&lt;strong>6、docker registry&lt;/strong>&lt;/h3>
&lt;p>Docker Registry是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。&lt;/p>
&lt;p>在Docker的运行过程中，Docker Daemon会与Docker Registry通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的job名称分别为”search”，”pull” 与 “push”。&lt;/p>
&lt;p>其中，在Docker架构中，Docker可以使用公有的Docker Registry，即大家熟知的Docker Hub，如此一来，Docker获取容器镜像文件时，必须通过互联网访问Docker Hub；同时Docker也允许用户构建本地私有的Docker Registry，这样可以保证容器镜像的获取在内网完成。&lt;/p>
&lt;h3 id="heading-7">&lt;/h3>
&lt;h3 id="7graph">&lt;strong>7、Graph&lt;/strong>&lt;/h3>
&lt;p>Graph在Docker架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph存储着本地具有版本信息的文件系统镜像，另一方面也通过GraphDB记录着所有文件系统镜像彼此之间的关系。&lt;/p>
&lt;p>Graph的架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQDticrbs6fTR1ZvAvK5ufFO1MicOjiaClM6SIXpA2EiaFqhBvjlnzdhb4YQ/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>其中，GraphDB是一个构建在SQLite之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录。它仅仅实现了大多数图数据库所拥有的一个小的子集，但是提供了简单的接口表示节点之间的关系。&lt;/p>
&lt;p>同时在Graph的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体rootfs。&lt;/p>
&lt;p>&lt;strong>8、driver&lt;/strong>&lt;/p>
&lt;p>Driver是Docker架构中的驱动模块。通过Driver驱动，Docker可以实现对Docker容器执行环境的定制。由于Docker运行的生命周期中，并非用户所有的操作都是针对Docker容器的管理，另外还有关于Docker运行信息的获取，Graph的存储与记录等。因此，为了将Docker容器的管理从Docker Daemon内部业务逻辑中区分开来，设计了Driver层驱动来接管所有这部分请求。&lt;/p>
&lt;p>在Docker Driver的实现中，可以分为以下三类驱动：graphdriver、networkdriver和execdriver。&lt;/p>
&lt;p>graphdriver主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的rootfs时，graphdriver从本地镜像存储目录中获取指定的容器镜像。&lt;/p>
&lt;p>在graphdriver的初始化过程之前，有4种文件系统或类文件系统在其内部注册，它们分别是aufs、btrfs、vfs和devmapper。而Docker在初始化之时，通过获取系统环境变量”DOCKER_DRIVER”来提取所使用driver的指定类型。而之后所有的graph操作，都使用该driver来执行。&lt;/p>
&lt;p>graphdriver的架构如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>networkdriver的用途是完成Docker容器网络环境的配置，其中包括Docker启动时为Docker环境创建网桥；Docker容器创建时为其创建专属虚拟网卡设备；以及为Docker容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。networkdriver的架构如下：&lt;/p>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;p>execdriver作为Docker容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在execdriver的实现过程中，原先可以使用LXC驱动调用LXC的接口，来操纵容器的配置以及生命周期，而现在execdriver默认使用native驱动，不依赖于LXC。具体体现在Daemon启动过程中加载的ExecDriverflag参数，该参数在配置文件已经被设为”native”。这可以认为是Docker在1.2版本上一个很大的改变，或者说Docker实现跨平台的一个先兆。execdriver架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQ2mENqgVLbqicwOZs40ibSm9FbCu5GKiaM7VicliaNWKKH1WYm4ibfkh8V98w/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;h3 id="9libcontainer">&lt;strong>9、libcontainer&lt;/strong>&lt;/h3>
&lt;p>libcontainer是Docker架构中一个使用Go语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的API。&lt;/p>
&lt;p>正是由于libcontainer的存在，Docker可以直接调用libcontainer，而最终操纵容器的namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。libcontainer架构如下：&lt;/p>
&lt;p>&lt;img src="https://mmbiz.qpic.cn/mmbiz_jpg/icNyEYk3VqGm9yicn3ViaynT53XK9VanZIQgial97GT7cIBa4wxBXwhKicjoH7PnjMx6bNFtavDz6jSjofmwEYRw9TA/640?wx_fmt=jpeg&amp;amp;tp=webp&amp;amp;wxfrom=5&amp;amp;wx_lazy=1&amp;amp;wx_co=1" alt="">&lt;/p>
&lt;p>另外，libcontainer提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer屏蔽了Docker上层对容器的直接管理。又由于libcontainer使用Go这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，因此很难说，未来的Docker就一定会紧紧地和Linux捆绑在一起。而于此同时，Microsoft在其著名云计算平台Azure中，也添加了对Docker的支持，可见Docker的开放程度与业界的火热度。&lt;/p>
&lt;p>暂不谈Docker，由于libcontainer的功能以及其本身与系统的松耦合特性，很有可能会在其他以容器为原型的平台出现，同时也很有可能催生出云计算领域全新的项目。&lt;/p>
&lt;h3 id="heading-8">&lt;/h3>
&lt;h3 id="10docker-container">&lt;strong>10、docker container&lt;/strong>&lt;/h3>
&lt;p>Docker container（Docker容器）是Docker架构中服务交付的最终体现形式。&lt;/p>
&lt;p>Docker按照用户的需求与指令，订制相应的Docker容器：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>用户通过指定容器镜像，使得Docker容器可以自定义rootfs等文件系统；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过指定计算资源的配额，使得Docker容器使用指定的计算资源；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过配置网络及其安全策略，使得Docker容器拥有独立且安全的网络环境；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>用户通过指定运行的命令，使得Docker容器执行指定的工作。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="">&lt;/p>
&lt;h2 id="四docker简单使用">&lt;strong>四、docker简单使用&lt;/strong>&lt;/h2>
&lt;h3 id="1安装">&lt;strong>1、安装&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">yum install docker -ysystemctl enable dockersystemctl start docker
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>注意：启动前应当设置源&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">vim /usr/lib/systemd/system/docker.service
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>2、docker版本查询&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 ~]# docker version
Client:Version: 1.13.1
API version: 1.26
Package version: docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64
Go version: go1.10.3
Git commit: b2f74b2/1.13.1
Built: Wed May 1 14:55:20 2019OS/
Arch: linux/amd64
Server:Version: 1.13.1
API version: 1.26 (minimum version 1.12)
Package version: docker-1.13.1-96.gitb2f74b2.el7.centos.x86_64
Go version: go1.10.3
Git commit: b2f74b2/1.13.1
Built: Wed May 1 14:55:20 2019OS/
Arch: linux/amd64
Experimental: false
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>3、搜索下载镜像&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker pull alpine    #下载镜像
docker search nginx   #查看镜像docker pull nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="4查看已经下载的镜像">&lt;strong>4、查看已经下载的镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 ~]# docker images
REPOSITORY         TAG      IMAGE  ID     CREATED      SIZEzxg/my_nginx      v1       b164f4c07c64  8 days ago    126 MBzxg/my_nginx      latest   f07837869dfc  8 days ago    126 MBdocker.io/nginx   latest   e445ab08b2be  2 weeks ago   126 MBdocker.io/alpine  latest   b7b28af77ffe  3 weeks ago   5.58 MBdocker.io/centos  latest   9f38484d220f  4 months ago  202 MB
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="5导出镜像">&lt;strong>5、导出镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker save nginx &amp;gt;/tmp/nginx.tar.gz
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="6删除镜像">&lt;strong>6、删除镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker rmi -f nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="7导入镜像">&lt;strong>7、导入镜像&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">docker load &amp;lt;/tmp/nginx.tar.gz
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="8默认配置文件">&lt;strong>8、默认配置文件&lt;/strong>&lt;/h3>
&lt;p>vim /usr/lib/systemd/system/docker.service &lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[Unit]
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果更改存储目录就添加　　&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">--graph=/opt/docker
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果更改DNS——默认采用宿主机的dns&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">--dns=xxxx的方式指定
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="heading-9">&lt;/h3>
&lt;h3 id="9运行hello-world">&lt;strong>9、运行hello world&lt;/strong>&lt;/h3>
&lt;p>这里用centos镜像echo一个hello word&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker images
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="10运行一个容器-run">&lt;strong>10、运行一个容器-run&lt;/strong>&lt;/h3>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it alpine sh   #运行并进入alpine
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>后台运行（-d后台运行）（&amp;ndash;name添加一个名字）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it -d --name test1 alpine
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>还有一种-rm参数，ctrl+c后就删除，可以测试环境用，生成环境用的少&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker run -it --rm --name centos nginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="11如何进入容器">&lt;strong>11、如何进入容器&lt;/strong>&lt;/h3>
&lt;p>三种方法，上面已经演示了一种&lt;/p>
&lt;p>第一种，需要容器本身的pid及util-linux，不推荐，暂时不演示了&lt;/p>
&lt;p>第二种，不分配bash终端的一种实施操作，不推荐，这种操作如果在开一个窗口也能看到操作的指令，所有人都能看到&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker ps
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第三种：exec方式，终端时分开的，推荐&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">[root@web1 overlay2]# docker exec -it mynginx sh
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="12查看docker进程及删除容器">&lt;strong>12、查看docker进程及删除容器&lt;/strong>&lt;/h3>
&lt;p>上面已经演示：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ docker ps
$ docker ps -a 　　　　　　&lt;span class="c1">#-a :显示所有的容器，包括未运行的&lt;/span>
&lt;span class="c1"># 查看容器详细信息&lt;/span>
$ docker inspect mynginx
&lt;span class="c1"># 查看日志**&lt;/span>
$ docker logs -f mynginx
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://www.cnblogs.com/zhangxingeng/p/11236968.html">Docker1 架构原理及简单使用 - 乐章 - 博客园&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.yuanshuli.com/post-64.html">【解决】docker启动报错：Running modprobe xt_conntrack failed with message: `modprobe: ERROR: could not insert &amp;lsquo;xt_conntrack&amp;rsquo;&amp;hellip;&amp;hellip; - 碧海长天&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch内核解析 - 查询篇</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E8%AF%BB%E6%B5%81%E7%A8%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E8%AF%BB%E6%B5%81%E7%A8%8B/</guid><description>&lt;h1 id="elasticsearch内核解析---查询篇">Elasticsearch内核解析 - 查询篇&lt;/h1>
&lt;h2 id="读操作">读操作&lt;/h2>
&lt;p>实时性和《&lt;a href="https://zhuanlan.zhihu.com/p/34669354">Elasticsearch内核解析 - 写入篇&lt;/a>》中的“写操作”一样，对于搜索而言是近实时的，延迟在100ms以上，对于NoSQL则需要是实时的。&lt;/p>
&lt;p>一致性指的是写入成功后，下次读操作一定要能读取到最新的数据。对于搜索，这个要求会低一些，可以有一些延迟。但是对于NoSQL数据库，则一般要求最好是强一致性的。&lt;/p>
&lt;p>结果匹配上，NoSQL作为数据库，查询过程中只有符合不符合两种情况，而搜索里面还有是否相关，类似于NoSQL的结果只能是0或1，而搜索里面可能会有0.1，0.5，0.9等部分匹配或者更相关的情况。&lt;/p>
&lt;p>结果召回上，搜索一般只需要召回最满足条件的Top N结果即可，而NoSQL一般都需要返回满足条件的所有结果。&lt;/p>
&lt;p>搜索系统一般都是两阶段查询，第一个阶段查询到对应的Doc ID，也就是PK；第二阶段再通过Doc ID去查询完整文档，而NoSQL数据库一般是一阶段就返回结果。在Elasticsearch中两种都支持。&lt;/p>
&lt;p>目前NoSQL的查询，聚合、分析和统计等功能上都是要比搜索弱的。&lt;/p>
&lt;h2 id="lucene的读">Lucene的读&lt;/h2>
&lt;p>Elasticsearch使用了Lucene作为搜索引擎库，通过Lucene完成特定字段的搜索等功能，在Lucene中这个功能是通过IndexSearcher的下列接口实现的：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="n">TopDocs&lt;/span> &lt;span class="nf">search&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Query&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Document&lt;/span> &lt;span class="nf">doc&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">docID&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="nf">count&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Query&lt;/span> &lt;span class="n">query&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">......(&lt;/span>&lt;span class="n">其他&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第一个search接口实现搜索功能，返回最满足Query的N个结果；第二个doc接口通过doc id查询Doc内容；第三个count接口通过Query获取到命中数。&lt;/p>
&lt;p>这三个功能是搜索中的最基本的三个功能点，对于大部分Elasticsearch中的查询都是比较复杂的，直接用这个接口是无法满足需求的，比如分布式问题。这些问题都留给了Elasticsearch解决，我们接下来看Elasticsearch中相关读功能的剖析。&lt;/p>
&lt;h2 id="elasticsearch的读">Elasticsearch的读&lt;/h2>
&lt;p>Elasticsearch中每个Shard都会有多个Replica，主要是为了保证数据可靠性，除此之外，还可以增加读能力，因为写的时候虽然要写大部分Replica Shard，但是查询的时候只需要查询Primary和Replica中的任何一个就可以了。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-1ad1351408bdf0ce7f76f251d6ef8bc4_720w.jpg" alt="">&lt;/p>
&lt;p>Search On Replicas&lt;/p>
&lt;p>在上图中，该Shard有1个Primary和2个Replica Node，当查询的时候，从三个节点中根据Request中的preference参数选择一个节点查询。preference可以设置_local，_primary，_replica以及其他选项。如果选择了primary，则每次查询都是直接查询Primary，可以保证每次查询都是最新的。如果设置了其他参数，那么可能会查询到R1或者R2，这时候就有可能查询不到最新的数据。&lt;/p>
&lt;blockquote>
&lt;p>上述代码逻辑在OperationRouting.Java的searchShards方法中。&lt;/p>
&lt;/blockquote>
&lt;p>接下来看一下，Elasticsearch中的查询是如何支持分布式的。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-737f6cb48ccf22c50c2e630433c6ad48_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中通过分区实现分布式，数据写入的时候根据_routing规则将数据写入某一个Shard中，这样就能将海量数据分布在多个Shard以及多台机器上，已达到分布式的目标。这样就导致了查询的时候，潜在数据会在当前index的所有的Shard中，所以Elasticsearch查询的时候需要查询所有Shard，同一个Shard的Primary和Replica选择一个即可，查询请求会分发给所有Shard，每个Shard中都是一个独立的查询引擎，比如需要返回Top 10的结果，那么每个Shard都会查询并且返回Top 10的结果，然后在Client Node里面会接收所有Shard的结果，然后通过优先级队列二次排序，选择出Top 10的结果返回给用户。&lt;/p>
&lt;p>这里有一个问题就是请求膨胀，用户的一个搜索请求在Elasticsearch内部会变成Shard个请求，这里有个优化点，虽然是Shard个请求，但是这个Shard个数不一定要是当前Index中的Shard个数，只要是当前查询相关的Shard即可，这个需要基于业务和请求内容优化，通过这种方式可以优化请求膨胀数。&lt;/p>
&lt;p>Elasticsearch中的查询主要分为两类，Get请求：通过ID查询特定Doc；Search请求：通过Query查询匹配Doc。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-1f4c1cf921049b841ae2612b4734cdb1_720w.jpg" alt="">&lt;/p>
&lt;blockquote>
&lt;p>上图中内存中的Segment是指刚Refresh Segment，但是还没持久化到磁盘的新Segment，而非从磁盘加载到内存中的Segment。&lt;/p>
&lt;/blockquote>
&lt;p>对于Search类请求，查询的时候是一起查询内存和磁盘上的Segment，最后将结果合并后返回。这种查询是近实时（Near Real Time）的，主要是由于内存中的Index数据需要一段时间后才会刷新为Segment。&lt;/p>
&lt;p>对于Get类请求，查询的时候是先查询内存中的TransLog，如果找到就立即返回，如果没找到再查询磁盘上的TransLog，如果还没有则再去查询磁盘上的Segment。这种查询是实时（Real Time）的。这种查询顺序可以保证查询到的Doc是最新版本的Doc，这个功能也是为了保证NoSQL场景下的实时性要求。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-c5455432442548d1f12c975684fc4a00_720w.jpg" alt="">&lt;/p>
&lt;p>多阶段查询&lt;/p>
&lt;p>所有的搜索系统一般都是两阶段查询，第一阶段查询到匹配的DocID，第二阶段再查询DocID对应的完整文档，这种在Elasticsearch中称为query_then_fetch，还有一种是一阶段查询的时候就返回完整Doc，在Elasticsearch中称作query_and_fetch，一般第二种适用于只需要查询一个Shard的请求。&lt;/p>
&lt;p>除了一阶段，两阶段外，还有一种三阶段查询的情况。搜索里面有一种算分逻辑是根据TF（Term Frequency）和DF（Document Frequency）计算基础分，但是Elasticsearch中查询的时候，是在每个Shard中独立查询的，每个Shard中的TF和DF也是独立的，虽然在写入的时候通过_routing保证Doc分布均匀，但是没法保证TF和DF均匀，那么就有会导致局部的TF和DF不准的情况出现，这个时候基于TF、DF的算分就不准。为了解决这个问题，Elasticsearch中引入了DFS查询，比如DFS_query_then_fetch，会先收集所有Shard中的TF和DF值，然后将这些值带入请求中，再次执行query_then_fetch，这样算分的时候TF和DF就是准确的，类似的有DFS_query_and_fetch。这种查询的优势是算分更加精准，但是效率会变差。另一种选择是用BM25代替TF/DF模型。&lt;/p>
&lt;p>在新版本Elasticsearch中，用户没法指定DFS_query_and_fetch和query_and_fetch，这两种只能被Elasticsearch系统改写。&lt;/p>
&lt;h2 id="elasticsearch查询流程">Elasticsearch查询流程&lt;/h2>
&lt;p>Elasticsearch中的大部分查询，以及核心功能都是Search类型查询，上面我们了解到查询分为一阶段，二阶段和三阶段，这里我们就以最常见的的二阶段查询为例来介绍查询流程。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-10acab5576a2359ca279331e81adc1e2_720w.jpg" alt="">&lt;/p>
&lt;p>查询流程&lt;/p>
&lt;p>&lt;strong>注册Action&lt;/strong>&lt;/p>
&lt;p>Elasticsearch中，查询和写操作一样都是在ActionModule.java中注册入口处理函数的。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">registerHandler.accept(new RestSearchAction(settings, restController));
......
actions.register(SearchAction.INSTANCE, TransportSearchAction.class);
......
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如果请求是Rest请求，则会在RestSearchAction中解析请求，检查查询类型，不能设置为dfs_query_and_fetch或者query_and_fetch，这两个目前只能用于Elasticsearch中的优化场景，然后将请求发给后面的TransportSearchAction处理。然后构造SearchRequest，将请求发送给TransportSearchAction处理。&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-02fdc9375a9d6af62ac6c2035b5a9730_720w.jpg" alt="">&lt;/p>
&lt;p>如果是第一阶段的Query Phase请求，则会调用SearchService的executeQueryPhase方法。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-be3a37c7fe08d4f1559b9eb0aaa8d37a_720w.jpg" alt="">&lt;/p>
&lt;p>如果是第二阶段的Fetch Phase请求，则会调用SearchService的executeFetchPhase方法。&lt;/p>
&lt;h2 id="client-node">&lt;strong>Client Node&lt;/strong>&lt;/h2>
&lt;p>Client Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。&lt;/p>
&lt;p>&lt;strong>1. Get Remove Cluster Shard&lt;/strong>&lt;/p>
&lt;p>判断是否需要跨集群访问，如果需要，则获取到要访问的Shard列表。&lt;/p>
&lt;p>&lt;strong>2. Get Search Shard Iterator&lt;/strong>&lt;/p>
&lt;p>获取当前Cluster中要访问的Shard，和上一步中的Remove Cluster Shard合并，构建出最终要访问的完整Shard列表。&lt;/p>
&lt;p>这一步中，会根据Request请求中的参数从Primary Node和多个Replica Node中选择出一个要访问的Shard。&lt;/p>
&lt;p>&lt;strong>3. For Every Shard:Perform&lt;/strong>&lt;/p>
&lt;p>遍历每个Shard，对每个Shard执行后面逻辑。&lt;/p>
&lt;p>&lt;strong>4. Send Request To Query Shard&lt;/strong>&lt;/p>
&lt;p>将查询阶段请求发送给相应的Shard。&lt;/p>
&lt;p>&lt;strong>5. Merge Docs&lt;/strong>&lt;/p>
&lt;p>上一步将请求发送给多个Shard后，这一步就是异步等待返回结果，然后对结果合并。这里的合并策略是维护一个Top N大小的优先级队列，每当收到一个shard的返回，就把结果放入优先级队列做一次排序，直到所有的Shard都返回。&lt;/p>
&lt;p>翻页逻辑也是在这里，如果需要取Top 30~ Top 40的结果，这个的意思是所有Shard查询结果中的第30到40的结果，那么在每个Shard中无法确定最终的结果，每个Shard需要返回Top 40的结果给Client Node，然后Client Node中在merge docs的时候，计算出Top 40的结果，最后再去除掉Top 30，剩余的10个结果就是需要的Top 30~ Top 40的结果。&lt;/p>
&lt;p>上述翻页逻辑有一个明显的缺点就是每次Shard返回的数据中包括了已经翻过的历史结果，如果翻页很深，则在这里需要排序的Docs会很多，比如Shard有1000，取第9990到10000的结果，那么这次查询，Shard总共需要返回1000 * 10000，也就是一千万Doc，这种情况很容易导致OOM。&lt;/p>
&lt;p>另一种翻页方式是使用search_after，这种方式会更轻量级，如果每次只需要返回10条结构，则每个Shard只需要返回search_after之后的10个结果即可，返回的总数据量只是和Shard个数以及本次需要的个数有关，和历史已读取的个数无关。这种方式更安全一些，推荐使用这种。&lt;/p>
&lt;p>如果有aggregate，也会在这里做聚合，但是不同的aggregate类型的merge策略不一样，具体的可以在后面的aggregate文章中再介绍。&lt;/p>
&lt;p>&lt;strong>6. Send Request To Fetch Shard&lt;/strong>&lt;/p>
&lt;p>选出Top N个Doc ID后发送给这些Doc ID所在的Shard执行Fetch Phase，最后会返回Top N的Doc的内容。&lt;/p>
&lt;h2 id="query-phase">Query Phase&lt;/h2>
&lt;p>接下来我们看第一阶段查询的步骤：&lt;/p>
&lt;p>&lt;strong>1. Create Search Context&lt;/strong>&lt;/p>
&lt;p>创建Search Context，之后Search过程中的所有中间状态都会存在Context中，这些状态总共有50多个，具体可以查看DefaultSearchContext或者其他SearchContext的子类。&lt;/p>
&lt;p>&lt;strong>2. Parse Query&lt;/strong>&lt;/p>
&lt;p>解析Query的Source，将结果存入Search Context。这里会根据请求中Query类型的不同创建不同的Query对象，比如TermQuery、FuzzyQuery等，最终真正执行TermQuery、FuzzyQuery等语义的地方是在Lucene中。&lt;/p>
&lt;p>这里包括了dfsPhase、queryPhase和fetchPhase三个阶段的preProcess部分，只有queryPhase的preProcess中有执行逻辑，其他两个都是空逻辑，执行完preProcess后，所有需要的参数都会设置完成。&lt;/p>
&lt;p>由于Elasticsearch中有些请求之间是相互关联的，并非独立的，比如scroll请求，所以这里同时会设置Context的生命周期。&lt;/p>
&lt;p>同时会设置lowLevelCancellation是否打开，这个参数是集群级别配置，同时也能动态开关，打开后会在后面执行时做更多的检测，检测是否需要停止后续逻辑直接返回。&lt;/p>
&lt;p>&lt;strong>3. Get From Cache&lt;/strong>&lt;/p>
&lt;p>判断请求是否允许被Cache，如果允许，则检查Cache中是否已经有结果，如果有则直接读取Cache，如果没有则继续执行后续步骤，执行完后，再将结果加入Cache。&lt;/p>
&lt;p>&lt;strong>4. Add Collectors&lt;/strong>&lt;/p>
&lt;p>Collector主要目标是收集查询结果，实现排序，对自定义结果集过滤和收集等。这一步会增加多个Collectors，多个Collector组成一个List。&lt;/p>
&lt;ol>
&lt;li>FilteredCollector*：*先判断请求中是否有Post Filter，Post Filter用于Search，Agg等结束后再次对结果做Filter，希望Filter不影响Agg结果。如果有Post Filter则创建一个FilteredCollector，加入Collector List中。&lt;/li>
&lt;li>PluginInMultiCollector：判断请求中是否制定了自定义的一些Collector，如果有，则创建后加入Collector List。&lt;/li>
&lt;li>MinimumScoreCollector：判断请求中是否制定了最小分数阈值，如果指定了，则创建MinimumScoreCollector加入Collector List中，在后续收集结果时，会过滤掉得分小于最小分数的Doc。&lt;/li>
&lt;li>EarlyTerminatingCollector：判断请求中是否提前结束Doc的Seek，如果是则创建EarlyTerminatingCollector，加入Collector List中。在后续Seek和收集Doc的过程中，当Seek的Doc数达到Early Terminating后会停止Seek后续倒排链。&lt;/li>
&lt;li>CancellableCollector：判断当前操作是否可以被中断结束，比如是否已经超时等，如果是会抛出一个TaskCancelledException异常。该功能一般用来提前结束较长的查询请求，可以用来保护系统。&lt;/li>
&lt;li>EarlyTerminatingSortingCollector：如果Index是排序的，那么可以提前结束对倒排链的Seek，相当于在一个排序递减链表上返回最大的N个值，只需要直接返回前N个值就可以了。这个Collector会加到Collector List的头部。EarlyTerminatingSorting和EarlyTerminating的区别是，EarlyTerminatingSorting是一种对结果无损伤的优化，而EarlyTerminating是有损的，人为掐断执行的优化。&lt;/li>
&lt;li>TopDocsCollector：这个是最核心的Top N结果选择器，会加入到Collector List的头部。TopScoreDocCollector和TopFieldCollector都是TopDocsCollector的子类，TopScoreDocCollector会按照固定的方式算分，排序会按照分数+doc id的方式排列，如果多个doc的分数一样，先选择doc id小的文档。而TopFieldCollector则是根据用户指定的Field的值排序。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>5. lucene::search&lt;/strong>&lt;/p>
&lt;p>这一步会调用Lucene中IndexSearch的search接口，执行真正的搜索逻辑。每个Shard中会有多个Segment，每个Segment对应一个LeafReaderContext，这里会遍历每个Segment，到每个Segment中去Search结果，然后计算分数。&lt;/p>
&lt;p>搜索里面一般有两阶段算分，第一阶段是在这里算的，会对每个Seek到的Doc都计算分数，为了减少CPU消耗，一般是算一个基本分数。这一阶段完成后，会有个排序。然后在第二阶段，再对Top 的结果做一次二阶段算分，在二阶段算分的时候会考虑更多的因子。二阶段算分在后续操作中。&lt;/p>
&lt;p>具体请求，比如TermQuery、WildcardQuery的查询逻辑都在Lucene中，后面会有专门文章介绍。&lt;/p>
&lt;p>&lt;strong>6. rescore&lt;/strong>&lt;/p>
&lt;p>根据Request中是否包含rescore配置决定是否进行二阶段排序，如果有则执行二阶段算分逻辑，会考虑更多的算分因子。二阶段算分也是一种计算机中常见的多层设计，是一种资源消耗和效率的折中。&lt;/p>
&lt;p>Elasticsearch中支持配置多个Rescore，这些rescore逻辑会顺序遍历执行。每个rescore内部会先按照请求参数window选择出Top window的doc，然后对这些doc排序，排完后再合并回原有的Top 结果顺序中。&lt;/p>
&lt;p>&lt;strong>7. suggest::execute()&lt;/strong>&lt;/p>
&lt;p>如果有推荐请求，则在这里执行推荐请求。如果请求中只包含了推荐的部分，则很多地方可以优化。推荐不是今天的重点，这里就不介绍了，后面有机会再介绍。&lt;/p>
&lt;p>&lt;strong>8. aggregation::execute()&lt;/strong>&lt;/p>
&lt;p>如果含有聚合统计请求，则在这里执行。Elasticsearch中的aggregate的处理逻辑也类似于Search，通过多个Collector来实现。在Client Node中也需要对aggregation做合并。aggregate逻辑更复杂一些，就不在这里赘述了，后面有需要就再单独开文章介绍。&lt;/p>
&lt;p>上述逻辑都执行完成后，如果当前查询请求只需要查询一个Shard，那么会直接在当前Node执行Fetch Phase。&lt;/p>
&lt;h2 id="fetch-phase">Fetch Phase&lt;/h2>
&lt;p>Elasticsearch作为搜索系统时，或者任何搜索系统中，除了Query阶段外，还会有一个Fetch阶段，这个Fetch阶段在数据库类系统中是没有的，是搜索系统中额外增加的阶段。搜索系统中额外增加Fetch阶段的原因是搜索系统中数据分布导致的，在搜索中，数据通过routing分Shard的时候，只能根据一个主字段值来决定，但是查询的时候可能会根据其他非主字段查询，那么这个时候所有Shard中都可能会存在相同非主字段值的Doc，所以需要查询所有Shard才能不会出现结果遗漏。同时如果查询主字段，那么这个时候就能直接定位到Shard，就只需要查询特定Shard即可，这个时候就类似于数据库系统了。另外，数据库中的二级索引又是另外一种情况，但类似于查主字段的情况，这里就不多说了。&lt;/p>
&lt;p>基于上述原因，第一阶段查询的时候并不知道最终结果会在哪个Shard上，所以每个Shard中管都需要查询完整结果，比如需要Top 10，那么每个Shard都需要查询当前Shard的所有数据，找出当前Shard的Top 10，然后返回给Client Node。如果有100个Shard，那么就需要返回100 * 10 = 1000个结果，而Fetch Doc内容的操作比较耗费IO和CPU，如果在第一阶段就Fetch Doc，那么这个资源开销就会非常大。所以，一般是当Client Node选择出最终Top N的结果后，再对最终的Top N读取Doc内容。通过增加一点网络开销而避免大量IO和CPU操作，这个折中是非常划算的。&lt;/p>
&lt;p>Fetch阶段的目的是通过DocID获取到用户需要的完整Doc内容。这些内容包括了DocValues，Store，Source，Script和Highlight等，具体的功能点是在SearchModule中注册的，系统默认注册的有：&lt;/p>
&lt;ul>
&lt;li>ExplainFetchSubPhase&lt;/li>
&lt;li>DocValueFieldsFetchSubPhase&lt;/li>
&lt;li>ScriptFieldsFetchSubPhase&lt;/li>
&lt;li>FetchSourceSubPhase&lt;/li>
&lt;li>VersionFetchSubPhase&lt;/li>
&lt;li>MatchedQueriesFetchSubPhase&lt;/li>
&lt;li>HighlightPhase&lt;/li>
&lt;li>ParentFieldSubFetchPhase&lt;/li>
&lt;/ul>
&lt;p>除了系统默认的8种外，还有通过插件的形式注册自定义的功能，这些SubPhase中最重要的是Source和Highlight，Source是加载原文，Highlight是计算高亮显示的内容片断。&lt;/p>
&lt;p>上述多个SubPhase会针对每个Doc顺序执行，可能会产生多次的随机IO，这里会有一些优化方案，但是都是针对特定场景的，不具有通用性。&lt;/p>
&lt;p>Fetch Phase执行完后，整个查询流程就结束了。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Elasticsearch中的查询流程比较简单，更多的查询原理都在Lucene中，后续我们会有针对不同请求的Lucene原理介绍性文章。&lt;/p></description></item><item><title>Elasticsearch写流程</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</guid><description>&lt;h1 id="elasticsearch写流程">Elasticsearch写流程&lt;/h1>
&lt;h2 id="lucene的写操作及其问题">lucene的写操作及其问题&lt;/h2>
&lt;p>Elasticsearch底层使用Lucene来实现doc的读写操作，Lucene通过&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">addDocument&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">deleteDocuments&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">updateDocument&lt;/span>&lt;span class="o">(...);&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>三个方法来实现文档的写入，更新和删除操作。但是存在如下问题&lt;/p>
&lt;ol>
&lt;li>&lt;strong>没有并发设计&lt;/strong>。 lucene只是一个搜索引擎库，并没有涉及到分布式相关的设计，因此要想使用Lucene来处理海量数据，并利用分布式的能力，就必须在其之上进行分布式的相关设计。&lt;/li>
&lt;li>&lt;strong>非实时&lt;/strong>。 将文件写入lucence后并不能立即被检索，需要等待lucene生成一个完整的segment才能被检索&lt;/li>
&lt;li>&lt;strong>数据存储不可靠&lt;/strong>。 写入lucene的数据不会立即被持久化到磁盘，如果服务器宕机，那存储在内存中的数据将会丢失&lt;/li>
&lt;li>&lt;strong>不支持部分更新&lt;/strong> 。lucene中提供的updateDocuments仅支持对文档的全量更新，对部分更新不支持&lt;/li>
&lt;/ol>
&lt;h2 id="2-elasticsearch的写入方案">2. Elasticsearch的写入方案&lt;/h2>
&lt;p>针对Lucene的问题，ES做了如下设计&lt;/p>
&lt;h3 id="21-分布式设计">2.1 分布式设计：&lt;/h3>
&lt;p>为了支持对海量数据的存储和查询，Elasticsearch引入分片的概念，一个索引被分成多个分片，每个分片可以有一个主分片和多个副本分片，每个分片副本都是一个具有完整功能的lucene实例。分片可以分配在不同的服务器上，同一个分片的不同副本不能分配在相同的服务器上。&lt;/p>
&lt;p>在进行写操作时，ES会根据传入的_routing参数（或mapping中设置的_routing, 如果参数和设置中都没有则默认使用_id), 按照公式 &lt;code>shard_num=hash(\routing)%num_primary_shards&lt;/code>,计算出文档要分配到的分片，在从集群元数据中找出对应主分片的位置，将请求路由到该分片进行文档写操作。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-8ce7b65ede7b511a2d6d02530ed501d6_720w.jpg" alt="">&lt;/p>
&lt;h3 id="22-近实时性-refresh操作">2.2 近实时性-refresh操作&lt;/h3>
&lt;p>当一个文档写入Lucene后是不能被立即查询到的，Elasticsearch提供了一个refresh操作，会定时地调用lucene的reopen(新版本为openIfChanged)为内存中新写入的数据生成一个新的segment，此时被处理的文档均可以被检索到。refresh操作的时间间隔由 &lt;code>refresh_interval&lt;/code>参数控制，默认为1s, 当然还可以在写入请求中带上refresh表示写入后立即refresh，另外还可以调用refresh API显式refresh。&lt;/p>
&lt;h3 id="23-数据存储可靠性">2.3 数据存储可靠性&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>引入translog&lt;/strong> 当一个文档写入Lucence后是存储在内存中的，即使执行了refresh操作仍然是在文件系统缓存中，如果此时服务器宕机，那么这部分数据将会丢失。为此ES增加了translog， 当进行文档写操作时会先将文档写入Lucene，然后写入一份到translog，写入translog是落盘的(如果对可靠性要求不是很高，也可以设置异步落盘，可以提高性能，由配置 &lt;code>index.translog.durability&lt;/code>和 &lt;code>index.translog.sync_interval&lt;/code>控制)，这样就可以防止服务器宕机后数据的丢失。由于translog是追加写入，因此性能要比随机写入要好。与传统的分布式系统不同，这里是先写入Lucene再写入translog，原因是写入Lucene可能会失败，为了减少写入失败回滚的复杂度，因此先写入Lucene.&lt;/li>
&lt;li>&lt;strong>flush操作&lt;/strong> 另外每30分钟或当translog达到一定大小(由 &lt;code>index.translog.flush_threshold_size&lt;/code>控制，默认512mb), ES会触发一次flush操作，此时ES会先执行refresh操作将buffer中的数据生成segment，然后调用lucene的commit方法将所有内存中的segment fsync到磁盘。此时lucene中的数据就完成了持久化，会清空translog中的数据(6.x版本为了实现sequenceIDs,不删除translog)&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-9418f89986e3a141b3acf83cf158885a_720w.jpg" alt="">&lt;/p>
&lt;ol>
&lt;li>&lt;strong>merge操作&lt;/strong> 由于refresh默认间隔为1s中，因此会产生大量的小segment，为此ES会运行一个任务检测当前磁盘中的segment，对符合条件的segment进行合并操作，减少lucene中的segment个数，提高查询速度，降低负载。不仅如此，merge过程也是文档删除和更新操作后，旧的doc真正被删除的时候。用户还可以手动调用_forcemerge API来主动触发merge，以减少集群的segment个数和清理已删除或更新的文档。&lt;/li>
&lt;li>&lt;strong>多副本机制&lt;/strong> 另外ES有多副本机制，一个分片的主副分片不能分片在同一个节点上，进一步保证数据的可靠性。&lt;/li>
&lt;/ol>
&lt;h3 id="24-部分更新">2.4 部分更新&lt;/h3>
&lt;p>lucene仅支持对文档的整体更新，ES为了支持局部更新，在Lucene的Store索引中存储了一个_source字段，该字段的key值是文档ID， 内容是文档的原文。当进行更新操作时先从_source中获取原文，与更新部分合并后，再调用lucene API进行全量更新， 对于写入了ES但是还没有refresh的文档，可以从translog中获取。另外为了防止读取文档过程后执行更新前有其他线程修改了文档，ES增加了版本机制，当执行更新操作时发现当前文档的版本与预期不符，则会重新获取文档再更新。&lt;/p>
&lt;h2 id="3-es的写入流程">3. ES的写入流程&lt;/h2>
&lt;p>ES的任意节点都可以作为协调节点(coordinating node)接受请求，当协调节点接受到请求后进行一系列处理，然后通过_routing字段找到对应的primary shard，并将请求转发给primary shard, primary shard完成写入后，将写入并发发送给各replica， raplica执行写入操作后返回给primary shard， primary shard再将请求返回给协调节点。大致流程如下图：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-b0b0c96aaeadfde4da0685dae4b9908f_720w.jpg" alt="">&lt;/p>
&lt;h3 id="31-coordinating节点">3.1 coordinating节点&lt;/h3>
&lt;p>ES中接收并转发请求的节点称为coordinating节点，ES中所有节点都可以接受并转发请求。当一个节点接受到写请求或更新请求后，会执行如下操作：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>ingest pipeline&lt;/strong> 查看该请求是否符合某个ingest pipeline的pattern, 如果符合则执行pipeline中的逻辑，一般是对文档进行各种预处理，如格式调整，增加字段等。如果当前节点没有ingest角色，则需要将请求转发给有ingest角色的节点执行。&lt;/li>
&lt;li>&lt;strong>自动创建索引&lt;/strong> 判断索引是否存在，如果开启了自动创建则自动创建，否则报错&lt;/li>
&lt;li>&lt;strong>设置routing&lt;/strong> 获取请求URL或mapping中的_routing，如果没有则使用_id, 如果没有指定_id则ES会自动生成一个全局唯一ID。该_routing字段用于决定文档分配在索引的哪个shard上。&lt;/li>
&lt;li>&lt;strong>构建BulkShardRequest&lt;/strong> 由于Bulk Request中包含多种(Index/Update/Delete)请求，这些请求分别需要到不同的shard上执行，因此协调节点，会将请求按照shard分开，同一个shard上的请求聚合到一起，构建BulkShardRequest&lt;/li>
&lt;li>&lt;strong>将请求发送给primary shard&lt;/strong> 因为当前执行的是写操作，因此只能在primary上完成，所以需要把请求路由到primary shard所在节点&lt;/li>
&lt;li>&lt;strong>等待primary shard返回&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>目前的Elasticsearch有两个明显的身份，一个是分布式搜索系统，另一个是分布式NoSQL数据库，对于这两种不同的身份，读写语义基本类似，但也有一点差异。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-aced78c779161b2a5baa366f63d86883_720w.jpg" alt="">&lt;/p>
&lt;h2 id="写操作">&lt;strong>写操作&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>实时性：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>搜索系统的Index一般都是NRT（Near Real Time），近实时的，比如Elasticsearch中，Index的实时性是由refresh控制的，默认是1s，最快可到100ms，那么也就意味着Index doc成功后，需要等待一秒钟后才可以被搜索到。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NoSQL数据库的Write基本都是RT（Real Time），实时的，写入成功后，立即是可见的。Elasticsearch中的Index请求也能保证是实时的，因为Get请求会直接读内存中尚未Flush到存储介质的TransLog。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>可靠性：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>搜索系统对可靠性要求都不高，一般数据的可靠性通过将原始数据存储在另一个存储系统来保证，当搜索系统的数据发生丢失时，再从其他存储系统导一份数据过来重新rebuild就可以了。在Elasticsearch中，通过设置TransLog的Flush频率可以控制可靠性，要么是按请求，每次请求都Flush；要么是按时间，每隔一段时间Flush一次。一般为了性能考虑，会设置为每隔5秒或者1分钟Flush一次，Flush间隔时间越长，可靠性就会越低。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>NoSQL数据库作为一款数据库，必须要有很高的可靠性，数据可靠性是生命底线，决不能有闪失。如果把Elasticsearch当做NoSQL数据库，此时需要设置TransLog的Flush策略为每个请求都要Flush，这样才能保证当前Shard写入成功后，数据能尽量持久化下来。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>上面简单介绍了下NoSQL数据库和搜索系统的一些异同，我们会在后面有一篇文章，专门用来介绍Elasticsearch作为NoSQL数据库时的一些局限和特点。&lt;/p>
&lt;h2 id="读操作">读操作&lt;/h2>
&lt;p>下一篇《Elasticsearch内核解析 - 查询篇》中再详细介绍。&lt;/p>
&lt;p>上面大概对比了下搜索和NoSQL在写方面的特点，接下来，我们看一下Elasticsearch 6.0.0版本中写入流程都做了哪些事情，希望能对大家有用。&lt;/p>
&lt;h2 id="关键点">&lt;strong>关键点&lt;/strong>&lt;/h2>
&lt;p>在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑：&lt;/p>
&lt;ul>
&lt;li>可靠性：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。&lt;/li>
&lt;li>一致性：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。&lt;/li>
&lt;li>原子性：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。&lt;/li>
&lt;li>隔离性：多个写入操作相互不影响。&lt;/li>
&lt;li>实时性：写入后是否可以立即被查询到。&lt;/li>
&lt;li>性能：写入性能，吞吐量到底怎么样。&lt;/li>
&lt;/ul>
&lt;p>Elasticsearch作为分布式系统，也需要在写入的时候满足上述的四个特点，我们在后面的写流程介绍中会涉及到上述四个方面。&lt;/p>
&lt;p>接下来,我们一层一层剖析Elasticsearch内部的写机制。&lt;/p>
&lt;h2 id="lucene的写">&lt;strong>Lucene的写&lt;/strong>&lt;/h2>
&lt;p>众所周知，Elasticsearch内部使用了Lucene完成索引创建和搜索功能，Lucene中写操作主要是通过IndexWriter类实现，IndexWriter提供三个接口：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">addDocument&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">updateDocuments&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">deleteDocuments&lt;/span>&lt;span class="o">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>通过这三个接口可以完成单个文档的写入，更新和删除功能，包括了分词，倒排创建，正排创建等等所有搜索相关的流程。只要Doc通过IndesWriter写入后，后面就可以通过IndexSearcher搜索了，看起来功能已经完善了，但是仍然有一些问题没有解：&lt;/p>
&lt;ol>
&lt;li>上述操作是单机的，而不是我们需要的分布式。&lt;/li>
&lt;li>文档写入Lucene后并不是立即可查询的，需要生成完整的Segment后才可被搜索，如何保证实时性？&lt;/li>
&lt;li>Lucene生成的Segment是在内存中，如果机器宕机或掉电后，内存中的Segment会丢失，如何保证数据可靠性 ？&lt;/li>
&lt;li>Lucene不支持部分文档更新，但是这又是一个强需求，如何支持部分更新？&lt;/li>
&lt;/ol>
&lt;p>上述问题，在Lucene中是没有解决的，那么就需要Elasticsearch中解决上述问题。&lt;/p>
&lt;p>Elasticsearch在解决上述问题时，除了我们在上一篇《Elasticsearch数据模型简介》中介绍的几种系统字段外，在引擎架构上也引入了多重机制来解决问题。我们再来看Elasticsearch中的写机制。&lt;/p>
&lt;h2 id="elasticsearch的写">&lt;strong>Elasticsearch的写&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。&lt;/p>
&lt;p>此外，Elasticsearch整体架构上采用了一主多副的方式：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-8203d235d8cfc14849012e6ea229fa89_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch一主多副&lt;/p>
&lt;p>每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。&lt;/p>
&lt;p>请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。&lt;/p>
&lt;p>这种模式下，写入操作的延时就等于latency = Latency(Primary Write) + Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。&lt;/p>
&lt;p>采用多个副本后，避免了单机或磁盘故障发生时，对已经持久化后的数据造成损害，但是Elasticsearch里为了减少磁盘IO保证读写性能，一般是每隔一段时间（比如5分钟）才会把Lucene的Segment写入磁盘持久化，对于写入内存，但还未Flush到磁盘的Lucene数据，如果发生机器宕机或者掉电，那么内存中的数据也会丢失，这时候如何保证？&lt;/p>
&lt;p>对于这种问题，Elasticsearch学习了数据库中的处理方式：增加CommitLog模块，Elasticsearch中叫TransLog。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-20a780ddd33a74b37a81e18d3baf8983_720w.jpg" alt="">&lt;/p>
&lt;p>Refresh &amp;amp;&amp;amp; Flush&lt;/p>
&lt;p>在每一个Shard中，写入流程分为两部分，先写入Lucene，再写入TransLog。&lt;/p>
&lt;p>写入请求到达Shard后，先写Lucene文件，创建好索引，此时索引还在内存里面，接着去写TransLog，写完TransLog后，刷新TransLog数据到磁盘上，写磁盘成功后，请求返回给用户。这里有几个关键点，一是和数据库不同，数据库是先写CommitLog，然后再写内存，而Elasticsearch是先写内存，最后才写TransLog，一种可能的原因是Lucene的内存写入会有很复杂的逻辑，很容易失败，比如分词，字段长度超过限制等，比较重，为了避免TransLog中有大量无效记录，减少recover的复杂度和提高速度，所以就把写Lucene放在了最前面。二是写Lucene内存后，并不是可被搜索的，需要通过Refresh把内存的对象转成完整的Segment后，然后再次reopen后才能被搜索，一般这个时间设置为1秒钟，导致写入Elasticsearch的文档，最快要1秒钟才可被从搜索到，所以Elasticsearch在搜索方面是NRT（Near Real Time）近实时的系统。三是当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。四是每隔一段比较长的时间，比如30分钟后，Lucene会把内存中生成的新Segment刷新到磁盘上，刷新后索引文件已经持久化了，历史的TransLog就没用了，会清空掉旧的TransLog。&lt;/p>
&lt;p>上面介绍了Elasticsearch在写入时的两个关键模块，Replica和TransLog，接下来，我们看一下Update流程：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-e728bc042a75f8798925d708dc61b1ef_720w.jpg" alt="">&lt;/p>
&lt;p>Update&lt;/p>
&lt;p>Lucene中不支持部分字段的Update，所以需要在Elasticsearch中实现该功能，具体流程如下：&lt;/p>
&lt;ol>
&lt;li>收到Update请求后，从Segment或者TransLog中读取同id的完整Doc，记录版本号为V1。&lt;/li>
&lt;li>将版本V1的全量Doc和请求中的部分字段Doc合并为一个完整的Doc，同时更新内存中的VersionMap。获取到完整Doc后，Update请求就变成了Index请求。&lt;/li>
&lt;li>加锁。&lt;/li>
&lt;li>再次从versionMap中读取该id的最大版本号V2，如果versionMap中没有，则从Segment或者TransLog中读取，这里基本都会从versionMap中获取到。&lt;/li>
&lt;li>检查版本是否冲突(V1==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的Add请求。&lt;/li>
&lt;li>在Index Doc阶段，首先将Version + 1得到V3，再将Doc加入到Lucene中去，Lucene中会先删同id下的已存在doc id，然后再增加新Doc。写入Lucene成功后，将当前V3更新到versionMap中。&lt;/li>
&lt;li>释放锁，部分更新的流程就结束了。&lt;/li>
&lt;/ol>
&lt;p>介绍完部分更新的流程后，大家应该从整体架构上对Elasticsearch的写入有了一个初步的映象，接下来我们详细剖析下写入的详细步骤。&lt;/p>
&lt;h2 id="elasticsearch写入请求类型">&lt;strong>Elasticsearch写入请求类型&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中的写入请求类型，主要包括下列几个：Index(Create)，Update，Delete和Bulk，其中前3个是单文档操作，后一个Bulk是多文档操作，其中Bulk中可以包括Index(Create)，Update和Delete。&lt;/p>
&lt;p>在6.0.0及其之后的版本中，前3个单文档操作的实现基本都和Bulk操作一致，甚至有些就是通过调用Bulk的接口实现的。估计接下来几个版本后，Index(Create)，Update，Delete都会被当做Bulk的一种特例化操作被处理。这样，代码和逻辑都会更清晰一些。&lt;/p>
&lt;p>下面，我们就以Bulk请求为例来介绍写入流程。&lt;/p>
&lt;h2 id="elasticsearch写入流程图">&lt;strong>Elasticsearch写入流程图&lt;/strong>&lt;/h2>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-4e32cd77e69ae4932665d110d6bf13a1_720w.jpg" alt="">&lt;/p>
&lt;p>写入流程图&lt;/p>
&lt;ul>
&lt;li>红色：Client Node。&lt;/li>
&lt;li>绿色：Primary Node。&lt;/li>
&lt;li>蓝色：Replica Node。&lt;/li>
&lt;/ul>
&lt;h2 id="注册action">&lt;strong>注册Action&lt;/strong>&lt;/h2>
&lt;p>在Elasticsearch中，所有action的入口处理方法都是注册在ActionModule.java中，比如Bulk Request有两个注册入口，分别是Rest和Transport入口：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-7506f5d87f80ec63cbd2030b785441ec_720w.jpg" alt="">&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-ac3d0e6ee82d507d38110565d121febf_720w.jpg" alt="">&lt;/p>
&lt;p>如果请求是Rest请求，则会在RestBulkAction中Parse Request，构造出BulkRequest，然后发给后面的TransportAction处理。&lt;/p>
&lt;p>TransportShardBulkAction的基类TransportReplicationAction中注册了对Primary，Replica等的不同处理入口:&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-4ac69cd72079de47adfb2d81906579db_720w.jpg" alt="">&lt;/p>
&lt;p>这里对原始请求，Primary Node请求和Replica Node请求各自注册了一个handler处理入口。&lt;/p>
&lt;h2 id="client-node">&lt;strong>Client Node&lt;/strong>&lt;/h2>
&lt;p>Client Node 也包括了前面说过的Parse Request，这里就不再赘述了，接下来看一下其他的部分。&lt;/p>
&lt;p>&lt;strong>1. Ingest Pipeline&lt;/strong>&lt;/p>
&lt;p>在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。&lt;/p>
&lt;p>如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node。&lt;/p>
&lt;p>&lt;strong>2. Auto Create Index&lt;/strong>&lt;/p>
&lt;p>判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能。&lt;/p>
&lt;p>&lt;strong>3. Set Routing&lt;/strong>&lt;/p>
&lt;p>设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。&lt;/p>
&lt;p>在这一步中，如果没有指定&lt;em>id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID。&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>4. Construct BulkShardRequest&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard。&lt;/em>&lt;/p>
&lt;p>&lt;em>&lt;strong>5. Send Request To Primary&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;em>这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node。&lt;/em>&lt;/p>
&lt;h2 id="primary-node">&lt;strong>Primary Node&lt;/strong>&lt;/h2>
&lt;p>Primary 请求的入口是在PrimaryOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。&lt;/p>
&lt;p>&lt;strong>1. Index or Update or Delete&lt;/strong>&lt;/p>
&lt;p>循环执行每个Single Write Request，对于每个Request，根据操作类型(&lt;em>CREATE/INDEX/UPDATE/DELETE&lt;/em>)选择不同的处理逻辑。&lt;/p>
&lt;p>其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍。&lt;/p>
&lt;p>&lt;strong>2. Translate Update To Index or Delete&lt;/strong>&lt;/p>
&lt;p>这一步是Update操作的特有步骤，在这里，会将Update请求转换为Index或者Delete请求。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1。&lt;/p>
&lt;p>&lt;strong>3. Parse Doc&lt;/strong>&lt;/p>
&lt;p>这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，_uid = type # _id，对用户，_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument。&lt;/p>
&lt;p>&lt;strong>4. Update Mapping&lt;/strong>&lt;/p>
&lt;p>Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping。&lt;/p>
&lt;p>&lt;strong>5. Get Sequence Id and Version&lt;/strong>&lt;/p>
&lt;p>由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增1。&lt;/p>
&lt;p>&lt;strong>6. Add Doc To Lucene&lt;/strong>&lt;/p>
&lt;p>这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。&lt;/p>
&lt;p>如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。&lt;/p>
&lt;p>这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。&lt;/p>
&lt;p>Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，后续会有文章专门介绍Lucene中的写入。&lt;/p>
&lt;p>&lt;strong>7. Write Translog&lt;/strong>&lt;/p>
&lt;p>写完Lucene的Segment后，会以keyvalue的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。&lt;/p>
&lt;p>需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。&lt;/p>
&lt;p>这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint。&lt;/p>
&lt;p>&lt;strong>8. Renew Bulk Request&lt;/strong>&lt;/p>
&lt;p>这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样。&lt;/p>
&lt;p>&lt;strong>9. Flush Translog&lt;/strong>&lt;/p>
&lt;p>这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大。&lt;/p>
&lt;p>&lt;strong>10. Send Requests To Replicas&lt;/strong>&lt;/p>
&lt;p>这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。&lt;/p>
&lt;p>这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。&lt;/p>
&lt;p>发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler。&lt;/p>
&lt;p>&lt;strong>11. Receive Response From Replicas&lt;/strong>&lt;/p>
&lt;p>Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint。&lt;/p>
&lt;h2 id="replica-node">&lt;strong>Replica Node&lt;/strong>&lt;/h2>
&lt;p>Replica 请求的入口是在ReplicaOperationTransportHandler的messageReceived，我们来看一下相关的逻辑流程。&lt;/p>
&lt;p>&lt;strong>1. Index or Delete&lt;/strong>&lt;/p>
&lt;p>根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了。&lt;/p>
&lt;p>&lt;strong>2. Parse Doc&lt;/strong>&lt;/p>
&lt;p>&lt;strong>3. Update Mapping&lt;/strong>&lt;/p>
&lt;p>以上都和Primary Node中逻辑一致。&lt;/p>
&lt;p>&lt;strong>4. Get Sequence Id and Version&lt;/strong>&lt;/p>
&lt;p>Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行。&lt;/p>
&lt;p>&lt;strong>5. Add Doc To Lucene&lt;/strong>&lt;/p>
&lt;p>由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些。&lt;/p>
&lt;p>&lt;strong>6. Write Translog&lt;/strong>&lt;/p>
&lt;p>&lt;strong>7. Flush Translog&lt;/strong>&lt;/p>
&lt;p>以上都和Primary Node中逻辑一致。&lt;/p>
&lt;h2 id="最后">&lt;strong>最后&lt;/strong>&lt;/h2>
&lt;p>上面详细介绍了Elasticsearch的写入流程及其各个流程的工作机制，我们在这里再次总结下之前提出的分布式系统中的六大特性：&lt;/p>
&lt;ol>
&lt;li>可靠性：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。&lt;/li>
&lt;li>一致性：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现最终一致性。&lt;/li>
&lt;li>原子性：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。&lt;/li>
&lt;li>隔离性：仍然采用Version和局部锁来保证更新的是特定版本的数据。&lt;/li>
&lt;li>实时性：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。&lt;/li>
&lt;li>性能：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。后面我们也会有文章专门介绍Lucene中的优化思路。&lt;/li>
&lt;/ol></description></item><item><title>ElasticSearch基础</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/elasticsearch%E5%9F%BA%E7%A1%80/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/elasticsearch%E5%9F%BA%E7%A1%80/</guid><description>&lt;h1 id="elasticsearch基础">ElasticSearch基础&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch 是一个基于lucene的分布式可扩展的实时搜索和分析引擎。&lt;/p>
&lt;h2 id="特点">特点&lt;/h2>
&lt;ul>
&lt;li>分布式存储&lt;/li>
&lt;li>近实时检索&lt;/li>
&lt;/ul>
&lt;h2 id="核心概念">核心概念&lt;/h2>
&lt;ul>
&lt;li>索引(index)：&lt;/li>
&lt;li>分片(shard):&lt;/li>
&lt;li>分段(segment):&lt;/li>
&lt;li>Translog:&lt;/li>
&lt;/ul>
&lt;h3 id="写流程">写流程&lt;/h3>
&lt;h2 id="常用操作">常用操作&lt;/h2>
&lt;ul>
&lt;li>清空index数据&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># create index&lt;/span>
curl -X PUT http://192.168.0.10:20000/test6 --header &lt;span class="s2">&amp;#34;Content-Type: application/json&amp;#34;&lt;/span> -d index.json
cat index.json
&lt;span class="c1"># delete index&lt;/span>
curl -X DELETE http://192.168.0.10:20000/test6
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Elasticsearch数据模型</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="elasticsearch数据模型">Elasticsearch数据模型&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch是一个建立在全文搜索引擎库Apache Lucene 基础上的分布式搜索引擎，先来简单看一下Lucene中的一些数据模型：&lt;/p>
&lt;h2 id="lucene数据模型">Lucene数据模型&lt;/h2>
&lt;p>Lucene中包含了四种基本数据类型，分别是：&lt;/p>
&lt;ul>
&lt;li>Index：索引，由很多的Document组成。&lt;/li>
&lt;li>Document：由很多的Field组成，是Index和Search的最小单位。&lt;/li>
&lt;li>Field：由很多的Term组成，包括Field Name和Field Value。&lt;/li>
&lt;li>Term：由很多的字节组成，可以分词。&lt;/li>
&lt;/ul>
&lt;p>上述四种类型在Elasticsearch中同样存在，意思也一样。&lt;/p>
&lt;p>Lucene中存储的索引主要分为三种类型：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Invert Index：倒排索引，或者简称Index，通过Term可以查询到拥有该Term的文档。可以配置为是否分词，如果分词可以配置不同的分词器。索引存储的时候有多种存储类型，分别是：&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS：只存储DocID。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS：存储DocID和词频（Term Freq）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS_AND_POSITIONS：存储DocID、词频（Term Freq）和位置。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：存储DocID、词频（Term Freq）、位置和偏移。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DocValues：正排索引，采用列式存储。通过DocID可以快速读取到该Doc的特定字段的值。由于是列式存储，性能会比较好。一般用于sort，agg等需要高频读取Doc字段值的场景。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Store：字段原始内容存储，同一篇文章的多个Field的Store会存储在一起，适用于一次读取少量且多个字段内存的场景，比如摘要等。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Lucene中提供索引和搜索的最小组织形式是Segment，Segment中按照索引类型不同，分成了Invert Index，Doc Values和Store这三大类（还有一些辅助类，这里省略），每一类里面都是按照Doc为最小单位存储。Invert Index中存储的Key是Term，Value是Doc ID的链表；Doc Value中Key 是Doc ID和Field Name，Value是Field Value；Store的Key是Doc ID，Value是Filed Name和Filed Value。&lt;/p>
&lt;p>由于Lucene中没有主键概念和更新逻辑，所有对Lucene的更新都是Append一个新Doc，类似于一个只能Append的队列，所有Doc都被同等对等，同样的处理方式。其中的Doc由众多Field组成，没有特殊Field，每个Field也都被同等对待，同样的处理方式。&lt;/p>
&lt;p>从上面介绍来看，Lucene只是提供了一个索引和查询的最基本的功能，距离一个完全可用的完整搜索引擎还有一些距离：&lt;/p>
&lt;h2 id="lucene的不足">Lucene的不足&lt;/h2>
&lt;ol>
&lt;li>Lucene是一个单机的搜索库，如何能以分布式形式支持海量数据?&lt;/li>
&lt;li>Lucene中没有更新，每次都是Append一个新文档，如何做部分字段的更新？&lt;/li>
&lt;li>Lucene中没有主键索引，如何处理同一个Doc的多次写入？&lt;/li>
&lt;li>在稀疏列数据中，如何判断某些文档是否存在特定字段？&lt;/li>
&lt;li>Lucene中生成完整Segment后，该Segment就不能再被更改，此时该Segment才能被搜索，这种情况下，如何做实时搜索？&lt;/li>
&lt;/ol>
&lt;p>上述几个问题，对于搜索而言都是至关重要的功能诉求，我们接下来看看Elasticsearch中是如何来解这些问题的。&lt;/p>
&lt;h2 id="elasticsearch怎么做">Elasticsearch怎么做&lt;/h2>
&lt;p>在Elasticsearch中，为了支持分布式，增加了一个系统字段_routing（路由），通过_routing将Doc分发到不同的Shard，不同的Shard可以位于不同的机器上，这样就能实现简单的分布式了。&lt;/p>
&lt;p>采用类似的方式，Elasticsearch增加了_id、_version、_source和_seq_no等等多个系统字段，通过这些Elasticsearch中特有的系统字段可以有效解决上述的几个问题，新增的系统字段主要是下列几个：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-02d4de08ebd1f37d1b99cc84728f1cf3_720w.jpg" alt="">&lt;/p>
&lt;p>下面我们逐个字段的剖析下上述系统字段的作用，先来看第一个_id字段：&lt;/p>
&lt;h2 id="1-_id">&lt;strong>1. _id&lt;/strong>&lt;/h2>
&lt;p>Doc的主键，在写入的时候，可以指定该Doc的ID值，如果不指定，则系统自动生成一个唯一的UUID值。&lt;/p>
&lt;p>Lucene中没有主键索引，要保证系统中同一个Doc不会重复，Elasticsearch引入了_id字段来实现主键。每次写入的时候都会先查询id，如果有，则说明已经有相同Doc存在了。&lt;/p>
&lt;p>通过_id值（ES内部转换成_uid）可以唯一在Elasticsearch中确定一个Doc。&lt;/p>
&lt;p>Elasticsearch中，_id只是一个用户级别的虚拟字段，在Elasticsearch中并不会映射到Lucene中，所以也就不会存储该字段的值。&lt;/p>
&lt;p>_id的值可以由_uid解析而来（_uid =type + &amp;lsquo;#&amp;rsquo; + id），Elasticsearch中会存储_uid。&lt;/p>
&lt;h2 id="2-_uid">&lt;strong>2. _uid&lt;/strong>&lt;/h2>
&lt;p>_uid的格式是：type + &amp;lsquo;#&amp;rsquo; + id。&lt;/p>
&lt;p>_uid会存储在Lucene中，在Lucene中的映射关系如下：dex下可能存在多个id值相同的Doc，而6.0.0之后只支持单Type，同Index下id值是唯一的。&lt;/p>
&lt;p>uid会存储在Lucene中，在Lucene中的映射关系如下：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-b854f5d9505615613184bba5fe760088_720w.jpg" alt="">&lt;/p>
&lt;p>_uid 只是存储了倒排Index和原文store：倒排Index的目的是可以通过_id快速查询到文档；原文store用来在返回的Response里面填充完整的_id值。&lt;/p>
&lt;p>在Lucene中存储_uid，而不是_id的原因是，在6.0.0之前版本里面，_uid可以比_id表示更多的信息，比如Type。在6.0.0版本之后，同一个Index只能有一个Type，这时候Type就没多大意义了，后面Type应该会消失，那时候_id就会和_uid概念一样，到时候两者会合二为一，也能简化大家的理解。&lt;/p>
&lt;h2 id="3-_version">&lt;strong>3. _version&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中每个Doc都会有一个Version，该Version可以由用户指定，也可以由系统自动生成。如果是系统自动生成，那么每次Version都是递增1。&lt;/p>
&lt;p>_version是实时的，不受搜索的近实时性影响，原因是可以通过_uid从内存中versionMap或者TransLog中读取到。&lt;/p>
&lt;p>Version在Lucene中也是映射为一个特殊的Field存在。&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-a529eeb14094626f3523a92e0dfdb299_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中Version字段的主要目的是通过doc_id读取Version，所以Version只要存储为DocValues就可以了，类似于KeyValue存储。&lt;/p>
&lt;p>Elasticsearch通过使用version来保证对文档的变更能以正确的顺序执行，避免乱序造成的数据丢失：&lt;/p>
&lt;ol>
&lt;li>首次写入Doc的时候，会为Doc分配一个初始的Version：V0，该值根据VersionType不同而不同。&lt;/li>
&lt;li>再次写入Doc的时候，如果Request中没有指定Version，则会先加锁，然后去读取该Doc的最大版本V1，然后将V1+1后的新版本写入Lucene中。&lt;/li>
&lt;li>再次写入Doc的时候，如果Request中指定了Version：V1，则继续会先加锁，然后去读该Doc的最大版本V2，判断V1==V2，如果不相等，则发生版本冲突。否则版本吻合，继续写入Lucene。&lt;/li>
&lt;li>当做部分更新的时候，会先通过GetRequest读取当前id的完整Doc和V1，接着和当前Request中的Doc合并为一个完整Doc。然后执行一些逻辑后，加锁，再次读取该Doc的最大版本号V2，判断V1==V2，如果不相等，则在刚才执行其他逻辑时被其他线程更改了当前文档，需要报错后重试。如果相等，则期间没有其他线程修改当前文档，继续写入Lucene中。这个过程就是一个典型的read-then-update事务。&lt;/li>
&lt;/ol>
&lt;h2 id="4-_source">&lt;strong>4. _source&lt;/strong>&lt;/h2>
&lt;p>Elasticsearch中有一个重要的概念是source，存储原始文档，也可以通过过滤设置只存储特定Field。&lt;/p>
&lt;p>Source在Lucene中也是映射为了一个特殊的Field存在：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-5faeffae9ed270c1030c83499287dfc7_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_source字段的主要目的是通过doc_id读取该文档的原始内容，所以只需要存储Store即可。&lt;/p>
&lt;p>_source其实是名为_source的虚拟Store Field。&lt;/p>
&lt;p>Elasticsearch中使用_source字段可以实现以下功能：&lt;/p>
&lt;ul>
&lt;li>Update：部分更新时，需要从读取文档保存在_source字段中的原文，然后和请求中的部分字段合并为一个完整文档。如果没有_source，则不能完成部分字段的Update操作。&lt;/li>
&lt;li>Rebuild：最新的版本中新增了rebuild接口，可以通过Rebuild API完成索引重建，过程中不需要从其他系统导入全量数据，而是从当前文档的_source中读取。如果没有_source，则不能使用Rebuild API。&lt;/li>
&lt;li>Script：不管是Index还是Search的Script，都可能用到存储在Store中的原始内容，如果禁用了_source，则这部分功能不再可用。&lt;/li>
&lt;li>Summary：摘要信息也是来源于_source字段。&lt;/li>
&lt;/ul>
&lt;h2 id="5-_seq_no">&lt;strong>5. _seq_no&lt;/strong>&lt;/h2>
&lt;p>严格递增的顺序号，每个文档一个，Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。&lt;/p>
&lt;p>任何类型的写操作，包括index、create、update和Delete，都会生成一个_seq_no。&lt;/p>
&lt;p>_seq_no在Primary Node中由SequenceNumbersService生成，但其实真正产生这个值的是LocalCheckpointTracker，每次递增1：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="cm">/**
&lt;/span>&lt;span class="cm"> * The next available sequence number.
&lt;/span>&lt;span class="cm"> */&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">volatile&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">nextSeqNo&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="cm">/**
&lt;/span>&lt;span class="cm"> * Issue the next sequence number.
&lt;/span>&lt;span class="cm"> *
&lt;/span>&lt;span class="cm"> * @return the next assigned sequence number
&lt;/span>&lt;span class="cm"> */&lt;/span>
&lt;span class="kd">synchronized&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">generateSeqNo&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">nextSeqNo&lt;/span>&lt;span class="o">++;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>每个文档在使用Lucene的document操作接口之前，会获取到一个_seq_no，这个_seq_no会以系统保留Field的名义存储到Lucene中，文档写入Lucene成功后，会标记该seq_no为完成状态，这时候会使用当前seq_no更新local_checkpoint。&lt;/p>
&lt;p>checkpoint分为local_checkpoint和global_checkpoint，主要是用于保证有序性，以及减少Shard恢复时数据拷贝的数据拷贝量，更详细的介绍可以看这篇文章：&lt;a href="https://link.zhihu.com/?target=https%3A//www.elastic.co/blog/elasticsearch-sequence-ids-6-0">Sequence IDs: Coming Soon to an Elasticsearch Cluster Near You&lt;/a>。&lt;/p>
&lt;p>_seq_no在Lucene中的映射：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-a90804c654226608954774a639768afd_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_seq_no的作用有两个，一是通过doc_id查询到该文档的seq_no，二是通过seq_no范围查找相关文档，所以也就需要存储为Index和DocValues（或者Store）。由于是在冲突检测时才需要读取文档的_seq_no，而且此时只需要读取_seq_no，不需要其他字段，这时候存储为列式存储的DocValues比Store在性能上更好一些。&lt;/p>
&lt;p>_seq_no是严格递增的，写入Lucene的顺序也是递增的，所以DocValues存储类型可以设置为Sorted。&lt;/p>
&lt;p>另外，_seq_no的索引应该仅需要支持存储DocId就可以了，不需要FREQS、POSITIONS和分词。如果多存储了这些，对功能也没影响，就是多占了一点资源而已。&lt;/p>
&lt;h2 id="6-_primary_term">&lt;strong>6. _primary_term&lt;/strong>&lt;/h2>
&lt;p>_primary_term也和_seq_no一样是一个整数，每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。&lt;/p>
&lt;p>_primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-4a4184826fdd0a3d51001d3de3a57fb2_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_primary_term只需要通过doc_id读取到即可，所以只需要保存为DocValues就可以了.&lt;/p>
&lt;h2 id="7-_routing">&lt;strong>7. _routing&lt;/strong>&lt;/h2>
&lt;p>路由规则，写入和查询的routing需要一致，否则会出现写入的文档没法被查到情况。&lt;/p>
&lt;p>在mapping中，或者Request中可以指定按某个字段路由。默认是按照_Id值路由。&lt;/p>
&lt;p>_routing在Lucene中映射为：&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-a401f30a09d5f75b4adf79150544f536_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中文档级别的_routing主要有两个目的，一是可以查询到使用某种_routing的文档有哪些，当发生_routing变化时，可以对历史_routing的文档重新读取再Index，这个需要倒排Index。另一个是查询到文档后，在Response里面展示该文档使用的_routing规则，这里需要存储为Store。&lt;/p>
&lt;h2 id="8-_field_names">&lt;strong>8. _field_names&lt;/strong>&lt;/h2>
&lt;p>该字段会索引某个Field的名称，用来判断某个Doc中是否存在某个Field，用于exists或者missing请求。&lt;/p>
&lt;p>_field_names在Lucene中的映射：&lt;/p>
&lt;p>&lt;img src="https://pic2.zhimg.com/80/v2-81d5dbc1f9e816879ad063b2fc1e9ba9_720w.jpg" alt="">&lt;/p>
&lt;p>Elasticsearch中_field_names的目的是查询哪些Doc的这个Field是否存在，所以只需要倒排Index即可。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2></description></item><item><title>esrally for es on cfs</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/esrally/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/esrally/</guid><description>&lt;h1 id="esrally-for-es-on-cfs">esrally for es on cfs&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>esrally 是 elastic 官方开源的一款基于 python3 实现的针对 es 的压测工具，主要功能如下：&lt;/p>
&lt;ul>
&lt;li>自动创建、压测和销毁 es 集群&lt;/li>
&lt;li>可分 es 版本管理压测数据和方案&lt;/li>
&lt;li>完善的压测数据展示，支持不同压测之间的数据对比分析，也可以将数据存储到指定的es中进行二次分析&lt;/li>
&lt;li>支持收集 JVM 详细信息，比如内存、GC等数据来定位性能问题&lt;/li>
&lt;/ul>
&lt;h2 id="安装测试">安装测试&lt;/h2>
&lt;h3 id="测试环境">测试环境&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>节点类型&lt;/th>
&lt;th>节点数&lt;/th>
&lt;th>CPU&lt;/th>
&lt;th>内存&lt;/th>
&lt;th>存储&lt;/th>
&lt;th>网络&lt;/th>
&lt;th>备注&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>管理节点&lt;/td>
&lt;td>3&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>120 GB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>元数据节点&lt;/td>
&lt;td>10&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>16 x 1TB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>混合部署&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>数据节点&lt;/td>
&lt;td>10&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32 GB&lt;/td>
&lt;td>16 x 1TB SSD&lt;/td>
&lt;td>10 Gb/s&lt;/td>
&lt;td>混合部署&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="cfs配置">cfs配置&lt;/h3>
&lt;ul>
&lt;li>创建cfs vol&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="cp">#!/bin/sh
&lt;/span>&lt;span class="cp">&lt;/span>&lt;span class="c1"># ./create_vol.sh&lt;/span>
&lt;span class="nv">VolNames&lt;/span>&lt;span class="o">=&lt;/span>estest
&lt;span class="nv">leader&lt;/span>&lt;span class="o">=&lt;/span>10.194.139.42:8080 &lt;span class="c1">#cfs master leader节点的ip&lt;/span>
&lt;span class="nv">Capacity&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">1024&lt;/span> &lt;span class="c1">#unit GB&lt;/span>
&lt;span class="nv">Owner&lt;/span>&lt;span class="o">=&lt;/span>es01
&lt;span class="nv">DpCount&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">500&lt;/span>
&lt;span class="c1"># 创建vol&lt;/span>
curl &lt;span class="s2">&amp;#34;http://&lt;/span>&lt;span class="nv">$leader&lt;/span>&lt;span class="s2">/admin/createVol?name=&lt;/span>&lt;span class="nv">$VolName&lt;/span>&lt;span class="s2">&amp;amp;replicas=3&amp;amp;type=extent&amp;amp;capacity=&lt;/span>&lt;span class="nv">$Capacity&lt;/span>&lt;span class="s2">&amp;amp;owner=&lt;/span>&lt;span class="nv">$Owner&lt;/span>&lt;span class="s2">&amp;amp;followerRead=true&amp;#34;&lt;/span>
&lt;span class="c1"># 创建dp&lt;/span>
curl &lt;span class="s2">&amp;#34;http://&lt;/span>&lt;span class="nv">$leader&lt;/span>&lt;span class="s2">/dataPartition/create?count=&lt;/span>&lt;span class="nv">$DpCount&lt;/span>&lt;span class="s2">&amp;amp;name=&lt;/span>&lt;span class="nv">$VolName&lt;/span>&lt;span class="s2">&amp;amp;type=extent&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>
&lt;p>挂载cfs vol：&lt;/p>
&lt;p>在es运行节点运行cfs-client，挂载cfs vol到指定目录：&lt;code>/mnt/cfs&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ &lt;span class="nb">cd&lt;/span> &lt;span class="nv">$CFS_ROOT&lt;/span>
$ bin/cfs-client -c conf/cfs-client.json
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>cfs client 配置文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="err">//&lt;/span> &lt;span class="err">cfs-client.json&lt;/span>
&lt;span class="p">{&lt;/span>
&lt;span class="nt">&amp;#34;mountPoint&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/mnt/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;volName&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;estest&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;owner&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;es01&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;masterAddr&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;10.194.139.42:8080,10.194.139.44:8080,10.194.139.45:8080&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;logDir&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/export/Logs/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;warnLogDir&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;/export/Logs/cfs&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;logLevel&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;debug&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;consulAddr&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;http://cbconsul-cfs01.cbmonitor.svc.ht7.n.jd.local&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;exporterPort&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">9613&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nt">&amp;#34;profPort&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;11094&amp;#34;&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="es配置">es配置&lt;/h3>
&lt;p>修改es配置文件&lt;code>elasticsearch.yml&lt;/code>中配置项&lt;code>path.data&lt;/code>为cfs挂载目录:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="c"># elasticsearch.yml&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="c">#...&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">path.data&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/mnt/cfs/es/&amp;lt;HOST_NAME&amp;gt; &lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">## HOST_NAME为节点主机名，如果节点运行多个es，每个es需配置不同的目录&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="c">#...&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="esrally">esrally&lt;/h3>
&lt;ul>
&lt;li>centos7&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">$ yum install -y python3 python3-devel
&lt;span class="c1"># install git&lt;/span>
$ yum install -y http://opensource.wandisco.com/centos/6/git/x86_64/wandisco-git-release-6-1.noarch.rpm
$ yum install -y git
$ pip3 install esrally
$ &lt;span class="nv">target_hosts&lt;/span>&lt;span class="o">=&lt;/span>10.194.132.2:20000,10.194.132.5:20000,10.194.132.71:20000,10.194.134.196:20000
$ esrally --track&lt;span class="o">=&lt;/span>pmc &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --target-hosts&lt;span class="o">=&lt;/span>&lt;span class="nv">$target_hosts&lt;/span> &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --pipeline&lt;span class="o">=&lt;/span>benchmark-only
$ esrally --pipeline&lt;span class="o">=&lt;/span>benchmark-only &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --track&lt;span class="o">=&lt;/span>http_logs &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --target-hosts&lt;span class="o">=&lt;/span>&lt;span class="nv">$target_hosts&lt;/span> &lt;span class="se">\
&lt;/span>&lt;span class="se">&lt;/span> --report-file&lt;span class="o">=&lt;/span>/tmp/report_http_logs.md
&lt;span class="c1"># 指定集群，运行测试，test-mode参数只会运行1000条文档 es集群必须处理green状态，否则会被禁止race&lt;/span>
&lt;span class="c1"># 去掉--offline --test-mode可以让其把相关文件夹创建，然后结束掉&lt;/span>
esrally --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
esrally race --track&lt;span class="o">=&lt;/span>geonames --challenge&lt;span class="o">=&lt;/span>append-no-conflicts --user-tag&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;car:1g&amp;#34;&lt;/span> --car&lt;span class="o">=&lt;/span>1gheap --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
esrally race --track&lt;span class="o">=&lt;/span>geonames --challenge&lt;span class="o">=&lt;/span>append-no-conflicts --user-tag&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;car:2g&amp;#34;&lt;/span> --car&lt;span class="o">=&lt;/span>2gheap --pipeline&lt;span class="o">=&lt;/span>benchmark-only --target-hosts&lt;span class="o">=&lt;/span>127.0.0.1:9200 --offline --test-mode --client-options&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;basic_auth_user:&amp;#39;elastic&amp;#39;,basic_auth_password:&amp;#39;your_password&amp;#39;&amp;#34;&lt;/span>
&lt;span class="c1"># 对比2次的测试结果，根据esrally list races显示的时间戳当参数值，如果报错就使用Race ID&lt;/span>
esrally compare --baseline&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Race ID&amp;#39;&lt;/span> --contender&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Race ID&amp;#39;&lt;/span>
&lt;span class="c1"># 修改集群的分片数和副本数&lt;/span>
vim /home/esrally/.rally/benchmarks/tracks/default/geonames/index.json
vim /home/esrally/.rally/benchmarks/tracks/default/geonames/challenges/default.json
&lt;span class="c1"># 常用命令&lt;/span>
esrally list tracks
esrally list cars
esrally list races
esrally list pipelines
$ esrally list races
$ esrally compare --baseline 30889a15-66d3-4336-b6cb-0304834d853a --contender 1d1d5a98-54cb-486d-8339-98fe72ff054c
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="测试结果">测试结果&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>metric&lt;/th>
&lt;th>local-r0&lt;/th>
&lt;th>local-r1&lt;/th>
&lt;th>local-r2&lt;/th>
&lt;th>cfs-r0&lt;/th>
&lt;th>cfs-r1&lt;/th>
&lt;th>cfs-r2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Cumulative indexing time of primary&lt;br> shards&lt;/td>
&lt;td>48.1956&lt;/td>
&lt;td>50.1214&lt;/td>
&lt;td>53.6981&lt;/td>
&lt;td>61.514&lt;/td>
&lt;td>60.6874&lt;/td>
&lt;td>61.6843&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative merge time of primary shards&lt;/td>
&lt;td>19.3734&lt;/td>
&lt;td>19.759&lt;/td>
&lt;td>16.3446&lt;/td>
&lt;td>20.7753&lt;/td>
&lt;td>15.1702&lt;/td>
&lt;td>5.61673&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative refresh time of primary shard&lt;/td>
&lt;td>6.52128&lt;/td>
&lt;td>5.99413&lt;/td>
&lt;td>5.61948&lt;/td>
&lt;td>12.7984&lt;/td>
&lt;td>10.6824&lt;/td>
&lt;td>9.8606&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cumulative flush time of primary shards&lt;/td>
&lt;td>0.00513&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1.67E-05&lt;/td>
&lt;td>0.0028&lt;/td>
&lt;td>0.0147333&lt;/td>
&lt;td>0.03075&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://justice.bj.cn/Users/zhuzhengyi/Documents/gitnote/img/2020-06-03-13-36-05-image.png" alt="">&lt;/p>
&lt;ul>
&lt;li>
&lt;p>local-r0、local-r1, local-r2: 分别表示es    &lt;code>path.data&lt;/code>使用本地磁盘，replica分别为0，1，2时的数据；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cfs-r0，cfs-r1, cfs-r2分别表示es &lt;code>path.data&lt;/code>使用cfs 卷，replica分别为0,1,2时esrally的数据；&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ol>
&lt;li>&lt;a href="https://esrally.readthedocs.io/en/latest/">https://esrally.readthedocs.io/en/latest/&lt;/a>&lt;u>summary_report&lt;/u>.html&lt;/li>
&lt;li>&lt;a href="https://www.jianshu.com/p/c89975b50447">https://www.jianshu.com/p/c89975b50447&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://esrally.lyremelody.org/zh_CN/latest/quickstart.html">快速入门 — Rally 0.9.0 文档&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>ES倒排索引原理</title><link>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/</link><pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/elasticsearch/es%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/</guid><description>&lt;h1 id="es倒排索引原理">ES倒排索引原理&lt;/h1>
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>Elasticsearch通过Lucene的倒排索引技术实现比关系型数据库更快的过滤。它对多条件的过滤支持非常好，比如年龄在18和30之间，性别为女性这样的组合查询。倒排索引很多地方都有介绍，但是其比关系型数据库的b-tree索引快在哪里？到底为什么快呢？&lt;/p>
&lt;p>笼统的来说，b-tree索引是为写入优化的索引结构。当我们不需要支持快速的更新的时候，可以用预先排序等方式换取更小的存储空间，更快的检索速度等好处，其代价就是更新慢。要进一步深入的化，还是要看一下Lucene的倒排索引是怎么构成的。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-378bc62acf1a493c402291a8f8e99e6a_720w.jpg" alt="">&lt;/p>
&lt;p>这里有好几个概念。我们来看一个实际的例子，假设有如下的数据：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-e6b81003803254b1d11b3384626c93ab_720w.jpg" alt="">&lt;/p>
&lt;p>这里每一行是一个document。每个document都有一个docid。那么给这些document建立的倒排索引就是：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-c1cf40e4c4218fd3e992258c08e4e334_720w.jpg" alt="">&lt;/p>
&lt;p>可以看到，倒排索引是per field的，一个字段由一个自己的倒排索引。18,20这些叫做 term，而[1,3]就是posting list。Posting list就是一个int的数组，存储了所有符合某个term的文档id。那么什么是term dictionary 和 term index？&lt;/p>
&lt;p>假设我们有很多个term，比如：&lt;/p>
&lt;p>Carla,Sara,Elin,Ada,Patty,Kate,Selena&lt;/p>
&lt;p>如果按照这样的顺序排列，找出某个特定的term一定很慢，因为term没有排序，需要全部过滤一遍才能找出特定的term。排序之后就变成了：&lt;/p>
&lt;p>Ada,Carla,Elin,Kate,Patty,Sara,Selena&lt;/p>
&lt;p>这样我们可以用二分查找的方式，比全遍历更快地找出目标的term。这个就是 term dictionary。有了term dictionary之后，可以用 logN 次磁盘查找得到目标。但是磁盘的随机读操作仍然是非常昂贵的（一次random access大概需要10ms的时间）。所以尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个term dictionary本身又太大了，无法完整地放到内存里。于是就有了term index。term index有点像一本字典的大的章节表。比如：&lt;/p>
&lt;p>A开头的term ……………. Xxx页&lt;/p>
&lt;p>C开头的term ……………. Xxx页&lt;/p>
&lt;p>E开头的term ……………. Xxx页&lt;/p>
&lt;p>如果所有的term都是英文字符的话，可能这个term index就真的是26个英文字符表构成的了。但是实际的情况是，term未必都是英文字符，term可以是任意的byte数组。而且26个英文字符也未必是每一个字符都有均等的term，比如x字符开头的term可能一个都没有，而s开头的term又特别多。实际的term index是一棵trie 树：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-e4632ac1392b01f7a39d963fddb1a1e0_720w.jpg" alt="">&lt;/p>
&lt;p>例子是一个包含 &amp;ldquo;A&amp;rdquo;, &amp;ldquo;to&amp;rdquo;, &amp;ldquo;tea&amp;rdquo;, &amp;ldquo;ted&amp;rdquo;, &amp;ldquo;ten&amp;rdquo;, &amp;ldquo;i&amp;rdquo;, &amp;ldquo;in&amp;rdquo;, 和 &amp;ldquo;inn&amp;rdquo; 的 trie 树。这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。再加上一些压缩技术（搜索 Lucene Finite State Transducers） term index 的尺寸可以只有所有term的尺寸的几十分之一，使得用内存缓存整个term index变成可能。整体上来说就是这样的效果。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-e4599b618e270df9b64a75eb77bfb326_720w.jpg" alt="">&lt;/p>
&lt;p>现在我们可以回答“为什么Elasticsearch/Lucene检索可以比mysql快了。Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。&lt;/p>
&lt;p>额外值得一提的两点是：term index在内存中是以FST（finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary在磁盘上是以分block的方式保存的，一个block内部利用公共前缀压缩，比如都是Ab开头的单词就可以把Ab省去。这样term dictionary可以比b-tree更节约磁盘空间。&lt;/p>
&lt;h2 id="如何联合索引查询">如何联合索引查询？&lt;/h2>
&lt;p>所以给定查询过滤条件 age=18 的过程就是先从term index找到18在term dictionary的大概位置，然后再从term dictionary里精确地找到18这个term，然后得到一个posting list或者一个指向posting list位置的指针。然后再查询 gender=女 的过程也是类似的。最后得出 age=18 AND gender=女 就是把两个 posting list 做一个“与”的合并。&lt;/p>
&lt;p>这个理论上的“与”合并的操作可不容易。对于mysql来说，如果你给age和gender两个字段都建立了索引，查询的时候只会选择其中最selective的来用，然后另外一个条件是在遍历行的过程中在内存中计算之后过滤掉。那么要如何才能联合使用两个索引呢？有两种办法：&lt;/p>
&lt;ul>
&lt;li>使用skip list数据结构。同时遍历gender和age的posting list，互相skip；&lt;/li>
&lt;li>使用bitset数据结构，对gender和age两个filter分别求出bitset，对两个bitset做AN操作。&lt;/li>
&lt;/ul>
&lt;p>PostgreSQL 从 8.4 版本开始支持通过bitmap联合使用两个索引，就是利用了bitset数据结构来做到的。当然一些商业的关系型数据库也支持类似的联合索引的功能。Elasticsearch支持以上两种的联合索引方式，如果查询的filter缓存到了内存中（以bitset的形式），那么合并就是两个bitset的AND。如果查询的filter没有缓存，那么就用skip list的方式去遍历两个on disk的posting list。&lt;/p>
&lt;h2 id="利用-skip-list-合并">利用 Skip List 合并&lt;/h2>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-eafa46683272ff1b2081edbc8db5469f_720w.jpg" alt="">&lt;/p>
&lt;p>以上是三个posting list。我们现在需要把它们用AND的关系合并，得出posting list的交集。首先选择最短的posting list，然后从小到大遍历。遍历的过程可以跳过一些元素，比如我们遍历到绿色的13的时候，就可以跳过蓝色的3了，因为3比13要小。&lt;/p>
&lt;p>整个过程如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">Next -&amp;gt; 2
Advance(2) -&amp;gt; 13
Advance(13) -&amp;gt; 13
Already on 13
Advance(13) -&amp;gt; 13 MATCH!!!
Next -&amp;gt; 17
Advance(17) -&amp;gt; 22
Advance(22) -&amp;gt; 98
Advance(98) -&amp;gt; 98
Advance(98) -&amp;gt; 98 MATCH!!!
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后得出的交集是[13,98]，所需的时间比完整遍历三个posting list要快得多。但是前提是每个list需要指出Advance这个操作，快速移动指向的位置。什么样的list可以这样Advance往前做蛙跳？skip list：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-a8b78c8e861c34a1afd7891284852b34_720w.jpg" alt="">&lt;/p>
&lt;p>从概念上来说，对于一个很长的posting list，比如：&lt;/p>
&lt;p>[1,3,13,101,105,108,255,256,257]&lt;/p>
&lt;p>我们可以把这个list分成三个block：&lt;/p>
&lt;p>[1,3,13] [101,105,108] [255,256,257]&lt;/p>
&lt;p>然后可以构建出skip list的第二层：&lt;/p>
&lt;p>[1,101,255]&lt;/p>
&lt;p>1,101,255分别指向自己对应的block。这样就可以很快地跨block的移动指向位置了。&lt;/p>
&lt;p>Lucene自然会对这个block再次进行压缩。其压缩方式叫做Frame Of Reference编码。示例如下：&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-9c03d3e449e3f8fb8182287048ad6db7_720w.jpg" alt="">&lt;/p>
&lt;p>考虑到频繁出现的term（所谓low cardinality的值），比如gender里的男或者女。如果有1百万个文档，那么性别为男的posting list里就会有50万个int值。用Frame of Reference编码进行压缩可以极大减少磁盘占用。这个优化对于减少索引尺寸有非常重要的意义。当然mysql b-tree里也有一个类似的posting list的东西，是未经过这样压缩的。&lt;/p>
&lt;p>因为这个Frame of Reference的编码是有解压缩成本的。利用skip list，除了跳过了遍历的成本，也跳过了解压缩这些压缩过的block的过程，从而节省了cpu。&lt;/p>
&lt;h2 id="利用bitset合并">利用bitset合并&lt;/h2>
&lt;p>Bitset是一种很直观的数据结构，对应posting list如：&lt;/p>
&lt;p>[1,3,4,7,10]&lt;/p>
&lt;p>对应的bitset就是：&lt;/p>
&lt;p>[1,0,1,1,0,0,1,0,0,1]&lt;/p>
&lt;p>每个文档按照文档id排序对应其中的一个bit。Bitset自身就有压缩的特点，其用一个byte就可以代表8个文档。所以100万个文档只需要12.5万个byte。但是考虑到文档可能有数十亿之多，在内存里保存bitset仍然是很奢侈的事情。而且对于个每一个filter都要消耗一个bitset，比如age=18缓存起来的话是一个bitset，18&amp;lt;=age&amp;lt;25是另外一个filter缓存起来也要一个bitset。&lt;/p>
&lt;p>所以秘诀就在于需要有一个数据结构：&lt;/p>
&lt;ul>
&lt;li>可以很压缩地保存上亿个bit代表对应的文档是否匹配filter；&lt;/li>
&lt;li>这个压缩的bitset仍然可以很快地进行AND和 OR的逻辑操作。&lt;/li>
&lt;/ul>
&lt;p>Lucene使用的这个数据结构叫做 Roaring Bitmap。&lt;/p>
&lt;p>&lt;img src="https://pic3.zhimg.com/80/v2-9482b84c4aa3fb77a959c1ead553037e_720w.jpg" alt="">&lt;/p>
&lt;p>其压缩的思路其实很简单。与其保存100个0，占用100个bit。还不如保存0一次，然后声明这个0重复了100遍。&lt;/p>
&lt;p>这两种合并使用索引的方式都有其用途。Elasticsearch对其性能有详细的对比（&lt;a href="https://link.zhihu.com/?target=https%3A//www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps">https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps&lt;/a>）。简单的结论是：因为Frame of Reference编码是如此 高效，对于简单的相等条件的过滤缓存成纯内存的bitset还不如需要访问磁盘的skip list的方式要快。&lt;/p>
&lt;h2 id="如何减少文档数">如何减少文档数？&lt;/h2>
&lt;p>一种常见的压缩存储时间序列的方式是把多个数据点合并成一行。Opentsdb支持海量数据的一个绝招就是定期把很多行数据合并成一行，这个过程叫compaction。类似的vivdcortext使用mysql存储的时候，也把一分钟的很多数据点合并存储到mysql的一行里以减少行数。&lt;/p>
&lt;p>这个过程可以示例如下：&lt;/p>
&lt;p>&lt;img src="https://pic1.zhimg.com/80/v2-252d8f8ebe62e508f62049e80a9b9468_720w.jpg" alt="">&lt;/p>
&lt;p>可以看到，行变成了列了。每一列可以代表这一分钟内一秒的数据。&lt;/p>
&lt;p>Elasticsearch有一个功能可以实现类似的优化效果，那就是Nested Document。我们可以把一段时间的很多个数据点打包存储到一个父文档里，变成其嵌套的子文档。示例如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-text" data-lang="text">{timestamp:12:05:01, idc:sz, value1:10,value2:11}
{timestamp:12:05:02, idc:sz, value1:9,value2:9}
{timestamp:12:05:02, idc:sz, value1:18,value:17}
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以打包成：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="p">{&lt;/span>
    &lt;span class="err">max_timestamp:12:05:02,&lt;/span>
    &lt;span class="err">min_timestamp:&lt;/span> &lt;span class="err">1205:01,&lt;/span>
    &lt;span class="err">idc:sz,&lt;/span>
    &lt;span class="err">records:&lt;/span> &lt;span class="err">[&lt;/span>
&lt;span class="err">{timestamp:12:05:01,&lt;/span> &lt;span class="err">value1:10,value2:11&lt;/span>&lt;span class="p">}&lt;/span>
        &lt;span class="p">{&lt;/span>&lt;span class="err">timestamp:12:05:02,&lt;/span> &lt;span class="err">value1:9,value2:9&lt;/span>&lt;span class="p">}&lt;/span>
        &lt;span class="p">{&lt;/span>&lt;span class="err">timestamp:12:05:02,&lt;/span> &lt;span class="err">value1:18,value:17&lt;/span>&lt;span class="p">}&lt;/span>
    &lt;span class="err">]&lt;/span>
&lt;span class="err">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这样可以把数据点公共的维度字段上移到父文档里，而不用在每个子文档里重复存储，从而减少索引的尺寸。&lt;/p>
&lt;p>&lt;img src="https://pic4.zhimg.com/80/v2-917578288797efab8f67e7b74d5ec6a3_720w.jpg" alt="">&lt;/p>
&lt;p>在存储的时候，无论父文档还是子文档，对于Lucene来说都是文档，都会有文档Id。但是对于嵌套文档来说，可以保存起子文档和父文档的文档id是连续的，而且父文档总是最后一个。有这样一个排序性作为保障，那么有一个所有父文档的posting list就可以跟踪所有的父子关系。也可以很容易地在父子文档id之间做转换。把父子关系也理解为一个filter，那么查询时检索的时候不过是又AND了另外一个filter而已。前面我们已经看到了Elasticsearch可以非常高效地处理多filter的情况，充分利用底层的索引。&lt;/p>
&lt;p>使用了嵌套文档之后，对于term的posting list只需要保存父文档的doc id就可以了，可以比保存所有的数据点的doc id要少很多。如果我们可以在一个父文档里塞入50个嵌套文档，那么posting list可以变成之前的1/50。&lt;/p></description></item></channel></rss>