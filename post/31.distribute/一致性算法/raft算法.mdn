# Raft算法

## 简介

- `Raft`算法是2013年斯坦福大学的Diego Ongaro、John Ousterhout 两人发布的一致性算法，论文：《In Search of an Understandable Consensus Algorithm》；

- 与`Paxos`相比，Raft 易理解、易实现；

- Raft 和 Paxos 一样, 只要保证超过半数的节点正常就能够提供服务；

## 基本思想

* Raft将整个时间划分为一个个的小周期, 称为**任期**(`Term`)；

* 每个任期又分为`选举`和选举后`正常操作`两个阶段；

* `选举`阶段为每个周期开始的阶段，目的是通过投票选出一个`Leader`;

* 一个Term周期内只能有一个合法的`Leader`;

* 成功选举出`Leader`后，进入`正常操作`阶段；

* 选举阶段，整个Raft集群不处理外界客户端的请求；

* 正常操作阶段，只有`Leader`节点能正常接收并处理客户端请求，其他节点如果接收客户端请求，只能缓存或转发给`Leader`，由`Leader`进行处理；

* `Leader`接收到客户端请求后，先将请求追加到本地日志中，然后将请求发送给各个`Follower`节点；

* `Follower`接收到请求后，将请求写入本地日志，并给`Leader`发送响应；

* `Leader`收到多数`Follower`写入成功响应后，给客户端发送响应，告知状态机执行后结果；

![image-20190523192955386](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/12-20-57-24-image-20190523192955386.png)

## 关键概念

- **Log**(日志)：
- **Election**(选举)：Raft 的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为 Follower 某个节点定时器触发选举后 Term 递增，状态由 Follower 转为 Candidate，向其他节点发起 RequestVote RPC 请求
- **Term**(任期)：在 Raft 中使用了一个可以理解为周期（第几届、任期）的概念，用 Term 作为一个周期，每个 Term 都是一个连续递增的编号，每一轮选举都是一个 Term 周期，在一个 Term 中只能产生一个 Leader
- **Index**(日志序号)：

### 角色

基本的Raft 集群的节点分为以下三种角色：

- **Candidate**(候选者)：负责发起选举，发起投票及接收投票。Raft 刚启动时由一个节点从 Follower 转为 Candidate 发起选举，选举出 Leader 后从 Candidate 转为 Leader 状态；
- **Leader**(领导者)：负责日志的同步管理，处理来自客户端的请求，与 Follower 保持这 heartBeat 的联系；
- **Follower**(跟随者)：负责响应来自 Leader 或者 Candidate 的请求；

改进Raft集群另外提供如下几种角色：

- **Learner**(学习者)：只读节点，不参与选举投票及日志复制过程，只被动复制 follower 日志；
- **PreCandidate**(预选者)：

### 状态机

```mermaid
graph LR
    Follower(Follower)
    Candidate(Candidae)
    Leader(Leader)
    PreCandidate(PreCandidate)

  S((START)) --> Follower
    Follower -- elect timeout --> Candidate
    Follower -- high term --> Follower

    Candidate --elect timeout--> Candidate
    Candidate --quorum --> Leader
    Candidate --high term/hb Event--> Follower

    Leader --high term--> Follower

    Follower -- elect_timeout preVote --> PreCandidate
    PreCandidate --elect quorum--> Candidate
    PreCandidate --elect timeout--> PreCandidate
    PreCandidate --high term--> Follower
    Candidate --elect timeout prevote--> PreCandidate
```

![image-20190523141929145](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/12-20-55-36-fb8549c8d081a00433750711486cd9844ecf6559.png)

- 所有节点初始状态都是 Follower 角色；
- 选举计时器超时，转换为 Candidate 进行选举
- Candidate 收到大多数节点的选票则转换为 Leader；发现 Leader 或者收到更高任期的请求则转换为 Follower
- Leader 在收到更高任期的请求后转换为 Follower

### 计时器

raft 定义了两种计时器：

- **选举计时器**：Candidate 节点在进行选举时，如果在一个选举计时器周期内，任然没有获取多数投票，将重新发起选举。 默认值是 1000ms，最大值 50000ms。

- **心跳计时器**：Follower 节点在一个心跳计时器周期内没有收到 Leader 的心跳消息，该 Follower 将会转为 Candidate 状态，发起选举。默认值 100ms，
  
  选举定时必须要大于 5 倍心跳定时，建议是 10 倍关系。

### 关键数据结构

- Entry
- Message
- Peer
- Snapshot：
- HardState：持久化状态；
- SoftState: 内存状态；
- 需要持久化的状态：
  - currentTerm: 当前任期；
  - votedFor：投给票的节点 ID；
  - log：日志序列
- 内存中的状态：
  - commitIndex：
  - lastApplied：
- leader 内存状态：
  - nextIndex[]：
  - matchIndex[]：

![image-20190523190540129](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2020/11/12-20-57-38-image-20190523190540129.png)

- firstLogIndex：标识当前日志序列的起始位置，如果日志不做压缩处理，也就是没有快照模块的话，那么 firstLogIndex 就是零值。

- lastLogIndex：

- commitIndex：表示当前已经提交的日志，也就是成功同步到 majority 的日志位置的最大值

- applyIndex：是已经 apply 到状态机的日志索引，它的值必须小于等于 commitIndex，因为只有已经提交的日志才可以 apply 到状态机

## 角色

* 共有持久存储的数据：
  
  * id: raft 节点id;
  
  * term：当前任期;
  
  * log: raft日志序列，只能append和truncate, 无法insert()，update()；

* 共有内存数据：
  
  * peers: 其他节点；
  
  * queued_reqs: 以缓存的req，用来缓存选举过程中收到的req；
  
  * proxied_reqs: 已转发的req，用来记录follower转发到leader的req, 在一个新的选举期内被清空；
  
  * node_tx: raft外部消息入口，通过该管道接收外界输入的消息；
  
  * state_tx: 指令状态机指令入口，通过该管道向指令状态机发送指令；

### Candidate

* 私有数据：
  
  * votes: 已经收到的投票总数(包括自己投自己的一票)，如果votes>半数，则当选为leader；
  
  * election_ticks: 当前轮选举已过的ticks数, 每个tick来到时+1；
  
  * election_timeout: 当前轮选举超时数， election_ticks>election_timeout时，发起新一轮选举(term++)；

### Follower

* follower内存状态：
  
  * leader: 当前term的leader；
  
  * voted_for: 当前term所投的leader；
  
  * leader_seen_ticks: leader心跳计数tick, 每个tick +1, 收到leader hb msg， 重置为0；
  
  * leader_seen_timeout: leader心跳timeount, `leader_seen_ticks` > `leader_seen_timeout`时, follower将转入candidate;

### Leader

* leader内存状态：
  
  * `heartbeat_ticks`: 心跳tick计数;
  
  * `peers_next_index`:  将要发送到peers的下一个index.每个peer的`peer_next_index`最开始置为leader启动时的最后一个log index+1。在复制过程中leader将发送以为起始的(term,peer_next_index-1)所有后续log给follower，follower检查该(term,peer_next_index-1)数据是否在log中存在，如果不存在, 则会发送拒绝复制的响应。leader接收到拒绝复制响应后，依次减小改值`peer_next_index`，直到被接受；
  
  * `peers_last_index`: 已经发送到peers的最后一个index，` peers_last_index[peerId] < peers_next_index[peerId] `；

## 快照

Raft日志会一直增长

## 算法流程

Raft 算法流程分为以下几个步骤：

1. **启动**：系统启动阶段；

2. **选举**：通过投票，产生leader的阶段；

3. **日志复制**：当选举成功产生 Leader 后，系统进入日志复制阶段，Leader 持续将收到的客户端日志按顺序复制到 Follower：

4. **变更**：

![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2021/09/04-10-58-18-raft-alg.gif)

### 启动

1. 初始启动时，所有节点(Node)均为`Follower`状态，任期`term`置为 1；

2. 如果`Follower`节点在`选举超时时间`内未收到其他节点的 append/hearbeat/snapshot 消息，则状态变为`Candidate`，进入选举状态；

3. 重启时，需load 以下状态：
   
   * term；
   
   * 

### 选举

选举阶段主要在`Candidate`和`Follower`节点之间进行：

1. `Follower`节点的心跳计时器如果超时，将转入`Candidate`状态；
2. `Candidate`节点先给自己投一票，然后给其他节点发送`拉票请求SolictVoteReq`，该请求包含本节点最后一条log的(term,index)；
3. `Leader`,`Candidate`节点将忽略的`拉票请求`;
4. `Follower`节点收到`SolictVoteReq(拉票请求)`后，按如下规则处理：
   1. 已投过票(voteFor!=0)，忽略该Req（一个任期内(term), 每个follower至多投一票）；
   2. 比较`SolictVoteReq拉票请求`中的`term`或`index`和本节点最后一条日志的term和index；
   3. 如果`req.term<self.log.last_term` || （ req.term==self.log.last_term&&req.index<self.log.last_index)，忽略请求(candidate的日志比当前节点的日志要旧)；
   4. 否则, 该节点发送`GrantVoteAck(投票消息)`给`拉票节点`，并记录投票信息到voteFor；
   5. 上述规则保证了新当选的leader一定拥有所有committed的log；
5. `Candidate`节点收到`GrantVoteAck(投票响应)`后：
   1. 计算获得票数，如果票数达到多数赞成票（>一半投票者），则自身当选为`Leader`，转为领导人状态, 同时向其他节点发送`心跳消息`, 并将缓存的`客户端请求`立即执行；
   2. 如果收到其他节点的 Leader 心跳包，且该心跳包的任期要>=自身节点，则表明该其他节点已成功当选为 leader，自身节点将转为`Follower`状态；
   3. 否则，选举计数器超时，表明该选举周期内没有任何节点当选，接下来将`term`加 1 后，重新进行选举；

### 日志复制

每个**term**在选举成功后，进入日志复制阶段。日志复制阶段中`Leader`接收外部`Client`的请求，将请求日志复制到各`Follower`的过程。日志复制的流程如下：

1. `Leader`接收`Client`消息；
2. `Leader`将消息增加上(index, term)作为`Log`，将`Log`追加到日志存储中;
3. `Leader`通过`AppendEntries` RPC调用将日志`复制`到所有`Follower`，日志复制的范围是；
4. `Follower`收到`AppendEntries` RPC，记录Log并返回ACK给`Leader`;
5. `Leader` 收到大多数 `Follower` 节点的Ack，就可以commit，则通过`指令状态机`apply消息，并结果返回给 client；
6. `Follower`上的所有已提交log被异步有序()的`apply`到指令状态机中；

![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2021/09/07-18-13-28-2021-09-07-18-13-23-image.png)

### Log Commit

* Raft 不会通过计算副本数目的方式去提交一个**之前任期内**的日志条目;

* 只有leader**当前任期里**的日志条目通过计算副本数目可以被提交；

![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2022/03/23-17-14-07-2022-03-23-17-13-57-image.png)

* 为什么领导人无法决定对**老任期号**的日志条目进行提交?
  
  * (a) , S1 是Leader，部分Follower(S2)复制了索引位置 2 的日志条目;
  
  * (b) , S1 崩溃了, S5在Term 3 赢得选举(S5, S3, S4赞成)，然后从Client收了一条日志条目放在了索引 2 处;
  
  * (c)，S5 崩溃了, S1 重新启动，选举(Term4)成功(S1,S2,S3,S4赞成)，后开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。
  
  * (d), S1 又崩溃了，S5 可以重新被选举(Term5)成功（S2，S3 和 S4 赞成），然后覆盖了他们在索引 2 处的日志。
  
  * 反之，如果S1在崩溃之前，把自己主导的新任期里产生的日志条目复制到了大多数机器上，
  
  * (e) ，那么在后面任期里面这些新的日志条目就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。

### 指令状态机的执行

* leader节点在收到client消息后，先将消息append到log中，而后并行发送到所有的followers；

* 当leader收到多数节点成功ack后，提交消息(commitIndex)，并向本节点的指令状态机发送Apply指令，让指令状态机执行消息，执行成功后，向客户端返回；

* follower节点收到leader节点的hb, 会对比hb中的commitIndex和本节点的oldCommitIndex, 然后将所有在该范围内的[oldCommittedIndex+1, committedIndex]日志都apply到本节点的指令状态机中；

### 配置改变

* 配置变更包括节点数量的改变，配置参数的变更等。

* 最简单的方式是：停止集群、改变成员、启动集群。这种方式在执行时会导致集群整体不可用；

* Raft集群成员配置作为一个特殊日志从 leader 节点同步到其它节点去；

* Raft 使用一种两阶段方法平滑切换集群成员配置来避免遇到前一节描述的问题，具体流程如下：

## 安全性

Raft 增加了如下两条限制以保证安全性：

- 拥有最新的已提交的 log entry 的 Follower 才有资格成为 Leader。

- Leader 只能推进 commit index 来提交当前 term 的已经复制到大多数服务器上的日志，旧 term 日志的提交要等到提交当前 term 的日志来间接提交（log index 小于 commit index 的日志被间接提交）。

### 日志压缩

在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft 采用对整个系统进行 snapshot 来解决，snapshot 之前的日志都可以丢弃。

每个副本独立的对自己的系统状态进行 snapshot，并且只能对已经提交的日志记录进行 snapshot。

当 Leader 要发给某个日志落后太多的 Follower 的 log entry 被丢弃，Leader 会将 snapshot 发给 Follower。或者当新加进一台机器时，也会发送 snapshot 给它。发送 snapshot 使用 InstalledSnapshot RPC（RPC 细节参见八、Raft 算法总结）。

![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2021/10/07-09-28-51-2021-10-07-09-28-04-image.png)

### 成员变更

* 成员变更是在集群运行过程中副本发生变化，如增加/减少副本数、节点替换等。

因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在 Cold 和 Cnew 中同时存在两个不相交的多数派，进而可能选出两个 Leader，形成不同的决议，破坏安全性。

为了解决这一问题，Raft 提出了两阶段的成员变更方法。

集群先从旧成员配置 Cold 切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置 Cold 和新成员配置 Cnew 的组合 Cold U Cnew，一旦共同一致 Cold U Cnew 被提交，系统再切换到新成员配置 Cnew。

如果一次只增加或减少一个节点，那么并不会出现上面说的两个多数派的问题

当主节点收到对当前集群（C_old）新增/移除节点的请求时，它会将新的集群配置（C_new）作为一条新的日志加入到队列中，并用上文提到的机制复制到其它各个节点。

当一个节点收到新的日志时，日志中的 C_new 会立即生效，即该节点的日志会被复制到 C_new 中配置的其它节点，且日志是否被提交也以 C_new 中指定的节点作为依据。

这意味着节点不需要等 C_new 日志被提交后才开始启用 C_new，且每个节点总是使用它的日志中最新的配置。

当主节点提交 C_new 日志后，新增/移除节点的操作就算结束。此时，主节点能确定至少 C_new 中的多数节点已经启用了 C_new 配置，同时，那些还没有启用 C_new 的节点也不再可能组成新的“多数节点”。

![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2022/03/24-11-11-56-2022-03-24-11-11-42-image.png)

## 实现要点

- Batch and Pipeline

- 并行 Append Log

- 异步 Apply log

- Snapshot

- 异步 Lease Read

- ReadIndex

### 预投票（PreVote）

- 问题：当一个 Follower 节点（A）与其他节点网络隔离时，由于心跳超时，将会变成候选人并发起选举，这时会递增 Term。由于网络隔离，其选举将无法成功，于是会持续选举，导致 Term 不断增大。当网络恢复后，该 A 节点会把其 Term 发送给其他节点，由于该 Term 很大概率大于其他节点 Term，从而引发其他节点进入选举流程。但此时，由于 A 节点的被隔离很久，日志不可能为最新的，所以其不会成为 Leader，导致集群一直在选举。Raft 论文中提出了**PreVote**算法来解决该问题。
  
  ![](https://raw.githubusercontent.com/ZhuZhengyi/notebook-images/master/2022/03/28-16-40-31-2022-03-28-16-40-26-image.png)

- 主要思想：
  
  - 在发起正式投票前，先进行预投票(pre-vote)，预投票时自身 term 不变，但投票 tern+1。
  
  - 确认自己能获得集群大多数节点的投票时，才将自己的 term+1，然后正式进行投票。
  
  - 由此就可以避免在网络分区的时孤立节点的 term 持续增大，导致后续选举的反复。

- 预投票是一个典型的 2PC 事务，

为此，需要增加一个新的 PreCandidate 状态。

### Read

* 为保证 Read 操作的一致性，最简单的方法是将 Read 也走一遍 raft log(Log Read)，这样性能将很差。

* 因为 leader 节点能保证已经 committed 的 log 是最新的 log，所以可以直接从 leader 读取，为此需保证 leader 的有效性，有两种方式：

#### ReadIndex

1. Leader 将当前自己的 commit index 记录到一个 local 变量 ReadIndex 里面。
2. 向其他节点发起一次 heartbeat，如果大多数节点返回了对应的 heartbeat response，那么 leader 就能够确定现在自己仍然是 leader。
3. Leader 等待自己的状态机执行，直到 apply index 超过了 ReadIndex，这样就能够安全的提供 linearizable read 了。
4. Leader 执行 read 请求，将结果返回给 client。

注意：

leader 刚通过选举成为 leader 的时候， commit index 并不能够保证是当前整个系统最新的 commit index，此时首先提交一个 no-op 的 entry，保证 leader 的 commit index 成为最新的。

https://jin-yang.github.io/post/golang-raft-etcd-sourcode-consistent-reading.html

#### Lease Read

虽然 ReadIndex 比原来的 Raft log read 快了很多，但毕竟还是有 Heartbeat 的开销，在 Raft 论文里面，提到了一种通过 clock + heartbeat 的 lease read 优化方法。

也就是 leader 发送 heartbeat 的时候，会首先记录一个时间点 start，当系统大部分节点都回复了 heartbeat response，那么我们就可以认为 leader 的 lease 有效期可以到 `start + election timeout / clock drift bound `这个时间点。

为什么能够这么认为呢？主要是在于 Raft 的选举机制，因为 follower 会在至少 election timeout 的时间之后，才会重新发生选举，所以下一个 leader 选出来的时间一定可以保证大于 `start + election timeout / clock drift bound`。

虽然采用 lease 的做法很高效，但仍然会面临风险问题，也就是我们有了一个预设的前提，各个服务器的 CPU clock 的时间是准的，即使有误差，也会在一个非常小的 bound 范围里面，如果各个服务器之间 clock 走的频率不一样，有些太快，有些太慢，这套 lease 机制就可能出问题。

### Follower Read

1. 先去 Leader 查询最新的 committed index；

2. 然后拿着 committed Index 去 Follower read，从而保证能从 Follower 中读到最新的数据；
   
   当前 etcd 就实现了 Follower read

### 幽灵复现

1. https://zhuanlan.zhihu.com/p/47025699

## 参考

1. [线性一致性和 Raft](https://zhuanlan.zhihu.com/p/47117804)
2. [Raft 的 PreVote 实现机制](https://zhuanlan.zhihu.com/p/35697913)
3. [Etcd 之 Lease read](https://zhuanlan.zhihu.com/p/50455478)
4. [Raft 协议精解](https://juejin.im/post/5af066f1f265da0b715634b9)
5. [TiKV 源码解析系列 - Lease Read](https://mp.weixin.qq.com/s?__biz=MzI3NDIxNTQyOQ==&mid=2247484499&idx=1&sn=79acb9b4b2f8baa3296f2288c4a0a45b&scene=0#wechat_redirect)
6. [Raft TLA+形式化验证](https://blog.csdn.net/solotzg/article/details/80669924)
7. [分布式系统一致性协议 Raft 理解 - Jefferywang 的烂笔头](https://blog.wangjunfeng.com/post/raft/)
8. [「图解 Raft」让一致性算法变得更简单 - ZingLix Blog](https://zinglix.xyz/2020/06/25/raft/)
9. [raft-zh_cn/raft-zh_cn.md at master · maemual/raft-zh_cn · GitHub](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)
10. [条分缕析 Raft 算法](https://mp.weixin.qq.com/s/lUbVBVzvNVxhgbcHQBbkkQ)
11. [Raft 为什么是更易理解的分布式一致性算法 - mindwind - 博客园](https://www.cnblogs.com/mindwind/p/5231986.html)
12. [全面理解Raft协议](https://zhuanlan.zhihu.com/p/125573685)
13. [Raft 协议实战系列（五）—— 集群成员变更与日志压缩](https://juejin.cn/post/6902274909959880711)
14. [浅谈分布式存储之raft](https://zhuanlan.zhihu.com/p/114221938)
15. https://segmentfault.com/a/1190000022248118
16. https://www.sofastack.tech/projects/sofa-jraft/raft-introduction/
17. https://pingcap.com/zh/blog/building-distributed-db-with-raft
18. [深入浅出etcd/raft](https://mrcroxx.github.io/categories/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAetcd/raft/)
