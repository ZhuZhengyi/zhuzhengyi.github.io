# HBase RPC

## 简介

HBase主要包含Master，RegionServer，Client 3个组件组成。组件之间通过Rpc 和 zk进行通信。RPC通信功能主要基于Protobuf和NIO这两个组件来实现，

![](assets/2020-01-07-11-42-09-image.png)

## 配置参数

1. hbase.ipc.server.listen.queue.size : 
   
   存放连接请求的等待队列长度,默认与ipc.server.listen.queue.size参数值相同，为128个。

2. hbase.regionserver.handler.count
   
   regionserver 的 rpc请求队列处理线程数，默认为 30

3. hbase.master.handler.count
   
   master rpc请求队列处理线程数，默认为 25

4. hbase.ipc.server.read.threadpool.size
   
   Reader线程数，默认为10个。reader 的个数决定了从网络 io 里读取数据的速度也就是网络吞吐量

5. hbase.ipc.server.max.callqueue.size
   
   单个消费队列所允许的存储空间上限(默认为1GB)，超过该上限客户端会抛出以下异常

6. hbase.ipc.server.max.callqueue.length
   
   单个消费队列的长度限制，默认值为10倍的Handler数。

7. hbase.ipc.server.callqueue.handler.factor
   
   该参数用于决定消费队列的个数。

8. hbase.ipc.server.callqueue.read.share

9. 

```
hbase.ipc.server.read.threadpool.size

默认： 10
Reader线程数
```

## 结构

RPC报文

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-11-06-09-image.png)

* 请求头

![](assets/2020-01-07-11-06-34-image.png)

```protobuf
message RequestHeader {
    optional uint32 call_id = 1;
    optional RPCTInfo trace_info = 2;
    optional string method_name = 3;
    optional bool request_param = 4;
```

![](assets/2020-01-07-11-07-17-image.png)

```protobuf
message ResponseHeader {
    optional uint32 call_id = 1;
```

## RPC实现

RpcServer配置三个队列：

* callQueue

绝大部分Call请求存在该队列中：callQueue上maxQueueLength为${ipc.server.max.callqueue.length},默认是${hbase.master.handler.count}*DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER，目前0.95.1中，每个Handler上CallQueue的最大个数默认值(DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER)为10。

* PriorityQueue

如果设置priorityHandlerCount的个数，会创建与callQueue相当容量的queue存储Call，该优先级队列对应的Handler的个数由rpcServer实例化时传入。

* replicationQueue

由于RpcServer由HMaster和RegionServer共用，该功能仅为RegionServer提供，queue的大小为${ipc.server.max.callqueue.size}指定，默认为1024*1024*1024，handler的个数为hbase.regionserver.replication.handler.count。

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-11-18-46-image.png)

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-11-29-06-image.png)

* rpc-client

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-12-34-08-image.png)

* rpc-server

rpc-server基于nio 的reactor模型设计，其主要流程如下：

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-12-34-46-image.png)

* scheduler

hbase rpc实现了两种调度器（FifoRpcScheduler和SimpleRpcScheduler）。FifoRpcScheduler是master默认的调度器，直接将CallRunner对象放到线程池中去执行。而SimpleRpcScheduler是RS默认调度器，分成三种不同的executor，对于不同的请求，使用的不同的executor去执行。

![](/Users/zhuzhengyi/Documents/gitnote/img/2020-01-07-12-35-19-image.png)

## 参考

1. [Hbase 调优之 RPC - 云+社区 - 腾讯云](https://cloud.tencent.com/developer/article/1005560)

2. https://blog.csdn.net/yangzishiw/article/details/78840107

3. [The internals of HBase Rpc (Protobuf) | Binospace](http://www.binospace.com/index.php/in-depth-analysis-hbase-rpc-0-95-version-implementation-mechanism/)

4. http://09itblog.site/?p=874

5. 
