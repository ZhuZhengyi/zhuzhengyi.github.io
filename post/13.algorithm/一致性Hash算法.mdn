---

---

# 一致性哈希算法

一致性哈希算法在 1997 年由麻省理工学院的 Karger 等人在解决分布式 Cache 中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和 CARP 十分类似。一致性哈希修正了 CARP 使用的简单哈希算法带来的问题，使得 DHT 可以在 P2P 环境中真正得到应用。

但现在一致性 hash 算法在分布式系统中也得到了广泛应用，研究过 memcached 缓存数据库的人都知道，memcached 服务器端本身不提供分布式 cache 的一致性，而是由客户端来提供，具体在计算一致性 hash 时采用如下步骤：

1. 首先求出 memcached 服务器（节点）的哈希值，并将其配置到 0 ～ 232 的圆（continuum）上。
2. 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。
3. 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过 232 仍然找不到服务器，就会保存到第一台 memcached 服务器上。

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff01ae569b6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

从上图的状态中添加一台 memcached 服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但 Consistent Hashing 中，只有在园（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示：

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff01cb23ca9?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

## 一致性 Hash 性质

考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其 hash 值（通常与系统中的节点数目有关），由于 hash 值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性 hash 就显得至关重要，良好的分布式 cahce 系统中的一致性 hash 算法应该满足以下几个方面：

- **平衡性(Balance)**

平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。

- **单调性(Monotonicity)**

单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P 表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从 P1 到 P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在 P2P 系统内，缓冲的变化等价于 Peer 加入或退出系统，这一情况在 P2P 系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。

- **分散性(Spread)**

在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

- **负载(Load)**

负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

- **平滑性(Smoothness)**

平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。

## 原理

### 基本概念

一致性哈希算法（Consistent Hashing）最早在论文《[Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](http://dl.acm.org/citation.cfm?id=258660)》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-2^32-1（即哈希值是一个 32 位无符号整形），整个哈希空间环如下：

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff019d48ed5?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

整个空间按顺时针方向组织。0 和 232-1 在零点中方向重合。

下一步将各个服务器使用 Hash 进行一个哈希，具体可以选择服务器的 ip 或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用 ip 地址哈希后在环空间的位置如下：

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff019f1cecc?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

接下来使用如下算法定位数据访问到相应服务器：将数据 key 使用相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。

例如我们有 Object A、Object B、Object C、Object D 四个数据对象，经过哈希计算后，在环空间上的位置如下：

![](data:image/svg+xml;utf8,<?xml version="1.0"?><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="746" height="775"></svg>)

根据一致性哈希算法，数据 A 会被定为到 Node A 上，B 被定为到 Node B 上，C 被定为到 Node C 上，D 被定为到 Node D 上。

下面分析一致性哈希算法的容错性和可扩展性。现假设 Node C 不幸宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

下面考虑另外一种情况，如果在系统中增加一台服务器 Node X，如下图所示：

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff01dab936a?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

此时对象 Object A、B、D 不受影响，只有对象 C 需要重定位到新的 Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。

综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。

另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下，

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff040ae43be?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

此时必然造成大量数据集中到 Node A 上，而只有极少量会定位到 Node B 上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器 ip 或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：

![](https://user-gold-cdn.xitu.io/2018/4/26/162ffff03ecc7be4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到 Node A 上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

## JAVA 代码实现

```java
package org.java.base.hash;
import java.util.Collection;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.TreeMap;
import java.util.TreeSet;

public class ConsistentHash<T> {
 private final int numberOfReplicas;// 节点的复制因子,实际节点个数 * numberOfReplicas =
 // 虚拟节点个数
 private final SortedMap<Integer, T> circle = new TreeMap<Integer, T>();// 存储虚拟节点的hash值到真实节点的映射

 public ConsistentHash( int numberOfReplicas,
 Collection<T> nodes) {
 this.numberOfReplicas = numberOfReplicas;
 for (T node : nodes){
 add(node);
 }
 }

 public void add(T node) {
 for (int i = 0; i < numberOfReplicas; i++){
 // 对于一个实际机器节点 node, 对应 numberOfReplicas 个虚拟节点
 /*
 * 不同的虚拟节点(i不同)有不同的hash值,但都对应同一个实际机器node
 * 虚拟node一般是均衡分布在环上的,数据存储在顺时针方向的虚拟node上
 */
 String nodestr =node.toString() + i;
 int hashcode =nodestr.hashCode();
 System.out.println("hashcode:"+hashcode);
 circle.put(hashcode, node);

 }
 }

 public void remove(T node) {
 for (int i = 0; i < numberOfReplicas; i++)
 circle.remove((node.toString() + i).hashCode());
 }

 /*
 * 获得一个最近的顺时针节点,根据给定的key 取Hash
 * 然后再取得顺时针方向上最近的一个虚拟节点对应的实际节点
 * 再从实际节点中取得 数据
 */
 public T get(Object key) {
 if (circle.isEmpty())
 return null;
 int hash = key.hashCode();// node 用String来表示,获得node在哈希环中的hashCode
 System.out.println("hashcode----->:"+hash);
 if (!circle.containsKey(hash)) {//数据映射在两台虚拟机器所在环之间,就需要按顺时针方向寻找机器
 SortedMap<Integer, T> tailMap = circle.tailMap(hash);
 hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey();
 }
 return circle.get(hash);
 }

 public long getSize() {
 return circle.size();
 }

 /*
 * 查看表示整个哈希环中各个虚拟节点位置
 */
 public void testBalance(){
 Set<Integer> sets = circle.keySet();//获得TreeMap中所有的Key
 SortedSet<Integer> sortedSets= new TreeSet<Integer>(sets);//将获得的Key集合排序
 for(Integer hashCode : sortedSets){
 System.out.println(hashCode);
 }

 System.out.println("----each location 's distance are follows: ----");
 /*
 * 查看相邻两个hashCode的差值
 */
 Iterator<Integer> it = sortedSets.iterator();
 Iterator<Integer> it2 = sortedSets.iterator();
 if(it2.hasNext())
 it2.next();
 long keyPre, keyAfter;
 while(it.hasNext() && it2.hasNext()){
 keyPre = it.next();
 keyAfter = it2.next();
 System.out.println(keyAfter - keyPre);
 }
 }

 public static void main(String[] args) {
 Set<String> nodes = new HashSet<String>();
 nodes.add("A");
 nodes.add("B");
 nodes.add("C");

 ConsistentHash<String> consistentHash = new ConsistentHash<String>(2, nodes);
 consistentHash.add("D");

 System.out.println("hash circle size: " + consistentHash.getSize());
 System.out.println("location of each node are follows: ");
 consistentHash.testBalance();

 String node =consistentHash.get("apple");
 System.out.println("node----------->:"+node);
 }

}
```

## Jump 一致性哈希算法

jump consistent hash 是一种一致性哈希算法, 此算法**零内存消耗**，**均匀分配**，**快速**，并且**只有 5 行代码**。

```c
int32_t JumpConsistentHash(uint64_t key, int32_t num_buckets) {
    int64_t b = -1, j = 0;
    while (j < num_buckets) {
        b = j;
        key = key * 2862933555777941757ULL + 1;
        j = (b + 1) * (double(1LL << 31) / double((key >> 33) + 1));
    }
    return b;
}
```

## Kademlia 算法

Kademlia 算法在 2002 年由 Petar Maymounkov 和 David Mazières 所设计，以异或距离来对哈希表进行分层是其特点。Kademlia 后来被 eMule、BitTorrent 等 P2P 软件采用作为底层算法。

- 对于任意一个有[ 2(n−1) ,2𝑛)个节点的网络，最多只需要 n 步搜索即可找到目标节点；
- K-bucket 的更新机制一定程度上保持了网络的活性和安全性。
