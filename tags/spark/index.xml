<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spark on Justice的小站</title><link>https://justice.bj.cn/tags/spark.html</link><description>Recent content in spark on Justice的小站</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 17 Mar 2022 09:56:25 +0000</lastBuildDate><atom:link href="https://justice.bj.cn/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark_基础</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</link><pubDate>Thu, 17 Mar 2022 09:56:25 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html</guid><description>Spark 基础 简介 Spark 是一种快速、通用、可扩展的大数据分析引擎，2009 年诞生于加州大学伯克利分校 AMPLab，2010 年开源，2013 年 6 月成为 Apache 孵</description></item><item><title>SPARK-25299-_改进Spark_Shuffle可靠性</title><link>https://justice.bj.cn/post/30.architech/spark/spark-25299.html</link><pubDate>Thu, 20 Jan 2022 22:03:10 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark-25299.html</guid><description>SPARK-25299: 改进Spark Shuffle可靠性 背景动机 在分布式计算中，shuffle表示多个不同计算单元之间的数据交换。spark用shuffle来表</description></item><item><title>Spark_文件_IO_分析</title><link>https://justice.bj.cn/post/30.architech/spark/spark-shuffle.html</link><pubDate>Thu, 20 Jan 2022 22:03:10 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark-shuffle.html</guid><description>Spark 文件 IO 分析 1. Spark 简介 Spark 是一种是基于内存计算的大数据并行计算框架，主要分为 Driver、Worker 两个组件，可通过 yarn，mesos、k8</description></item><item><title>Zeus-_Uber的分布式Spark_Shuffle_Service</title><link>https://justice.bj.cn/post/30.architech/spark/zeus-uber-remote-spark-shuffle-service.html</link><pubDate>Fri, 14 Jan 2022 14:52:56 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/zeus-uber-remote-spark-shuffle-service.html</guid><description>Zeus: Uber的分布式Spark Shuffle Service 特性 垂直扩展 独立节点运行； 去中心化存储，节点无状态； 解决网络延迟 降低服务响应等待时间 流数据 性能优化 架构 优化点</description></item><item><title>Shuffle_Map_Task运算结果的处理</title><link>https://justice.bj.cn/post/30.architech/spark/spark_suffle_map_task.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark_suffle_map_task.html</guid><description>Shuffle Map Task运算结果的处理 这个结果的处理，分为两部分，一个是在Executor端是如何直接处理Task的结果的；还有就是Driver端，如果</description></item><item><title>spark</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C.html</guid><description>spark 简介 Spark vs MapReduce MapReduce Spark 运行模式 spark支持standlone、yarn、mesos等多种运行模式，其中standlone模式主要用于线下环境的测</description></item><item><title>Spark-3.0_源码</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E6%BA%90%E7%A0%81.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E6%BA%90%E7%A0%81.html</guid><description>Spark-3.0 源码 类 DiskBlockManager: 逻辑块和物理磁盘的逻辑映射管理； BlockManager: 1</description></item><item><title>Spark_Sql</title><link>https://justice.bj.cn/post/30.architech/spark/spark-sql.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark-sql.html</guid><description>Spark Sql 简介 参考 http://hbasefly.com/2017/03/01/sparksql-catalyst/ Spark SQL Catalyst优化器 - 简书 https://zhuanlan.zhihu.com/p/50493032 http://www.jasongj.com/spark/adaptive_execution/</description></item><item><title>Spark_存储体系</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E5%AD%98%E5%82%A8%E4%BD%93%E7%B3%BB.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E5%AD%98%E5%82%A8%E4%BD%93%E7%B3%BB.html</guid><description>Spark 存储体系 简介 参考 https://www.codercto.com/a/85944.html https://www.cnblogs.com/cenglinjinran/p/8476199.html https://juejin.im/post/5bd854955188255ca65da7ed https://blog.guopengfei.top/2019/05/15/Spark%E5%AD%98%E5%82%A8%E4%BD%93%E7%B3%BB%E8%AF%A6%E8%A7%A3/ http://www.linuxboy.net/yunjisuan/51238.html https://blog.csdn.net/anzhsoft/article/details/42519333 Spark源码阅读之存储体系&amp;ndash;存储体系概述与shuffle服务 - ChouYarn - 博客园</description></item><item><title>Spark存储体系详解</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E5%AD%98%E5%82%A8%E4%BD%93%E7%B3%BB%E8%AF%A6%E8%A7%A3.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E5%AD%98%E5%82%A8%E4%BD%93%E7%B3%BB%E8%AF%A6%E8%A7%A3.html</guid><description>Spark存储体系详解 存储体系的职责 在研究Spark存储体系之前，我们先搞清楚一个重要的问题：对于一个数据计算引擎，存储体系在其中的职责是什</description></item><item><title>管理_Apache_Spark_的_Java_和_Scala_依赖项</title><link>https://justice.bj.cn/post/30.architech/spark/spark%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86.html</link><pubDate>Fri, 25 Dec 2020 10:57:29 +0000</pubDate><guid>https://justice.bj.cn/post/30.architech/spark/spark%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86.html</guid><description>管理 Apache Spark 的 Java 和 Scala 依赖项 Spark 应用通常依赖于第三方 Java 或 Scala 库。以下是在向 Cloud Dataproc 集群提交 Spark 作业时添加这些依赖项的建议方法： 使用 gcloud dataproc jobs submit 命令从本地机器提交</description></item></channel></rss>